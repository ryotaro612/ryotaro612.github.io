<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blanket</title>
    <link>https://nryotaro.dev/</link>
    <description>Recent content on Blanket</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 08 Oct 2023 09:38:50 -0400</lastBuildDate><atom:link href="https://nryotaro.dev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A design methodology for reliable software systems (1972)</title>
      <link>https://nryotaro.dev/posts/a_design_methodology_for_reliable_software_systems/</link>
      <pubDate>Sun, 08 Oct 2023 09:38:50 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_design_methodology_for_reliable_software_systems/</guid>
      <description>&lt;p&gt;スタンフォードで数学の博士号を取得した&lt;a href=&#34;https://computerhistory.org/profile/barbara-liskov/&#34;&gt;Liskov&lt;/a&gt;は以前勤めていたMitre Corporationに戻った後、最初にVenusとよばれるタイムシェアリングシステムの開発プロジェクトを担当し、その次にソフトウェア危機に対処する開発手法の研究に取り組んだ。
&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/1479992.1480018&#34;&gt;A design methodology for reliable software systems&lt;/a&gt;は、Venusの開発から得られた構造化プログラミングの方法論である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</title>
      <link>https://nryotaro.dev/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/</link>
      <pubDate>Sat, 07 Oct 2023 10:28:13 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.13461&#34;&gt;BART&lt;/a&gt;は&lt;a href=&#34;https://browse.arxiv.org/pdf/1706.03762.pdf&#34;&gt;Transformer&lt;/a&gt;をつかった系列変換モデルの事前学習である。
ノイズを入れたテキストからもとのテキストを復元できるようにモデルを訓練する。
BARTの特徴は、ノイズの作り方に制限がないところにある。
比較したノイズの作り方は、ランダムに選んだトークンから文書を始めることで回転する、BERTとおなじトークンのマスキング、トークンの一部の削除、文書中の文の順序の入れ換え、ある区間中にあるトークンをまとめて1つの&lt;code&gt;[MASK]&lt;/code&gt;に置き換える方法の5種類である。
最後のトークンを1つのマスクキングするときに最もよい結果になった。
マスクに置き換える区間の長さは\(\lambda = 3\)のポアソン分布によって決まる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Communicating Sequential Processes (1978)</title>
      <link>https://nryotaro.dev/posts/communicating_sequenctial_processes/</link>
      <pubDate>Sat, 30 Sep 2023 11:34:47 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/communicating_sequenctial_processes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~crary/819-f09/Hoare78.pdf&#34;&gt;Communicating Sequential Processes (CSP)&lt;/a&gt;は、あるプロセスの出力を別のプロセスの入力に渡し、入出力のあるプロセスを並行実行するプログラミングモデルである。
たとえば、GoのgoroutinesはCSPに&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall16/cos418/docs/P1-concurrency.pdf&#34;&gt;もとづく&lt;/a&gt;軽量スレッドである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
      <link>https://nryotaro.dev/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</link>
      <pubDate>Sat, 23 Sep 2023 10:21:03 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.10084&#34;&gt;Sentence-BERT&lt;/a&gt;は、テキストをコサイン類似度で意味の類似度を比較できる固定長のベクトルに変換できる。
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt;を2つのテキストを入力し類似度を出力するように訓練できるが、あるテキストに類似するテキストを求めたい場合には組合せ爆発が生じる。
Sentence-BERTは、類似度が定義されたテキストの組から、類似するテキスト同士を近い位置に写像できるように学習する。
ネットワークは、BERTを使ったシャムネットワークであり、重みを共有した2つのBERTに1つずつテキストを入力し、両方の出力を目的関数に入力する。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
    <item>
      <title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning(2008)</title>
      <link>https://nryotaro.dev/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/</link>
      <pubDate>Sat, 16 Sep 2023 08:59:45 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://machinelearning.org/archive/icml2008/papers/391.pdf&#34;&gt;A Unified Architecture for Natural Language Processing&lt;/a&gt;は、複数のタスクの訓練データで重みの更新を繰り返すマルチタスクの深層学習である。
複数のタスクから順にあるタスクを選び、選んだタスクからランダムに取り出したサンプルで重みを更新するオンライン学習である。
ネットワークの層は、入力に近い方から、Word embedding、時間遅延ニューラルネットワーク(Time-Delay Neural Networks, TDNN)層、TDNN層の全時刻にわたる各ユニットの最大値を出力するMax Layer, 全結合層、ソフトマックスからなる。
Word embeddingのみタスク間で重みを共有し、後続の層の重みはタスクごとに異なる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Transmission of Information(1927)</title>
      <link>https://nryotaro.dev/posts/transmission_of_information/</link>
      <pubDate>Sat, 09 Sep 2023 12:10:45 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/transmission_of_information/</guid>
      <description>&lt;p&gt;Hartleyは、&lt;a href=&#34;https://monoskop.org/images/a/a6/Hartley_Ralph_VL_1928_Transmission_of_Information.pdf&#34;&gt;Transmission of Information&lt;/a&gt;で、通信システムの情報伝達性能を評価するために、情報の定量的な尺度を提唱した。
シャノンが1948年に情報エントロピーを&lt;a href=&#34;https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&#34;&gt;発表&lt;/a&gt;する21年前である。
Hartleyは、通信内容の意味や解釈を捨象し、記号列の有限な候補からある記号列を選ぶことを情報みなした。
シャノンは、有限の候補から記号列を選択する考えを継承し、さらに、記号が順に確率的に選ばれるとみなすことでHartleyの情報量の定義を情報エントロピーに発展させた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</title>
      <link>https://nryotaro.dev/posts/roberta/</link>
      <pubDate>Sun, 27 Aug 2023 13:13:39 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/roberta/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.11692.pdf&#34;&gt;RoBERTa&lt;/a&gt;は、&lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt;のアーキテクチャを変えずに、事前学習の改善によって性能を向上をはかる。
BERTの事前学習では、Masked Language ModelとNext Sentence Predictionの2つのタスクでパラメータを更新する。
一方、RoBERTaはMasked Language Modelのみを採用する。
さらに、Byte Pair Encodingのサブワードの単位を文字からバイトに変更し、語彙集合数が50,000に増えている。
ミニバッチのサイズもまたBERTより大きい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Go To Statement Considered Harmful(1968)</title>
      <link>https://nryotaro.dev/posts/go_to_statement_considered_harmful/</link>
      <pubDate>Sat, 26 Aug 2023 10:46:18 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/go_to_statement_considered_harmful/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf&#34;&gt;Go To Statement Considered Harmful&lt;/a&gt;でEdger Dijkstraは機械語以外でのGO TO文の使用を批判した。
ソースコードは機械語にコンパイルされ、プログラムとして実行される。
ソースコードは静的な成果物であるが、プログラムは時間とともに変化する。
プログラマにとって理解しやすいのは実行中のプログラムよりもソースコードの方なので、ソースコードからプログラムの実行状態を予測できることがのぞましい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Attention with Relative Position Representations(2018)</title>
      <link>https://nryotaro.dev/posts/self_attention_with_relative_position_representations/</link>
      <pubDate>Sat, 19 Aug 2023 10:02:50 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/self_attention_with_relative_position_representations/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.02155.pdf&#34;&gt;Self-Attention with Relative Position Representations&lt;/a&gt;は、&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Transformer&lt;/a&gt;のQKV注意機構に系列の要素間の相対位置をあたえる手法である。
Transformerは、RNNやCNNとはちがい、要素の位置をモデルにあたえる決まった方法をもたない。
Transformerの原論文は、正弦関数で要素の位置からベクトルを計算し、それを埋め込みベクトルに加算することで、位置情報をエンコードする。
正弦関数を使う理由は、訓練データにない長さの系列に対する汎化性能を周期性で上げられると仮定したからである。
提案手法は、この仮定を継承し、要素間の相対位置で位置情報をエンコードすることで、訓練データにない系列長の入力に対する汎化性能の向上をはかる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Adjusting Binary Search Trees(1985)</title>
      <link>https://nryotaro.dev/posts/self_adjusting_binary_search_trees/</link>
      <pubDate>Sat, 12 Aug 2023 13:49:56 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/self_adjusting_binary_search_trees/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~sleator/papers/self-adjusting.pdf&#34;&gt;Self-Adjusting Binary Search Tree&lt;/a&gt;は、スプレー木といい、ノードを参照するたびに、ノードを木のルートに移動させる抽象データ構造である。
参照、挿入、削除の償却時間計算量は、ノード数\(n\)の木であれば\(\log(n)\)である。
さらに、十分な数の参照にかかる時間計算量は、その参照の系列に最適化された二分探索木の計算量の定数倍におさまる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(2020)</title>
      <link>https://nryotaro.dev/posts/exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer/</link>
      <pubDate>Sat, 05 Aug 2023 13:27:09 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;は、
複数の異なる自然言語処理を、統一的にテキストからテキストを生成する問題とみなした大規模な実験によって、自然言語処理への転移学習の応用に見通しをつけた。
この統一的なアプローチは、&lt;em&gt;T&lt;/em&gt;ext-&lt;em&gt;t&lt;/em&gt;o-&lt;em&gt;T&lt;/em&gt;ext &lt;em&gt;T&lt;/em&gt;ransfer &lt;em&gt;T&lt;/em&gt;ransformer(T5)と名づけられた。
既存の技術への理解を深めることを目的としており、新たなアルゴリズムの提案はない。
一方、調査する技術の限界を調べるために、大きなデータが必要であったことから、新たなデータセットColossal Clean Crawled Corpus(C4)が作られた。
C4には、&lt;a href=&#34;https://commoncrawl.org/&#34;&gt;Common Crawl&lt;/a&gt;から抽出された英語のテキストがふくまれる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Catalan Numbers(2016)</title>
      <link>https://nryotaro.dev/posts/catalan_number/</link>
      <pubDate>Sat, 29 Jul 2023 15:24:15 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/catalan_number/</guid>
      <description>&lt;p&gt;カタラン数は、いくつかの組合せの問題の解で知られる自然数の系列である。
\(n\)個の対応する括弧&amp;quot;(, )&amp;ldquo;からなる文字列の組合せの数や、一辺の長さが\(n\)の格子の左上から右下への対角線より上を通る最短経路の数は、\(n\)番目のカタラン数である。
このページでは、この2つの問題の解がカタラン数であるとして、漸化式と二項係数の2つの形式によるカタラン数の一般項を求める。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Abstract Syntax Networks for Code Generation and Semantic Parsing</title>
      <link>https://nryotaro.dev/posts/abstract_syntax_networks_for_code_generation_and_semantic_parsing/</link>
      <pubDate>Sat, 01 Jul 2023 17:53:10 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/abstract_syntax_networks_for_code_generation_and_semantic_parsing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aclanthology.org/P17-1105/&#34;&gt;Abstract Syntax Networks&lt;/a&gt;は、非構造的な文章などの入力から、抽象構文木(AST)にしたがう系列を生成できるencoder decoderである。
decoderは、ASTの生成規則にある記号に対応するモジュールを再帰的に構成したネットワークである。
モジュールは、右辺のどの生成規則を選択すべきか推定する。
そして、選択した規則のモジュールをさらに再帰的に選択することで、ASTにしたがう出力を生成する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fibonacchi Heaps and Their Uses in Improved Network Optimization Algorithms(1987)</title>
      <link>https://nryotaro.dev/posts/fibonacci_heaps_and_their_uses_in_improving_network_optimization_algorithms/</link>
      <pubDate>Sat, 24 Jun 2023 10:38:25 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/fibonacci_heaps_and_their_uses_in_improving_network_optimization_algorithms/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://web.eecs.umich.edu/~pettie/matching/Fredman-Tarjan-Fibonacci-Heaps.pdf&#34;&gt;Fibonacci heaps&lt;/a&gt;(F-heaps)は、ダイクストラアルゴリズムの高速化ために開発された木構造の抽象データ型である。
ノードは、1つの値を保存し、親へのポインタをもつ。
また、2つのポインタによって、同じ階層にある兄弟ノードからなる双方向リストに組み込まれる。
ルート階層にあるノードはいずれも親をもたない。
そのほかにも、削除時に使用されるboolean型の変数がノードごとに1つある。
ヒープにある要素の数\(n\)に対して、要素を償却時間計算量\(O(\log n)\)でき、また、定数の償却時間計算量でほかの主要な操作を行える。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
    <item>
      <title>Layer Normalization</title>
      <link>https://nryotaro.dev/posts/layer_normalization/</link>
      <pubDate>Sat, 17 Jun 2023 10:18:51 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/layer_normalization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;層正規化&lt;/a&gt;(Layer Normalization)は、ユニットへの入力の重みつきの和を正規化し、学習時間を短縮する手法である。
各層ごとにユニットへの入力の和の平均と標準偏差を求め、この2つの統計量で、その層にあるユニットへの入力を正規化する。
先行研究には&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34;&gt;バッチ正規化&lt;/a&gt;がある。
バッチ正規化では、各ミニバッチにおいて、ユニットごとに入力の重みつき和の平均と分散を計算し、正規化する。
バッチ正規化でも学習時間を短縮できるが、その効果はミニバッチのサイズに依存し、また、RNNに簡単に応用することはできない欠点がある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Universal Classes of Hash Functions(1977)</title>
      <link>https://nryotaro.dev/posts/universal_classes_of_hash_functions/</link>
      <pubDate>Sat, 10 Jun 2023 17:32:42 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/universal_classes_of_hash_functions/</guid>
      <description>&lt;p&gt;値の保存と参照からなる任意のリクエストの系列を、系列長の時間計算量で処理できるハッシュ関数と連想配列を示す。
求めるハッシュ関数の集合があるとき、各ハッシュ関数の時間計算量の平均がリクエストの系列長に等しくなる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unsupervised Cross-lingual Representation Learning at Scale(2020)</title>
      <link>https://nryotaro.dev/posts/unsupervised_cross_lingual_representation_learning_at_scale/</link>
      <pubDate>Sat, 03 Jun 2023 12:32:35 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/unsupervised_cross_lingual_representation_learning_at_scale/</guid>
      <description>&lt;p&gt;多言語モデルを大規模なコーパスで訓練し、含意関係認識、質問応答、固有表現抽出において、多言語版の&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt;を上まわる予測性能を実現した。
モデルのアーキテクチャは&lt;a href=&#34;https://arxiv.org/abs/1907.11692&#34;&gt;RoBERTa&lt;/a&gt;で、&lt;a href=&#34;https://arxiv.org/abs/1901.07291&#34;&gt;Lample and Conneau, 2019&lt;/a&gt;に近い方法でモデルを訓練する。
LampleとConneauの手法を含む従来の多言語の言語モデルの評価実験では、wipediaやwikipediaと同程度の大きさのコーパスが使われていた。
従来の訓練データに対し、100言語からなる2.5TBのCommonCrawlをコーパスに使い、コーパスを大規模化することによるモデルへの影響を分析した。
パラメタ数などのモデル大きさを固定し、言語の種類数を30まで増やしたところ、コーパスの小さい言語の性能が向上したが、それ以上増やすと逆に予測性能が低下した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Majority Consensus Approach to Concurrency Control for Multiple Copy Databases(1979)</title>
      <link>https://nryotaro.dev/posts/a_mojority_consensus_approach_to_concurrency_control_for_multiple_copy_databases/</link>
      <pubDate>Sat, 27 May 2023 13:41:06 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_mojority_consensus_approach_to_concurrency_control_for_multiple_copy_databases/</guid>
      <description>&lt;p&gt;データベースを同期するためのアルゴリズム&lt;a href=&#34;https://pages.cs.wisc.edu/~remzi/Classes/739/Fall2018/Papers/thomas79-quorums.pdf&#34;&gt;majority consensus&lt;/a&gt;を提案する。
アプリケーションは任意のデータベースに更新リクエストを送信でき、データベースは更新リクエストの処理について含意をとる。
データベースは、タイムスタンプのついたレコードの集合である。
タイムスタンプは、レコードの値の更新時刻をあらわす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>WORD TRANSLATION WITHOUT PARALLEL DATA(2018)</title>
      <link>https://nryotaro.dev/posts/word_translation_without_parallel_data/</link>
      <pubDate>Sun, 21 May 2023 14:43:39 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/word_translation_without_parallel_data/</guid>
      <description>&lt;p&gt;対訳コーパスを使わずに、ある言語のエンベディングから別の言語のエンベディングへの写像を学習する。
はじめに、敵対的生成ネットワークで写像を学習する。
次に、出現頻度の高い単語について、写像されたエンベディングと目的言語のエンベディングにプロクラステス分析を適用し、より正確な写像関数を求める。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Skip Lists: A Probabilistic Alternative to Balanced Trees(1990)</title>
      <link>https://nryotaro.dev/posts/skip_lists_a_probablistic_alternative_to_balanced_trees/</link>
      <pubDate>Sat, 06 May 2023 15:03:48 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/skip_lists_a_probablistic_alternative_to_balanced_trees/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf&#34;&gt;Skip Lists&lt;/a&gt;は、バランス木の代用になるアルゴリズムである。
Skip listsのノードは、ランダムに選ばれた後続のノードへのポインタをもつ。
ランダムに参照するノードを選ぶことで、バランス木よりも単純なアルゴリズムで、計算量を均衡をでき、さらに最悪計算量をより小さくできる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE(2021)</title>
      <link>https://nryotaro.dev/posts/an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale/</link>
      <pubDate>Sat, 29 Apr 2023 12:12:11 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale/</guid>
      <description>&lt;p&gt;画像認識に&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Transformer&lt;/a&gt;を使う手法を提案し、&lt;a href=&#34;https://arxiv.org/abs/1912.11370&#34;&gt;Big Transfer&lt;/a&gt;と&lt;a href=&#34;https://arxiv.org/abs/1911.04252&#34;&gt;Noisy Student&lt;/a&gt;と比較した。
論文が発表された2021年でも、画像認識にニューラルネットワークを使う場合、畳込みニューラルネット(CNN)が基本の選択肢になる。
自己注意機構を使った画像処理の&lt;a href=&#34;https://arxiv.org/abs/2003.07853&#34;&gt;先行研究&lt;/a&gt;はあるが、スケールするアーキテクチャではない。&lt;/p&gt;
&lt;p&gt;AN IMAGE IS WORTH 16x16 WORDSは、分割した画像をトークン（単語）のようにTransformerへ入力することで、Transformerを画像認識へ応用できるこを示した。
TransformerはCNNのように画像の向きや局所性を帰納バイアスにもたず、データが不十分でないと汎化性能は低い。
しかし、学習データを14M-300Mまで増やすと、CNNを越える汎化性能を発揮した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Virtual Time and Global States of Distributed Systems(1988)</title>
      <link>https://nryotaro.dev/posts/virtual_time_and_global_states_of_distributed_systems/</link>
      <pubDate>Sat, 22 Apr 2023 21:35:08 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/virtual_time_and_global_states_of_distributed_systems/</guid>
      <description>&lt;p&gt;分散システムのプロセス間で時刻が常に同期しているとは限らない。
プロセスの時刻から判断すると、プロセスでは、ほかのプロセスのイベントと比べてどちらが先に起きたか分からないイベントが起きえる。
&lt;a href=&#34;https://lamport.azurewebsites.net/pubs/time-clocks.pdf&#34;&gt;Lamport&lt;/a&gt;は、各プロセスに単調増加する論理的な時刻をもたせ、メッセージとともに時刻をプロセス間で交換することで、イベントの依存関係と矛盾せずにイベントを全順序に並べる手法を提案した。
先行するイベントは、必ず後続のイベントよりも小さい時刻をもつ。
しかし、逆は必ずしも成り立たない。
先行する場合もあれば、前後関係がないこともある。
&lt;a href=&#34;http://www.vs.inf.ethz.ch/publ/papers/VirtTimeGlobStates.pdf&#34;&gt;Virtual Time and Global States of Distributed Systems&lt;/a&gt;は、各プロセスの時刻を、プロセス数とおなじ数の長さの配列で表現する。
これにより、時刻の前後関係が定義されていることがイベント間に前後関係があることの必要十分条件であることを可能にした。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generating Long Sequences with Sparse Transformers(2019)</title>
      <link>https://nryotaro.dev/posts/generating_long_sequences_with_sparse_transformers/</link>
      <pubDate>Sat, 15 Apr 2023 10:59:43 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/generating_long_sequences_with_sparse_transformers/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer&lt;/a&gt;のQKV注意機構に入力するベクトルを限定し、長さ\(n\)の系列をQKV注意機構に入力したときの空間計算量を\(\mathcal{O}(n\sqrt{n})\)まで減らした&lt;a href=&#34;https://arxiv.org/abs/1904.10509&#34;&gt;研究&lt;/a&gt;である。
Transformerであれば、系列の要素は、要素自体の位置と以前の要素すべてを注意し、時間と空間計算量は\(\mathcal{O}(n^2)\)になる。
Sparse Transformerは、\(p\)個のパターンを用意し、パターンに該当する要素のみを各注意機構に入力し、\(p\)個の注意を生成する。
そして、\(p\)個の注意を合成し、1つの注意に変換する。
パターンは、画像やテキストなど、入力するデータの種類によって決めておく規則であり、たとえば、直近にある一定数の要素や等間隔に離れた要素を指定するパターンがありえある。
パターンが\(p\)であれば、計算量は\(\mathcal{O}\sqrt[p]{n}\)になる。実験の設定は\(p=2\)である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Storing a Sparse Table with O(1) Worst Case Access Time(1984)</title>
      <link>https://nryotaro.dev/posts/storing_a_sparse_table_with_o1_worst_case_access_time/</link>
      <pubDate>Sat, 08 Apr 2023 12:38:44 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/storing_a_sparse_table_with_o1_worst_case_access_time/</guid>
      <description>&lt;p&gt;単射のハッシュ関数は完全である。完全ハッシュ関数により、衝突することのないハッシュテーブルのデータ構造と計算時間量の証明を示す。
データ構造は、はじめに、\(U(|U|=m)\)の部分集合\(S\)(\(|S|=n\))の要素\(x\)をハッシュテーブルに格納するとき、ある\(U\)の要素\(k\)を使った関数\(f(x)=(kx\mod p)\mod n\)で\(x\)を格納するブロック\(W_j(0\le j &amp;lt; n)\)を決める。
\(p\)は\(p=m+1\)の素数である。
次に\(U\)の要素\(k&#39;_j\)をもちいた関数\(g(x) = ((k&#39;_jx) \mod p)\mod |W_j|^2\)で、\(x\)のエントリを特定する。
データ構造の証明は、\(f, g\)によって重複なくエントリを特定できる\(k\), \(k&#39;_j\)があることを示す。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Language Models are Unsupervised Mmultitask Learners(2018)</title>
      <link>https://nryotaro.dev/posts/language_models_are_unsupervised_multitask_learners/</link>
      <pubDate>Sat, 01 Apr 2023 09:39:17 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/language_models_are_unsupervised_multitask_learners/</guid>
      <description>&lt;p&gt;Zero-shotかつマルチタスク用のモデルとして&lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;GPT-1&lt;/a&gt;の後経のGPT-2を提案した。
&lt;a href=&#34;https://www.semanticscholar.org/paper/Multitask-Learning-Caruana/161ffb54a3fdf0715b198bb57bd22f910242eb49&#34;&gt;マルチタスク学習&lt;/a&gt;は、複数のタスクむけにモデルを訓練する手法である。
特徴の入力形式はタスクによらず同じであり、タスク間で知識を補うことで各タスクの汎化性能を向上させる。
GPT-1の用途がファインチューニングであるため、GPT-1とGPT-2では解けるタスクが違う。
学習のために、45,000,000件のリンクを含む高品質なコーパスであるWebTextを人の手もかりて用意した。
GPT-2のアーキテクチャは、GPT-1に層正規化の位置を変え、residual layerの重みをスケールしただけであり、GPT-1と大きな違いはない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Proving the Correctness of Multiprocess Programs (1977)</title>
      <link>https://nryotaro.dev/posts/proving_correctness_of_multiprocess_programs/</link>
      <pubDate>Sat, 25 Mar 2023 12:34:08 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/proving_correctness_of_multiprocess_programs/</guid>
      <description>&lt;p&gt;マルチプロセスプログラムの正しさを証明するための公理を提案する。
正しさの条件は、プログラムが安全性と活性を満たすことである。
安全はプログラムが特定の状態になりえないことを、活性はプログラムが特定の状態に必ず到達することを意味する。
たとえば、キューにメッセージを配信するproducerとキューから取り出すconsumerがあるとする。
このとき、容量以上のメッセージがキューに蓄積しない性質が安全性に、キューが満杯時にconsumerがメッセージを消費する性質が活性になりえる。
プログラム、安全性、活性を形式化し、安全性と活性を証明することで、プログラムの正しさを示す。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Deep Reinforcement Learing from Human Preferences (2017)</title>
      <link>https://nryotaro.dev/posts/deep_reinforcement_learning_from_human_preferences/</link>
      <pubDate>Mon, 20 Mar 2023 11:54:48 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/deep_reinforcement_learning_from_human_preferences/</guid>
      <description>&lt;p&gt;エージェントの行動を撮影した2つのビデオクリップから良い方を人間に選ばせ、報酬関数の学習データを生成する。
テーブルを掃除するロボットの制御などは、報酬関数の設計が難しい。
そこで、2つのビデオクリップとその選好を1つのサンプルとする訓練データで、モデルに報酬関数を学習させる。&lt;/p&gt;
&lt;p&gt;環境の状態と状態下での行動のペアの系列を生成し、1秒から2秒間のビデオクリップとして記録する。
そして、人間に、2つのビデオクリップをうち良い方を選んでもらう。
2つのビデオクップのうち一方が他方よりも良い確率を出力できるようにモデルを訓練する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoder Regular Contest 049 B - 高橋ノルム君</title>
      <link>https://nryotaro.dev/posts/arc049b/</link>
      <pubDate>Wed, 15 Mar 2023 11:04:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/arc049b/</guid>
      <description>&lt;p&gt;XとYは互いに干渉しないので、独立してX, Yを考えることができる。
\(x_i, x_{i+1}\)の間に最適な\(X\)があるとすると、\(X\)の位置を\(x_i, x_{i+1}\)を境界として三分探索で求めることができる。
これをすべての\(i\)について試行すればいい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Your Coffee Shop Doesn&#39;t Use Two-Phase Commit(2005)</title>
      <link>https://nryotaro.dev/posts/your_coffee_shop_doesnt_use_two_phase_commit/</link>
      <pubDate>Sun, 12 Mar 2023 16:17:35 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/your_coffee_shop_doesnt_use_two_phase_commit/</guid>
      <description>&lt;p&gt;非同期処理を、スターバックスの注文からコーヒーの提供までの流れにたとえたアネクドートである。
注文をうけたレジの店員は、どの客の注文か分かる目印をコーヒーカップに書き、カップをエスプレッソマシンの上にならべる。
客はバリスタのいるカウンターに移動し、レジの店員は次の顧客の注文をうけつける。
バリスタは、ならべられたカップをとり、コーヒーを注ぎ、客に提供する。&lt;/p&gt;
&lt;p&gt;レジの店員とバリスタは非同期にはたらいている。
バリスタのコーヒーの提供が滞っても、レジの店員は注文をうけることができる。
カップの列が長くなれば、バリスタの人数を増やせば、レジの店員に影響することなく、より速くコーヒーを提供できる。
それはキューで通信するプロデューサーとコンシューマーのようである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Indexing by Latent Semantic Analysis (1990)</title>
      <link>https://nryotaro.dev/posts/indexing_by_latent_semantic_analysis/</link>
      <pubDate>Sat, 04 Mar 2023 18:27:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/indexing_by_latent_semantic_analysis/</guid>
      <description>&lt;p&gt;特異値分解を応用した潜在的な意味にもとづく文書検索の手法である。
文書を、単語の出現回数が成分の列ベクトルとしてあつかう。
その列ベクトルからなる文書集合の行列に特異値分解(Singular Value Decomposition, SVD)を適用する。
大きい順に\(k\)個の特異値とその特異ベクトルを選んで、低ランクの行列をつくり、もとの行列を近似する。
単語の数が\(t\), 文書数が\(d\)のとき、低ランクの行列の左特異ベクトルの行列\(T\)と右特異ベクトルの転置行列\(D&#39;\)のサイズは、それぞれ、\(t\times k\), \(k \times d\)になる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging(1992)</title>
      <link>https://nryotaro.dev/posts/aries/</link>
      <pubDate>Sun, 26 Feb 2023 01:47:13 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/aries/</guid>
      <description>&lt;p&gt;ARIES(Algorithm for Recovery and Isolation Exploiting Semantics)は、ログ先行書き込み(write-ahead-logging, WAL)によるSAVEPOINTまでの部分的なトランザクションのロールバック、レコード単位のロック、回復のためのアルゴリズムである。
ログの対象と粒度はページの更新であり、レコードには単調増加するLSN(Log Sequence number)をわりあてる。
レコードは直前の更新のレコードへのポインタであるPrevLSNも含み、あるレコードから以前の更新のレコードへ遡行できる。&lt;/p&gt;
&lt;p&gt;ロールバックやリスタート時の更新は、通常時の更新とは別に、保証ログレコード(compensation log record, CLR)に記録される。
ARIESのCLSに記録された更新はUNDOされない。
一方、IMS, AS/400, DB2のCLSは、あるトランザクションが何度もロールバックされると、対応するCLSの更新をUNDOすることがある。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
    <item>
      <title>Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections(1992)</title>
      <link>https://nryotaro.dev/posts/scatter_gather_a_cluster_based_approach_to_browsing_large_document_collections/</link>
      <pubDate>Sun, 19 Feb 2023 20:23:11 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/scatter_gather_a_cluster_based_approach_to_browsing_large_document_collections/</guid>
      <description>&lt;p&gt;Scatter/Gatherは、ユーザーにクエリを入力するかわりに、システムが提示する文書のクラスタをユーザーに選ばせる情報検索の手法である。
ユーザーが目的の情報にだどりつける適当なクエリをいつも作れるとはかぎらない。
ユーザーが詳しくない分野の情報を調べるときは、クエリにつかえる用語を知らないかもしれない。
また、ある分野の情報を広く浅く収集したいときは、クエリの単語によって検索対象を狭く限定したくないかもしれない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoder Regular Contest 028 B 特別賞</title>
      <link>https://nryotaro.dev/posts/arc028b/</link>
      <pubDate>Sun, 19 Feb 2023 10:56:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/arc028b/</guid>
      <description>&lt;p&gt;Pythonであればheap, C++であればsetを2つ使えばよい。
新しい要素を追加する度に、一方に上位kの要素、他方にk+1以降の要素があるように、2つのset, heap間で要素を交換する。
類似の問題をLeetCodeのHard modeで見かけたことがある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zookeeper: Wait-free coordination for Internet-scale systems (2010)</title>
      <link>https://nryotaro.dev/posts/zookeeper_wait_free_coordination_for_internat_scale_systems/</link>
      <pubDate>Sun, 12 Feb 2023 15:47:06 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/zookeeper_wait_free_coordination_for_internat_scale_systems/</guid>
      <description>&lt;p&gt;ZooKeeperは、リーダ選出と構成管理の機能を提供するためのインメモリーデータベースである。
データーベースは、ファイルシステムのような階層構造のwait-freeな抽象データ型であり、各ノードにはデフォルトで最大1MBの少量のデータを保存できる。
たとえば、ノードに設定を保存して共有することも、設定ファイルが編集中でないことを示すフラグ代わりのファイルを置くこともできる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoder Regular Contest 007 C - 節約生活</title>
      <link>https://nryotaro.dev/posts/arc007/</link>
      <pubDate>Sat, 11 Feb 2023 11:04:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/arc007/</guid>
      <description>&lt;p&gt;文字数を\(n\)とすると複数のテレビを組合せたときの視聴パターンの重ね方は\(2^n\)個ある。
最大文字数は整数型のビット数より数分の1しかないので、ビット操作ですべての重ね方を試行することを考える。
各重ね方について、パターンを十分に連結した値の論理和に最大文字数以上連続する1があれば、その試行は要件を満たす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ARC 005C 器物損壊！高橋君</title>
      <link>https://nryotaro.dev/posts/arc005/</link>
      <pubDate>Tue, 07 Feb 2023 11:04:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/arc005/</guid>
      <description>&lt;p&gt;ダイクストラ法を使えば、スタート地点から各区画への最短距離を求められる。
塀に移動にかかる時間を十分に大きい値、かりに\(HM+1\)として、ダイクストラ法適用すると、
ゴール地点が\(3(HM+1)\)以下未満であり、そのときに限り、塀を壊す回数が2回以下でゴール地点に到達できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>From RankNet to LambdaRank to LambdaMART: An Overview(2010)</title>
      <link>https://nryotaro.dev/posts/from_ranknet_to_lambdarank_to_lambdamart_an_overview/</link>
      <pubDate>Fri, 03 Feb 2023 16:55:53 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/from_ranknet_to_lambdarank_to_lambdamart_an_overview/</guid>
      <description>&lt;p&gt;LambdaMARTは、勾配ブースティング決定木(MART)の訓練に、&lt;a href=&#34;https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf&#34;&gt;RankNet&lt;/a&gt;と&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf&#34;&gt;LambdaRank&lt;/a&gt;を応用したランキング学習である。
RankNetとLambdaRankを以前抄訳で解説した(&lt;a href=&#34;https://nryotaro.dev/posts/learning_to_rank_using_gradient_descent/&#34;&gt;RankNet&lt;/a&gt;, &lt;a href=&#34;https://nryotaro.dev/posts/learning_to_rank_with_nonsmooth_cost_functions/&#34;&gt;LambdaRank&lt;/a&gt;)。
RankNetは、名前にNetがついているが、RankNetの論文で提案されたものは、ネットワークアーキテクチャではなくランキング学習のための損失関数である。
入力されたベクトルをモデルが実数に写像することができれば、ニューラルネットワークでなくてもよい。
RankNetの学習は、モデルの出力する実数をスコアとみなし、あるクエリに該当する2つの事例のうち、より該当する事例に高いスコアを与えられるようにモデルを訓練する。
各訓練データはランキングの異なる2つの特徴とランキングの高い方を示すラベルであり、2つのスコアの差をシグモイドに与えたときの出力を、一方のサンプルが他方よりもランキングが高い確率とみなす。
そして、交差エントロピーが最小になるようにモデルを訓練する。
LambdaRankは、損失の計算を省き、スコアと損失の勾配で重みを更新することで、RankNetの学習を高速化する。
LambdaMARTは、RankNetに勾配ブースティング決定木を使い、残差の計算にLambdaRankを応用したランキング学習のモデルである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Principles of Transaction-Oriented Database Recovery(1983)</title>
      <link>https://nryotaro.dev/posts/principles_of_transaction_oriented_database_recovery/</link>
      <pubDate>Sun, 29 Jan 2023 15:14:29 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/principles_of_transaction_oriented_database_recovery/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;クラッシュしたRDMSのディスクとバッファの状態をクラッシュ前の状態にもどす手法を体系化する。
体系は、Propagation, Buffer handling, EOT Processing, Checkpointingの4要素に着目して技術を分類する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Learning to Rank with Nonsmooth Cost Functions (2006)</title>
      <link>https://nryotaro.dev/posts/learning_to_rank_with_nonsmooth_cost_functions/</link>
      <pubDate>Mon, 23 Jan 2023 16:23:52 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_to_rank_with_nonsmooth_cost_functions/</guid>
      <description>&lt;p&gt;情報検索の指標は、モデルの返す文書の順序を評価する。
指標の関数自体を損失関数にできれば、重みの更新を指標に最適化できる。
ところが、文書の順序を評価する指標は、重みによる微分が未定義や0になりえるので、損失関数には使えない。
LambdaRankは損失関数に直接は使えない関数を学習に応用するアルゴリズムである。
RankNetをLambdaRankで学習することで、学習時間を短縮し、NDCGを向上できた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Brewer&#39;s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web(2002)</title>
      <link>https://nryotaro.dev/posts/brewers_conjecture_and_the_feasibility_of_consistent_available_partition_tolerant_web/</link>
      <pubDate>Tue, 17 Jan 2023 11:42:12 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/brewers_conjecture_and_the_feasibility_of_consistent_available_partition_tolerant_web/</guid>
      <description>&lt;p&gt;分散システムは一貫性、可用性、分断耐性を同時に満たすことができない。
この予想は、2000年のPODCでBrewerが発表したCAP定理として知られている。
しかし、Brewerの&lt;a href=&#34;https://sites.cs.ucsb.edu/~rich/class/cs293-cloud/papers/Brewer_podc_keynote_2000.pdf&#34;&gt;講演&lt;/a&gt;では厳密な定義や証明はない。
そこで、それを補うために、ある2つの計算モデルでCAPが成立することを証明する。
計算モデルは、asynchronous network modelとpartially synchhronous modelである。
asynchronouse network modelでは、モデルのノードは、クロックをもたず、受信したメッセージとノード内の計算のみで出力を決定する。
partially synchronous modelでは、各ノードは、一定間隔で単調増加するクロックをもち、処理をタイムアウトしたり、スケジューリングしたりできる。
ただし、ノード間でクロックが同期されているとは限らない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Learning to Rank using Gradient Descent (2005)</title>
      <link>https://nryotaro.dev/posts/learning_to_rank_using_gradient_descent/</link>
      <pubDate>Sat, 07 Jan 2023 20:59:34 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_to_rank_using_gradient_descent/</guid>
      <description>&lt;p&gt;ランキング学習のニューラルネットワークRankNetを提案する。
RankNetは、ベクトルから実数を出力し、その出力が大きいほどランキングが高いと予測する。
損失関数には、ある2つのベクトルのうち片方が他方よりもランキングが高い確率をあたえる。
確率とみなす値は、両ベクトルのモデルの出力の差をシグモイド関数に入力したときの出力である。
RankNetの学習は、モデルの出力と訓練データの確率分布の交差エントロピーを最小化する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 PROGRAMMING WITH ABSTRACT DATA TYPES (1974)</title>
      <link>https://nryotaro.dev/posts/programming_with_abstract_data_types/</link>
      <pubDate>Wed, 28 Dec 2022 12:52:36 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/programming_with_abstract_data_types/</guid>
      <description>&lt;p&gt;高水準言語は、実装の詳細を意識せずに使える操作、データ構造、while文などの制御構造を提供する。
この3つを抽象とよぶ。
プログラミング言語から提供される抽象だけではプログラムを実装できないので、開発者は、足りない抽象を実装しなければならない。
抽象データ型は、言語から提供される抽象を組合せて作るデータ構造である。
抽象データ型に対する操作の実装は外部に公開されず、抽象データ型の特徴は抽象データ型に可能な操作によって決まる。
構造化プログラミングと抽象データ型を提供するプログラミング言語を開発し、抽象データ型を使うプログラムのソースコードを例示することで、ほとんどの処理が抽象データ型だけで実装可能なことを示唆した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 A taxonomy of web search(2002)</title>
      <link>https://nryotaro.dev/posts/a_taxonomy_of_web_search/</link>
      <pubDate>Fri, 23 Dec 2022 22:44:46 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_taxonomy_of_web_search/</guid>
      <description>&lt;p&gt;Web検索をnavigational, informational, transactionalの3つに分類する。
Navigationalは、ユーザーに訪問したい特定のサイトがあり、そのサイトを開くための検索である。
Informationalは、ユーザーに収集したい情報があり、その情報があると思われるサイトを検索する。
情報が断片的であれば、複数のサイトを訪問することもある。
Transactionalは、検索や種々のダウンロードなどインタラクションを要するサイトを訪問するための検索である。
ユーザーは、最終的な目的の情報のために訪問先のサイトでさらに検索をする。
たとえばECサイトへの検索が該当する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Epidemic Algorithm for Replicated Database Maintenance(1987)</title>
      <link>https://nryotaro.dev/posts/epidemic_algorithms_for_replicated_database_maintenance/</link>
      <pubDate>Mon, 19 Dec 2022 11:07:49 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/epidemic_algorithms_for_replicated_database_maintenance/</guid>
      <description>&lt;p&gt;Epidemic Algorithmは、データベース間の差分を解消するために、データベースがランダムに選んだ別のデータベースと同期する手法である。
Epidemicを邦訳すると伝染性である。
ここではデータベースにはマスタースレーブの区別はなく、任意のサーバーがレコードの変更リクエストを受理できる。
Epidemic Algorithmには、定期的にデーターベース全体を同期する手法と、変更を受理したときに当該の変更のみを別のデータベースに伝搬させる手法の2つがある。
前者は、データベース全体を同期するので変更のとりこぼしがいずれは解消されるが、同期に時間がかかる。
後者は、時間はかからないが、伝搬を終えるまでに同期できなかった変更があれば、その齟齬が残りつづける。
データベースがある時点で最新の変更をもたない確率を仮定し、その確率で次の同期で変更を取得できる確率を表し、手法間の同期する収束の速さを比較した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Syntactic Clustering of the Web(1997)</title>
      <link>https://nryotaro.dev/posts/syntactic_clustering_of_the_web/</link>
      <pubDate>Tue, 13 Dec 2022 13:38:46 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/syntactic_clustering_of_the_web/</guid>
      <description>&lt;p&gt;2つのWeb文書から抽出したn-gramの集合を比較し、文書の包含関係と類似度を測定する。
はじめに、大文字をすべて小文字にしたり、htmlタグを除外したりして文書に前処理をかける。
次に文書\(A\)からnグラムの集合\(S(A)\)を抽出する。
このとき、文書\(A\)と\(B\)間の類似度を\(|S(A)\cap S(B)|/|S(A) \cup S(B)|\)、\(A\)の\(B\)への包含の度合いを\(|S(A) \cap S(B)|/|S(A)|\)と定義する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Improved Data Stream Summary: The Count-Min Sketch and its Applications(2005)</title>
      <link>https://nryotaro.dev/posts/an_improved_data_stream_summary_the_count_min_sketch_and_its_application/</link>
      <pubDate>Wed, 07 Dec 2022 12:29:36 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_improved_data_stream_summary_the_count_min_sketch_and_its_application/</guid>
      <description>&lt;p&gt;Count-Min Sketch(CM Sketch)は、単位時間ごとに1要素ずつ更新される\(n\)次元のベクトルの要素を\(n\)の劣線形の計算量で集計できるデータ構造である。
空間の大きさはパラメタで指定する誤差大きさと誤差の生じる確率に依存する。
誤差と確率のパラメタを\(\epsilon\)と\(\delta\)とすると、CM sketchは\(\lceil{\ln \frac{1}{\delta}\rceil}\times \lceil\frac{e}{\epsilon}\rceil\)の二次元行列である。
\(\ln\)の底は\(e\)である。
要素の値、ベクトルの内積、ある範囲の要素の和を劣線形の計算量で求めることができる。
また、これらと二分探索を応用すれば、指定した値よりも大きい要素のみからなる範囲や分位数も計算できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 LOF: Identifying Density-Based Local Outliers(2000)</title>
      <link>https://nryotaro.dev/posts/lof_identifying_density_based_local_outliners/</link>
      <pubDate>Mon, 28 Nov 2022 12:22:20 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/lof_identifying_density_based_local_outliners/</guid>
      <description>&lt;p&gt;局所外れ値因子法(LOF)は、あるインスタンスが外れ値であるかを、その周囲のインスタンスとの距離から判定する。
異常検知の手法の中には異常か正常かの二値を出力するものがあるが、LOFは[0,1]区間の値を出力する。
1に近いほど正常とみなす。
要素間の距離が近くて密度の高いクラスタと低いクラスタがあり、密度の高いクラスタの少し遠くに外れ値とみなすべきインスタンスがあるとする。
このとき、密度の低いクラスタのインスタンス間の距離を基準にすると、密度の高いクラスタのインスタンスと外れ値のインスタンス間の距離は大きいとみなせない。&lt;/p&gt;
&lt;p&gt;他方で、外れ値のインスタンスのそばの密度の高いクラスタの距離を基準にすれば、外れ値のインスタンスは周囲のインスタンスから遠いとみなせる。
このように、密度の違うクラスタがあるデータセットでも、インスタンスに近いサンプル間の距離を基準にすることで、他のクラスタの密度で測れば正常なインタンスと判定される外れ値を検出できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MillWheel: Fault-Tolerant Stream Processing at Internet Scale(2013)</title>
      <link>https://nryotaro.dev/posts/millwheel_fault_tolerant_stream_processing_at_internet_scale/</link>
      <pubDate>Wed, 23 Nov 2022 11:44:08 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/millwheel_fault_tolerant_stream_processing_at_internet_scale/</guid>
      <description>&lt;p&gt;MillWheelはGoogleで開発されたストリーミング処理のフレームワークである。
開発者が羃等な処理をノードとする有向グラフを実装すれば、MillWheelがデータをノードに正確に一回だけ配信する。
ただし、転送が遅延しているデータは破棄される。
データは、キー、値、論理的な時刻の3組からなるレコードを単位として、ノードからノードに出力される。
向き先のノードは、レコードからキーへの関数を、根のノードから出力されるレコードに適用し、期待するキーに対応するレコードをノードに集約する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 RUSBoost: A Hybrid Approach to Alleviating Class Imbalance(2010)</title>
      <link>https://nryotaro.dev/posts/rusboost_a_hybrid_approach_to_alleviating_class_imbalance/</link>
      <pubDate>Sun, 13 Nov 2022 12:18:12 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/rusboost_a_hybrid_approach_to_alleviating_class_imbalance/</guid>
      <description>&lt;p&gt;RUSBoostは、random undersampling(RUS)とAdaBoostを応用し、不均衡データの予測性能を改善する。
類似の先行研究である&lt;a href=&#34;http://www.datascienceassn.org/sites/default/files/SMOTEBoost%20Improving%20Prediction%20of%20the%20Minority%20Class%20in%20Boosting%20.pdf&#34;&gt;SMOTEBoost&lt;/a&gt;よりも学習時間が短く、アルゴリズムが単純である。
アンダーサンプリングで学習データを減らせば、学習時間を短くできるが、学習器に与える情報が減る。
一方、AdaBoostは、前に訓練した弱学習器が誤分類したデータに高い重みを与え、次の弱学習器を学習し、予測性能を上げる。
AdaBoostを借りることで、単分類の難しい少数クラスのサンプルに高い重みを単純に与えられる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 GraphChi: Large-Scale Graph Computation on Just a PC(2012)</title>
      <link>https://nryotaro.dev/posts/graphchi_large_scale_graph_computation_on_just_a_pc/</link>
      <pubDate>Sat, 12 Nov 2022 09:57:01 -0500</pubDate>
      
      <guid>https://nryotaro.dev/posts/graphchi_large_scale_graph_computation_on_just_a_pc/</guid>
      <description>&lt;p&gt;GraphChiは、商業規模の有向グラフをコンシューマPCで計算できるとうたうデータ構造とプログラミングモデルである。
そのためには、任意のひとつのノードとそのノードに接続する全てのエッジを読み込めるメモリがあればよい。
順序つきのノードを互いに素なP個の集合に分け、それぞれをintervalをよぶ。
interval内のノードに向うエッジを根のノード順にソートし、エッジをP個のshardに分けて保存する。
1つのshardをディスクの連続領域に保存することで、あるノードとノードに接続するエッジを、高々P回のディスクへのアクセスでメモリに読み込める。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Popular Ensemble Methods: An Empirical Study(1999)</title>
      <link>https://nryotaro.dev/posts/popular_ensemble_methods_an_empirical_study/</link>
      <pubDate>Sat, 05 Nov 2022 10:19:41 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/popular_ensemble_methods_an_empirical_study/</guid>
      <description>&lt;p&gt;アンサンブル学習は、弱学習器を組みわせることで、より予測性能の高いモデルを生成する。
弱学習器にニューラルネットワークと決定木を使い、バギングとブースティングの予測性能を、23件のデータで測定した。
バギングはほぼすべての場合に1つの学習器より高精度だったが、まれにブースティングよりも著しく性能が低かった。
他方で、ブースティングは、単体の学習器よりも性能が低いことがあった。
特にニューラルネットワークを弱学習器にした場合に逆転の傾向がみられた。
ブースティグは、既存の弱学習器が誤答したサンプルを回答できる弱学習器を生成する。
&lt;a href=&#34;https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf&#34;&gt;Freundら&lt;/a&gt;は、データにノイズが多いと、学習の後半になるほどノイズを学習するので、性能が悪くなると示唆している。
データにノイズを混ぜ、その量とブースティングの精度を比較することで、ノイズが多いほどブースティングの性能がバギングと比べたときよりも悪くなることを確認した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 DEFINING LIVENESS(1985)</title>
      <link>https://nryotaro.dev/posts/defining_liveness/</link>
      <pubDate>Sat, 29 Oct 2022 09:04:26 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/defining_liveness/</guid>
      <description>&lt;p&gt;L.Lamportは、&lt;a href=&#34;https://lamport.azurewebsites.net/pubs/proving.pdf&#34;&gt;Proving the Correctness of Multiprocess Programs&lt;/a&gt;で、安全性と活性を導入し、並行プログラミングの正しさを議論した。
そこでは、安全性は、実行中に「よくないこと」が起きない性質であり、活性は「よいこと」が起きる性質である。
後に、Lamportは安全性を形式的に&lt;a href=&#34;https://link.springer.com/book/10.1007/3-540-15216-4#toc&#34;&gt;定義&lt;/a&gt;したが、活性には与えていない。
そこで、DEFINING LIVENESSは、並行プログラムの実行を状態系列とみなし、活性を形式的な定義した。
プログラムの一部である任意の有限長系列\(\alpha\)について、\(\alpha\)に後続する無限長の系列を\(\beta\)とするとき、性質\(P\)をみたす連結\(\alpha\beta\)が存在し、また、そのときに限り、\(\alpha\)は\(P\)の活性がある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning(2005)</title>
      <link>https://nryotaro.dev/posts/borderline-smote_a_new_over-sampling_method_inbalanced_data_sets_learning/</link>
      <pubDate>Sat, 22 Oct 2022 09:42:58 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/borderline-smote_a_new_over-sampling_method_inbalanced_data_sets_learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1106.1813.pdf&#34;&gt;SMOTE&lt;/a&gt;は不均衡データに対するオーバーサンプリングを使う手法である。
少数側のクラスのサンプルと近傍のサンプルから少数クラスのサンプルを合成する。
SMOTEはすべての少数クラスのサンプルが合成の材料になりえる。
対して、borderline-SMOTE1とborderline-SMOTE2は、境界から遠い少数クラスのサンプルを使わず、近いサンプルのみから合成する。
borderline-SMOTE1は合成の相手側として常に少数クラスのサンプルを選ぶ。
borderline-SMOTE2は多数クラスのサンプルを選ぶこともある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals(1997)</title>
      <link>https://nryotaro.dev/posts/data_cube_a_relational_aggregation_operator_generalizing_group_by_cross_tab_and_sub_totals/</link>
      <pubDate>Sat, 15 Oct 2022 10:28:20 -0400</pubDate>
      
      <guid>https://nryotaro.dev/posts/data_cube_a_relational_aggregation_operator_generalizing_group_by_cross_tab_and_sub_totals/</guid>
      <description>&lt;p&gt;SQLのGROUP BYと集約関数を組み合わせて0次元や1次元の値を出力できる。
一方、ピボットテーブルのように、ある属性値の同じレコードの別の属性値を集計し、さらに集計値の集計値を求めたいことがある。
(DATA) CUBEは、GROUP BYを多次元に拡張した演算子であり、ヒストグラム、ピボットテーブル、ロールアップ、ドリルダウンの計算に使える。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning(2008)</title>
      <link>https://nryotaro.dev/posts/adasyn_adaptive_syntetic_samping_approach_for_imbalanced_learning/</link>
      <pubDate>Sun, 11 Sep 2022 11:33:21 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/adasyn_adaptive_syntetic_samping_approach_for_imbalanced_learning/</guid>
      <description>&lt;p&gt;ADASYNはオーバーサンプリングで不均衡データの予測性能を向上させる。
以前&lt;a href=&#34;https://nryotaro.dev/smote_synthetic_minority_over_sampling_technique/&#34;&gt;抄訳&lt;/a&gt;した&lt;a href=&#34;https://arxiv.org/pdf/1106.1813.pdf&#34;&gt;SMOTE&lt;/a&gt;とおなじく、既存の学習データからサンプルを合成する。
SMOTEとの違いは、K近傍に多数クラスのサンプルの多い少数クラスのサンプルから、より多くのサンプルを合成する点にある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Sagas(1987)</title>
      <link>https://nryotaro.dev/posts/sagas/</link>
      <pubDate>Sun, 04 Sep 2022 00:45:37 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/sagas/</guid>
      <description>&lt;p&gt;データベースのトランザクションが長くなるほど、ほかのトランザクションを待たせてしまう。
トランザクションの時間は短いほどよい。
トランザクションを短い複数のトランザクションに分割でき、そのトランザクションの間にほかのトランザクションを実行できるなら、そのトランザクションをsagaと呼ぶ。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 SMOTE: Synthetic Minority Over-sampling Technique(2002)</title>
      <link>https://nryotaro.dev/posts/smote_synthetic_minority_over_sampling_technique/</link>
      <pubDate>Wed, 31 Aug 2022 19:07:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/smote_synthetic_minority_over_sampling_technique/</guid>
      <description>&lt;p&gt;SMOTEはオーバーサンプリングで不均衡データの予測性能の向上をはかる。
少数クラスのサンプルからk近傍にあるサンプルのうち1つをランダムに選ぶ。
もとのサンプルと選ばれたサンプルの各特徴の差に[0,1]区間のランダムな値を掛け、その値をもとのサンプルに足して、少数クラスのサンプルを合成する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Kafka: a Distributed Messaging System for Log Processing (2011)</title>
      <link>https://nryotaro.dev/posts/kafka_a_distributed_messaging_system_for_log_processing/</link>
      <pubDate>Mon, 22 Aug 2022 19:30:19 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/kafka_a_distributed_messaging_system_for_log_processing/</guid>
      <description>&lt;p&gt;Kafkaは、LinkedInで開発された分散メッセージングサービスである。
配信モデルはpullベースであり、consumerが、メッセージをリクエストし、メッセージを消費するスループットを制御する。
consumerは、関心のあるメッセージのストリームであるトピックを作る。
producerは、トピックつきのメッセージをborkerに送り、brokerがメッセージを保持する。
consumerは、brokerのトピックからメッセージを取得する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Pregel: A System for Large-Scale Graph Processing(2010)</title>
      <link>https://nryotaro.dev/posts/pregel_a_system_for_large_scale_graph_processing/</link>
      <pubDate>Fri, 19 Aug 2022 23:47:30 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/pregel_a_system_for_large_scale_graph_processing/</guid>
      <description>&lt;p&gt;Googleで開発されたPregelは、ソーシャルグラフやハイパーリンクのグラフなどの巨大なグラフデータの処理に適した計算モデルである。
計算モデル自体も、処理対象のデータとおなじく、ワーカーが別のワーカーとのメッセージの送受信を繰り返すグラフ構造をなす。
このモデルのオリジナルは、以前&lt;a href=&#34;https://nryotaro.dev/posts/a_bridging_model_for_parallel_computation/&#34;&gt;紹介&lt;/a&gt;したValiantの&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/79173.79181&#34;&gt;Bulk Synchronous Parallel model&lt;/a&gt;(BPS)である。
BPSは時間を一定間隔に区切り、次の間隔が始まる時点で、それまでのメッセージの送受信が全て成功していることを保証する。
間隔の中で、コンポーネントは別のコンポーネントにメッセージを送信する。
間隔を過ぎたとき、失敗した処理や未送信のあるコンポーネントがあれば、成功するまで計算を再実行させる。
Pregelでは、この間隔をsuperstepとよび、グラフ上の点であるワーカーがコンポーネントにあたる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Normalized Cuts and Image Segmentation (2000)</title>
      <link>https://nryotaro.dev/posts/normalized_cuts_and_image_segmentation/</link>
      <pubDate>Sat, 06 Aug 2022 15:03:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/normalized_cuts_and_image_segmentation/</guid>
      <description>&lt;p&gt;単純無向グラフのクラスタリングを応用して画像をセグメントに分割する。
カットの評価基準としてnormalized cutとその近似解を求め方を提案した。
Normalized cutが最小値のとき、異なるサブグラフにあるノード間の類似度は最小に、同じサブグラフにあるノードの類似度は最大になる。
Normalized cutをレイリー商に変形し、固有値問題をとくことで、Normalized cutの最小値の近似解を求める。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding diagnostic tests3: receiver operating characteristic curves(2007)</title>
      <link>https://nryotaro.dev/posts/understanding_diagnostic_tests3/</link>
      <pubDate>Sat, 06 Aug 2022 02:20:21 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/understanding_diagnostic_tests3/</guid>
      <description>&lt;p&gt;医学では陰性と陽性を区別する分割点をカットオフポイントと呼ぶ。
ROC曲線は、縦軸を感度(sensitivity)TP/(FN+TP), 横軸を1-特異度(specificity)1 - TN/(TN+FP)とするグラフで、この2値はトレードオフの関係にある。
トレードオフのもとでカットオフポイントの決める方法を先行研究から2つ紹介している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 The Transaction Concept: Virtues and Limitations(1981)</title>
      <link>https://nryotaro.dev/posts/the_transaction_concept/</link>
      <pubDate>Sat, 30 Jul 2022 16:29:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_transaction_concept/</guid>
      <description>&lt;p&gt;トランザクションのコミットとロールバックの2通りの実装方針を提案している。
1つは、オブジェクトの値とその値の有効期間を記録し、値の履歴を管理する。
もうひとつは、ログとロックを利用した実装で、操作時にUNDOログとREDOログを出力する。
ロールバック時には、ログをもとにコミット開始前の状態にシステムを復元する。
UNDOやREDOを再実行可能でなければならず、そのためにオブジェクトにしばしばバージョンをつけることがある。
2つの実装方針は異なるように見えるが、内部の実装は似かよったものになる。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
    <item>
      <title>On Spectral Clustering: Analysis and an algorithm</title>
      <link>https://nryotaro.dev/posts/on_spectral_clustering_analysis_and_an_algorithm/</link>
      <pubDate>Sat, 23 Jul 2022 14:06:59 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/on_spectral_clustering_analysis_and_an_algorithm/</guid>
      <description>&lt;p&gt;スペクトラルクラスタリングは、類似度を辺の重みとするグラフをデータから構築し、重みをもとにカットする辺を決めて、データをクラスタに分ける。
グラフラプラシアンのようなグラフを表現した行列をつくり、その固有ベクトルを求め、固有ベクトルを列ベクトルとして並べた行列にK-meansを適用する。
もとのデータに二重丸のような凸集合でないクラスタがあるときに、最初からK-meansを適用するよりも良い結果を期待できる。
また、データ分布に確率モデルにあてはめ、どのクラスタにデータを割りあてるか確率的に決めるクラスタリングと違い、データの分布を仮定しなくてよい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs(1979)</title>
      <link>https://nryotaro.dev/posts/how_to_make_a_multiprocessor_computer_that_correctly_executes_multiprocess_programs/</link>
      <pubDate>Sat, 16 Jul 2022 16:07:20 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/how_to_make_a_multiprocessor_computer_that_correctly_executes_multiprocess_programs/</guid>
      <description>&lt;p&gt;プロセッサは、高速化のためにソースコードと違う順序で命令を実行することがある。
順序の入れ換えが実行結果に影響しなければ、プロセッサは逐次的であるという。
マルチプロセッサに期待する実行結果は、各プロセッサがソースコードの順序で命令を実行し、かつ、全プロセッサの命令がある順序で逐次的に実行されるときの結果である。
任意のプログラムで実行結果が等しいならば、そのマルチプロセッサには逐次一貫性がある。
プロセッサがメモリ上のデータを共有する場合、各プロセッサが逐次的でも、マルチプロセッサが逐次一貫になるとは限らない。
マルチプロセッサが逐次一貫であるには、各プロセッサがソースコード通りの順序でメモリにリクエストを送り、メモリは各セルへのリクエストを到着順(FIFO)に処理しなければならない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Min-max Cut Algorithm for Graph Partitioning and Data Clustering</title>
      <link>https://nryotaro.dev/posts/a_min-max_cut_algorithm_for_graph_partitioning_and_data_clustering/</link>
      <pubDate>Sat, 09 Jul 2022 17:11:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_min-max_cut_algorithm_for_graph_partitioning_and_data_clustering/</guid>
      <description>&lt;p&gt;重みつき隣接行列で表した無向グラフを2分割するクラスタリングであり、グラフ間の類似を最小化し、グラフ内のノードの類似の最大化をはかる。
隣接行列を\(W_{uv}\), \(\text{cut}(A, B)=W(A, B)\), \(W(A, B) = \sum_{u\in A, v\in B}W_{uv}\)として、以下の目的関数を最小化する分割を探索する。
ノードを2つに分ける最適解はNP完全であるため、近似解をもとめる。
$$
\text{Mcut} = \frac{\text{cut}(A, B)}{W(A)} + \frac{\text{cut}(A, B)}{W(B)}
$$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial</title>
      <link>https://nryotaro.dev/posts/implementing_fault-tolerant_services_using_the_state_machine_approach_a_tutorial/</link>
      <pubDate>Sun, 26 Jun 2022 11:27:52 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/implementing_fault-tolerant_services_using_the_state_machine_approach_a_tutorial/</guid>
      <description>&lt;p&gt;サーバのレプリケーションによってフォールトトレランスを高めることができるが、レプリケーション間の合意形成のプロトコルが必要になる。
そこで、ステートマシンをモデルにプロトコルを定義する手法を、Fail Stopとビザンチン将軍問題で例示する。
ステートマシンは、状態変数を決定的に変更するコマンドからなり、入力系列のみから出力が決まる。
モデルは、複数のステートマシンを同じ初期状態から開始し、同じ入力を与え、出力の合意をはかる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LEAST ANGLE REGRESSION</title>
      <link>https://nryotaro.dev/posts/least_angle_regression/</link>
      <pubDate>Sat, 25 Jun 2022 15:35:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/least_angle_regression/</guid>
      <description>Least Angle Regression(LARS)はLASSO回帰のアルゴリズムで、目的変数の値とモデルの推定値の残差と相関の大きい入力変数をひとつずつモデルに取り込む。 ベクトル間の相関が大きいほど間の角度が小さくなるため、least angle(最小角)と名がつく。
初期状態のモデルは説明変数をもたず推定値は0である。 そのため、目的変数との相関が最大の説明変数を最初にモデルに取り込む。 次に、別の説明変数と残差の相関が取り込んだ説明変数の相関と等しくなるまで、取り込んだ説明変数の方向に、出力する推定値を更新する。 以降は、取り込んだすべての説明変数から等角度の方向に、取り込んでいない説明変数と残差の相関が取り込んだ説明変数との相関と等しくなるまで、モデルの推定値を更新する。
説明変数\(\boldsymbol{x}_1\), \(\boldsymbol{x}_2\)と目的変数\(\boldsymbol{y}\)で更新の様子を説明する。 初期状態の推定値\(\hat{\boldsymbol{\eta}}_0\)では、\(\boldsymbol{x}_2\)よりも\(\boldsymbol{x}_1\)のほうが\(\boldsymbol{y}-\boldsymbol{\hat{\eta}}_0\)と相関がある。そこで、\(\boldsymbol{x}_1\)を最初にモデルに取り込む。 \(\boldsymbol{x}_1\)の方向に\(\boldsymbol{\hat{\eta}_1}\)まで更新すると、\(\boldsymbol{x}_1\)と\(\boldsymbol{x}_2\)は、残差\(\boldsymbol{y}-\boldsymbol{\hat{\eta}}_1\)との相関が等しくなる。 そして今度は、\(\boldsymbol{x}_1\)と\(\boldsymbol{x}_2\)と等角度の方向に\(\boldsymbol{\eta}_2\)まで更新する。
論文へのリンク
引用元の画像が荒いため、画像は、スモールデータ解析と機械学習から引用しました。</description>
    </item>
    
    <item>
      <title>Consensus in the Presence of Patial Synchrony (1988)</title>
      <link>https://nryotaro.dev/posts/consensus_in_the_presence_of_partial_synchrony/</link>
      <pubDate>Sat, 18 Jun 2022 15:46:42 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/consensus_in_the_presence_of_partial_synchrony/</guid>
      <description>&lt;p&gt;同期的なシステムには、プロセッサ間のメッセージの到達時間や時刻の誤差に有界がある。
非同期的なシステムには、どちらの有界もない。
表題のpartial synchronousは、同期、非同期の中間的な環境である。
到達時間や時刻の誤差の有界について、存在するが値が分からない環境と、値が分かっているがどの時刻から値が保証されるか分からない環境を意味する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Regression Shrinkage and Selection via the Lasso</title>
      <link>https://nryotaro.dev/posts/regression_shrinkage_and_selection_via_the_lasso/</link>
      <pubDate>Sun, 12 Jun 2022 10:52:14 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/regression_shrinkage_and_selection_via_the_lasso/</guid>
      <description>&lt;p&gt;Lasso(least absolute shrinkage and selection operator)を提案した論文で、特定の回帰係数を0にする正則化項により、モデルを解釈しやすくする。
Subset selectionはデータの変化に敏感で、学習結果が安定しない。
Ridge回帰は、回帰係数を全体的に小さくすることで正則化項の制約をみたすため、説明変数を減らしてモデルを解釈しやすくはしない。
線形回帰のモデルから人工的に生成し、Lasso回帰、Subset selection, Ridge回帰を比較した。
結果、もとのモデルの少数から中程度の回帰係数の絶対値が小さく残りが0のときに、Lasso回帰の精度が最も高かった。
絶対値の大きい少数の回帰係数と0の回帰係数のときはSubset selectionが、絶対値の小さい回帰係数が多いときにRidge回帰の精度が最も高かった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>C-Store: A Column-oriented DBMS(2005)</title>
      <link>https://nryotaro.dev/posts/c-store_a_column_oriented_dbms/</link>
      <pubDate>Sat, 04 Jun 2022 13:20:16 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/c-store_a_column_oriented_dbms/</guid>
      <description>&lt;p&gt;C-Storeは、OLAP系の読み込みクエリに特化した列指向データベースで、標準のSQLでのデータへのアクセスをサポートする。
高速化のために、テーブルのデータを、ある列の値でソートした複数の投影として保存する。
読み込みクエリのSELECT句やソートに合致する投影があれば処理を高速化でき、さらに可用性も向上する。
しかし、同じ列を重複して複数の投影に含める場合、必要なディスク容量も増える。
そのため、C-Storeは4種類の圧縮アルゴリズムによって、データを圧縮して保存する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SIMPLS: an alternative approach to partial least squares regression</title>
      <link>https://nryotaro.dev/posts/simpls_an_alternative_approach_to_partial_least_squares_regression/</link>
      <pubDate>Sat, 28 May 2022 15:05:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/simpls_an_alternative_approach_to_partial_least_squares_regression/</guid>
      <description>&lt;p&gt;SIMPLSは、NIPALSとおなじく部分的最小二乗法（PLS）である。
NIPALSでは、潜在変数が、デフレーションされた説明変数の線形結合になる。
他方、SIMPLSの潜在変数は、中心化された説明変数の線形結合で表現できる。
そのため、NIPALSよりもSIMPLSの潜在変数の方が説明変数との関係を解釈しやすい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The WyCach Portfolio Management System(1992)</title>
      <link>https://nryotaro.dev/posts/experience_report_the_wycash_portfolio_management_system/</link>
      <pubDate>Sat, 28 May 2022 13:19:37 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/experience_report_the_wycash_portfolio_management_system/</guid>
      <description>&lt;p&gt;Ward Cunninghamが証券ポートフォリオ管理システムWyCASH+を開発したときの事例報告で、92年のオブジェクト指向の国際会議OOPSLAで発表された。
この発表が技術的負債の典拠であるが、文中には技術的(technical)はなく単に負債(debt)と書かれている。
アジャイルソフトウェア開発宣言が発表された2001年以前に、ウォーターフォールよりもインクリメンタルな開発のほうが最短で適当な品質のソフトウェアを構築できると主張した。
しかし、インクリメンタルな開発では、全ての知識が十分に揃っていない状態でコーディングするため、システムの部分的な修正を重ねる必要がある。
そして、オブジェクト指向言語のメッセージパッシングによって漸進的な修正が可能になるというのが発表の骨子である。
負債で会社の成長を早められるように、コードをリリースすることで後に修正する必要があるかもしれないが、よい設計を早く知ることができるため、Cunninghamは最初にリリースするコードを負債と形容した。
負債と対比されているのはプログラミングの前に設計を熟考することであり、こちらが前払い、全額払いと表現されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PLS-regression: a basic tool of chemometrics</title>
      <link>https://nryotaro.dev/posts/pls_regression_a_basic_tool_of_chemometrics/</link>
      <pubDate>Sat, 21 May 2022 12:21:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/pls_regression_a_basic_tool_of_chemometrics/</guid>
      <description>&lt;p&gt;重回帰分析は、説明変数の行列がフルランクでないとき、多重共線性により出力が不安定になる。
そのため、説明変数の数を減らし、説明変数間に相関関係がないようにする必要がある。
PLS（部分的最小二乗法）は、入力変数の線形結合で表わせる潜在変数を求め、潜在変数の線形結合で目的変数を表す。
説明変数の数より少ない数の潜在変数を求めることで、説明変数間の相関関係をとりのぞく。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Statistics and Causal Inference(1986)</title>
      <link>https://nryotaro.dev/posts/statistics_and_causal_inference/</link>
      <pubDate>Sat, 14 May 2022 12:29:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/statistics_and_causal_inference/</guid>
      <description>&lt;p&gt;ルービンの因果モデルによる統計的因果推定では、群\(U\)の要素を\(u\), 処置を\(t\), \(c\), \(Y\)を潜在的結果変数とすると、\(u\)に対して処置\(c\)を適用するときの処置\(t\)の因果効果を、\(T = Y_t(u) - Y_c(u)\)とみなす。
たとえば、\(u\)を人、\(t\), \(c\)を運動をする、しない、\(Y\)をコレステロールの値とすれば、運動とコレステロール値の関係になる。
おなじ\(u\)に対して\(Y_t(u)\)と\(Y_c(u)\)を両方観測することはできない。
因果効果は、観測するものではなく、推定するものになる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Family of Experiments on Test-Driven Development</title>
      <link>https://nryotaro.dev/posts/a_family_of_experiments_on_test_driven_development/</link>
      <pubDate>Fri, 29 Apr 2022 19:10:03 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_family_of_experiments_on_test_driven_development/</guid>
      <description>&lt;p&gt;TDDがコードに及ぼす影響を調べた先行研究はあるが、研究手法、被検者、プログラミング環境などの実験条件は様々、結論は違い、明確な結論はない。
表題の論文は、個別のTDDの実験結果の精度と汎化性を上げるために、実験結果に影響するとみられる実験条件をかえ、TDDとITL(iterative test-last development)を比較する12の実験を実施した。
被検者は4大学と企業12社で、5つのトイタスクを解かせ、通ったテストオラクルの割合で実装の品質を評価した。
結果、TDD初心者の被検者はITLによる実装のほうが品質が高かった。
エディタや言語、ITLとTDDの実験の被験順序、TDDとITLをどちらを先に学んだかは品質に影響しなかった。
学生よりも企業に所属するプロフェッショナルのほうがTDD、ITL両方で品質がよかったが、TDDで実装したときの品質の下がり幅は学生の下がり幅の2倍に及んだ。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
      <link>https://nryotaro.dev/posts/simcse/</link>
      <pubDate>Fri, 29 Apr 2022 16:25:57 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/simcse/</guid>
      <description>&lt;p&gt;対照学習は、意味の近い要素同士を近くに、異なる要素を遠くに配置する分散表現を獲得する。
SimCSEは、文の分散表現のための対照学習であり、教師なしと教師ありの2つの学習方法を提供する。
教師なし学習は、Dropout層を通したサンプルが近くに配置されるように、Dropout層の出力2つからなるペアを教師データにする。
教師あり学習は、自然言語推論(含意関係認識)の教師データをつかい、前提と含意の分散表現が近くになるように学習する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Stochastic Gradient Boosting</title>
      <link>https://nryotaro.dev/posts/stochastic_gradient_boosting/</link>
      <pubDate>Sat, 23 Apr 2022 14:05:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/stochastic_gradient_boosting/</guid>
      <description>&lt;p&gt;Gradient Boostingは、反復的に、モデルの予測と正解の残差に弱識別器をあてはめ、弱識別器をモデルに追加する。
Stochastic Gradient Boostingは、弱識別器の学習に非復元抽出したデータの部分集合をつかい、精度と学習速度を向上する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</title>
      <link>https://nryotaro.dev/posts/lightgbm/</link>
      <pubDate>Sat, 16 Apr 2022 14:32:01 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/lightgbm/</guid>
      <description>&lt;p&gt;LightGBMは、GBDTを高速化したアルゴリズムであり、XGBoostよりも必要な計算時間と消費メモリが少ない。
GBDTの処理時間のボトルネックは決定木の分岐を決めるところである。
その前処理で特徴の値をソートする場合は、ソートがボトルネックになる。
勾配の小さいサンプルを除外することでデータを減らし、また、同時に0でない値にならない排他的な特徴をマージすることで特徴の種類を減らし、ソートを高速化した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adam: A Method for Stochastic Optimization</title>
      <link>https://nryotaro.dev/posts/adam/</link>
      <pubDate>Sat, 09 Apr 2022 13:43:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/adam/</guid>
      <description>&lt;p&gt;ADAMはAdaptive moment estimationに由来し、名前のとおり、推定した1, 2次のモーメントによる学習率最適化のアルゴリズムである。
勾配が疎なときに有効なAdaGradの利点と、目的関数が時間とともに変化してもよいRMSPropの利点をそなえる。
一次や二次のモーメントを、指数関数的に加重を減少させる移動平均で推定する。
ただし、モーメントの初期値を0にすると最初のうちはモーメントの推定値が0に偏ってしまう。
そこで、反復回数がすくないほど推定値を大きくなるよう補正する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>XGBoost: A Scalable Tree Boosting System</title>
      <link>https://nryotaro.dev/posts/xgboost/</link>
      <pubDate>Sat, 26 Mar 2022 17:38:58 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/xgboost/</guid>
      <description>&lt;p&gt;XGBoostは、キャッシュやシャーディングによる高速な勾配ブースティングのライブラリであり、スパースなデータでも高速に学習できる。
情報利得が大きくなるにノードから枝をのばすときは、ノードにあるサンプルで分岐の条件を決定する。
このとき、分岐条件の特徴が欠損するサンプルを左右どちらかに無条件にふり分けると利得が大きくなるかを計算する。
これにより、欠損のないサンプル数の線形オーダまで計算量を削減する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Classifier Chains for Multi-label Classification</title>
      <link>https://nryotaro.dev/posts/classifier_chains_for_multi-label_classification/</link>
      <pubDate>Tue, 22 Mar 2022 16:30:13 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/classifier_chains_for_multi-label_classification/</guid>
      <description>&lt;p&gt;scikit-learnの&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html?highlight=chain#sklearn-multioutput-classifierchain&#34;&gt;Classifier Chain&lt;/a&gt;で実装されたClassifier Chainsは、ラベルの相関関係を特徴につかうマルチラベル分類のモデルで、相関関係をもちいる既存手法よりも計算量がすくない。
より細かくみれば、Classifier Chainsは、Classifier Chain Model(CC)とCCのアンサンブル学習であるEnsembles of Classifier Chains(ECC)の2つにわかれる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Effective Multi-Label Active Learning for Text Classification</title>
      <link>https://nryotaro.dev/posts/effective_multi-label_active_learning_for_text_classification/</link>
      <pubDate>Sat, 12 Mar 2022 15:25:23 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/effective_multi-label_active_learning_for_text_classification/</guid>
      <description>&lt;p&gt;SVMをつかったマルチラベル文書分類のための能動学習である。
ラベルをつければモデルの損失を最も小さくできるデータをさがす。
ラベルつきデータでSVMを学習し、さらに、その識別関数の値を特徴としてラベルの数を予測するロジスティック回帰を学習する。
ラベルのないデータを両モデルに入力し、ロジスティック回帰が予測するラベルの数だけ、識別関数の値の高い順にラベルを選び、そのデータのマルチラベルとみなす。
このとき、その推定したマルチラベルと識別関数の値がほど、損失関数を最も小さくできるデータとみなす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pegasos: Primal Estimated sub-GrAdient SOlver for SVM</title>
      <link>https://nryotaro.dev/posts/pegasos/</link>
      <pubDate>Sat, 12 Mar 2022 11:50:41 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/pegasos/</guid>
      <description>&lt;p&gt;Pegasosは、SVMの学習のための反復的なアルゴリズムであり、2022年3月現在、scikit-learnの&lt;a href=&#34;https://scikit-learn.org/stable/modules/sgd.html#implementation-details&#34;&gt;SGDClassifier&lt;/a&gt;で学習率を更新に採用されている。
Pegasosは、目的関数の最小値の近似値をもとめる。
求める最小値との誤差を\(\epsilon\), SVMの正則化パラメタを\(\lambda\), 各サンプルの説明変数の0でない要素数の上限を\(d\)とすると、線形カーネルをつかうSVMの時間計算量は、\(\tilde{O}(d/\lambda \epsilon)\)になる。
訓練データの数が計算量に影響しないので、教師データの数に対してスケールする。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Interior-Point Method for Large-Scale L1-Regularized Least Squares</title>
      <link>https://nryotaro.dev/posts/an_interior_point_method_for_large_scale_l1_regularized_least_squares/</link>
      <pubDate>Sat, 05 Mar 2022 13:11:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_interior_point_method_for_large_scale_l1_regularized_least_squares/</guid>
      <description>&lt;p&gt;前処理付共役勾配法により、スパースな説明変数をもつリッジ回帰の学習を高速化する。
リッジ回帰の目的関数は、凸関数だが微分可能ではない。
また、リッジ回帰の係数はスパースになりやすい。
そこで、目的関数を最小化する係数の探索を、線形不等式制約つきの凸二次計画問題とらえ、前処理付共役勾配法をつかった内点法で係数の更新方向を探索する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Regularization Paths for Generalized Linear Models via Coordinate Descent</title>
      <link>https://nryotaro.dev/posts/regularization_paths_for_generalized_liner_models_via_coordinate_descent/</link>
      <pubDate>Sat, 19 Feb 2022 15:02:19 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/regularization_paths_for_generalized_liner_models_via_coordinate_descent/</guid>
      <description>&lt;p&gt;ラッソ、リッジ、またはその両方をくみあわせるelastictnetを正則化項とする一般化線形モデルの学習を高速化した座標降下法である。
座標降下法を単純に実装すると、スパースで次元数の多い特徴だと学習に時間がかかる。
表題の手法は、その単純なパラメタの更新式の一部を、説明変数の内積におきかえ、学習データの数や次元数に対して学習時間を短縮する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using TF-IDF to Determine Word Relevance in Document Queries(2003)</title>
      <link>https://nryotaro.dev/posts/using_tfidf_to_determine_word_relevance_in_document_queries/</link>
      <pubDate>Sat, 19 Feb 2022 13:31:44 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/using_tfidf_to_determine_word_relevance_in_document_queries/</guid>
      <description>&lt;p&gt;自然言語による文書検索で、TF-IDFをベースラインにつかう利点と欠点を調べた。
クエリにある各単語のTF-IDF値の総和が最大の文書を最も関連する文書とみなす。
実験では、TFのみで検索する手法よりも予測性能がよかったが、類義語同士の同一判定をできない問題があった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An algorithm for suffix stripping (1980)</title>
      <link>https://nryotaro.dev/posts/an_algorithm_for_suffix_stripping/</link>
      <pubDate>Fri, 11 Feb 2022 13:23:45 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_algorithm_for_suffix_stripping/</guid>
      <description>&lt;p&gt;Poterのステミングで知られる。
アルゴリズムが単純で、辞書を必要としないところに特徴がある。
規則にしたがって単語の接尾辞を段階的に変換し、変換後の文字列を語幹とみなす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bringing GNU Emacs to Native Code</title>
      <link>https://nryotaro.dev/posts/bringing_gnu_emacs_to_native_code/</link>
      <pubDate>Sun, 30 Jan 2022 14:19:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bringing_gnu_emacs_to_native_code/</guid>
      <description>&lt;p&gt;Emacs 28から利用できる&lt;a href=&#34;https://gcc.gnu.org/onlinedocs/jit/&#34;&gt;libgccjit&lt;/a&gt;によるネイティブコンパイルの解説である。
パッケージアーカイブELPAにあるelisp-benchmarksによる比較ではバイトコンパイルよりも2.3倍から42倍の高速化を実現した。
ネイティブコンパイル時は、はじめに、Emacs Lispのコードをバイトコンパイラで、Emacs VM(Lisp Assembly Program, LAP)の中間表現に変換する。
つぎに、LAPをS式で静的単一代入形式の中間表現LIMPLEに変換する。
LIMPLEは、GCCの中間表現GIMPLEに由来し、ネイティブコンパイルの中核技術にあたる。
さいごに、LIMPLEをlibgccjitの中間表現に変換し、GCCでネイティブに実行可能なプログラムにコンパイルする。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Data Structure for Text Sequences</title>
      <link>https://nryotaro.dev/posts/data_structure_for_text_sequence/</link>
      <pubDate>Sat, 29 Jan 2022 18:15:25 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/data_structure_for_text_sequence/</guid>
      <description>&lt;p&gt;テキストエディタのためのデータ構造として、array, gap, list, line pointers, fixed size buffers, piece tablesを評価する。
とくに、Piece tablesを評価し詳しく解説する。
Piece tableはテキストを2つのバッファに記録する。
2つのバッファはfile buffer, add bufferで、file bufferは初期状態のテキストを保存し、add bufferは新たに挿入される文字列を保存する追記のみのバッファである。
名前のpieceはバッファ内の連続する部分文字列を意味する。
そして、pieceへのポインタのシーケンスがpiece tableである。
ポインタは、どちらのバッファか、参照する部分文字列の開始位置、長さの3つの情報からなる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</title>
      <link>https://nryotaro.dev/posts/semi_supervised_classification_with_graph_convolutional_networks/</link>
      <pubDate>Fri, 21 Jan 2022 21:01:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/semi_supervised_classification_with_graph_convolutional_networks/</guid>
      <description>&lt;p&gt;文書の特徴を点、引用などの文書間の関係を辺であらわすグラフ構造に畳込みニューラルネットワークを適用する。
半教師あり学習であり、ラベルのない文書は近くにある文書とおなじラベルになると仮定する。
&lt;a href=&#34;https://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf&#34;&gt;Zhu et al.&lt;/a&gt;の先行研究は、辺の情報をネットワークにあたず、文書の特徴のみを入力する。
表題の手法では、文書の特徴にくわえ、隣接行列で表現した辺の情報もネットワークにあたえる。
グラフの辺の数に対して線形にスケールし、隠れ層でグラフの分散表現を獲得できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dynamic Routing Between Capsules (2017)</title>
      <link>https://nryotaro.dev/posts/dynamic_routing_between_capsules/</link>
      <pubDate>Sun, 26 Dec 2021 22:26:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dynamic_routing_between_capsules/</guid>
      <description>&lt;p&gt;カプセルはおなじ層にあるニューロン(ユニット)のグループであり、カプセルの出力するベクトルは入力にある特定のエンティティの分散表現になる。
表題のdynamic routingは、カプセルの出力ベクトルを1つ上の層のどのカプセルに渡すべきかを学習する手法である。
これにより、プーリング層で失われるエンティティの空間上の位置情報をカプセルの出力するベクトルで表現する。
実験では、2層の畳み込み層と1層の全結合層からなる浅いニューラルネットワークをMNISTに適用し、長さでエンティティが存在する確率を、向きでエンティティの特徴を表現できるベクトルを学習できることを示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hierarchical Attention Networks for Document Classification (2016)</title>
      <link>https://nryotaro.dev/posts/hierarchical_attention_networks_for_document_classification/</link>
      <pubDate>Sun, 26 Dec 2021 13:24:12 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/hierarchical_attention_networks_for_document_classification/</guid>
      <description>&lt;p&gt;Hierarchical Attention Network(HAN)は、単語は文から文書は文からなる文書の構造をアーキテクチャに反映し、単語の注意から文の注意を、文の注意から文書の注意を計算する。
順方向と逆方向の2つのGRUでエンコードした単語の分散表現から注意を計算し、文ごとに、単語の注意の重みつき和を計算し文の分散表現とする。
さらに、文の分散表現を別の順、逆方向GRUにあたえ、単語とおなじように各文の注意を計算し、その重みつき和を文書の分散表現としてあつかう。
最後に、文書の分散表現を全結合層に入力し、ソフトマックス関数で文書のクラスを推定する。
単語の文の注意を推定できるため、単語と文の2つの粒度で文書の重要な箇所を可視化することができる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Effective Approaches to Attention-based Neural Machine Translation (2015)</title>
      <link>https://nryotaro.dev/posts/effective_approaches_to_attention_based_neural_machine_translation/</link>
      <pubDate>Sun, 26 Dec 2021 01:54:42 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/effective_approaches_to_attention_based_neural_machine_translation/</guid>
      <description>&lt;p&gt;注意機構をつかった翻訳用のニューラルネットワークを2つ例示し、翻訳における効果的な注意機構の使い方を提案した。
どちらもスタッキングしたLSTMを使うが、注意の計算で参照するLSTMの隠れ状態が違う。
ひとつは、1単語を出力するときに、すべての単語のLSTMの隠れ状態から注意を算出するグローバルなアプローチで、もうひとつは一部の単語の状態だけから注意を算出するローカルなアプローチである。
英語とドイツ語の双方への翻訳タスクに適用したところ、ローカルなアプローチで注意機構をつかわない手法と比べてBLEUスコアを5.0ポイントできた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Deep Learning Based Text Classification: A Comprehensive Review (2020)</title>
      <link>https://nryotaro.dev/posts/deep_learning_based_text_classification/</link>
      <pubDate>Sat, 25 Dec 2021 11:44:17 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/deep_learning_based_text_classification/</guid>
      <description>&lt;p&gt;深層学習によるテキスト分類のサーベイで、調査したモデル数の多さで論文の貢献を主張している。
文章の構成は、150個のモデル、40件のデータセット、定量的な評価指標の解説がつづく。
文書分類を広くとらえ、典型的なテキスト分類だけでなくQAやテキスト含意への言及もある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Monitoring Streams - A New Class of Data Management Applications (2002)</title>
      <link>https://nryotaro.dev/posts/monitoring_streams/</link>
      <pubDate>Sat, 11 Dec 2021 18:27:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/monitoring_streams/</guid>
      <description>&lt;p&gt;常時発生するセンサー情報などのストリームデータの監視にむいたDBMS, Auroraの設計の解説。
既存のDBMSは人の行動で起きるトランザクションを想定している。
一方、ストリームデータは、人の能動的な活動によらず絶えず発生し、異常値があればアラートを出す必要がある。
また、時系列に長期的なデータをとり、リアルタイムに応答するために不要なデータを切り捨て負荷を下げる必要もある。
Auroraは、以上のストリーミング、トリガー、不正確なデータ、リアルタイム性の4特性を扱えるモデルとして、ストリームデータの出力元をソース、ノードをストリームデータの計算、シンクをアプリケーションとするDAGで抽象化されたアーキテクチャをもつ。
ノードが計算には、ストリームをウィンドウに区切る、フィルタ、リサンプリングなどの8つがある。
DAGで表せる計算をAurora内部で実行し、その結果が接続するアプリケーションにわたる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Bridging Model for parallel Computation (1990)</title>
      <link>https://nryotaro.dev/posts/a_bridging_model_for_parallel_computation/</link>
      <pubDate>Sat, 11 Dec 2021 14:29:16 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_bridging_model_for_parallel_computation/</guid>
      <description>&lt;p&gt;バッチ処理グラフの最適化を目的として演算処理のバルク同期並列（bulk synchronous parallel、 BSP）を提唱した。
BPSは、Apache Giraph, Spark の GraphX API, Gelly APIで採用され、なかでもGoogleのPregelで知られるようになった&lt;a href=&#34;https://www.oreilly.co.jp/books/9784873118703/&#34;&gt;*1&lt;/a&gt;。
BPSの目的は、ハードウェアと並列計算の高水準なプログラミングモデルがどちらもBPSに準ずることで、高水準なプログラミングモデルで実装された並列計算を多様なハードウェア上で動かすことにある。
フォン・ノイマンモデル型のコンピュータであれば、ハードウェアの種類を意識することなく、その上で逐次計算をおこなう多様なソフトウェアを動かせる。
フォン・ノイマンモデルは、多様なハードウェアと多様なソフトウェアをつなぐ。
BPSは、並列計算用のハードウェアと高水準な並列計算のプログラミングモデルをつなぐためにある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Wait-Free Synchronization</title>
      <link>https://nryotaro.dev/posts/wait_free_synchronization/</link>
      <pubDate>Sat, 27 Nov 2021 12:41:19 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/wait_free_synchronization/</guid>
      <description>&lt;p&gt;あるデータ構造がwait-freeであり、データ構造への操作が有限回のステップで完了するのであれば、ほかの並行プロセスの処理速度によらず、任意のプロセスによるその操作は必ず完了する。
表題のsynchronizationは、wait-freeであるデータ構造で別のwait-freeなデータ構造を実装することを意味する。
実装可能かを定義するためにコンセンサス数を導入する。
あるデータ構造のコンセンサス数は、単純な含意形成問題を解くときに参加できるプロセス数の最大値である。
たとえば、read / writeレジスタのコンセンサス数は1, test &amp;amp; swapは2, compare &amp;amp; swapは無限である。
プロセス数を定義した上で、あるコンセンサス数のデータ構造を、それより小さいコンセンサス数のデータ構造では実装できないことを示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Live Migration of Virtual Machines</title>
      <link>https://nryotaro.dev/posts/live_migration_of_virtual_machines/</link>
      <pubDate>Sat, 20 Nov 2021 19:53:45 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/live_migration_of_virtual_machines/</guid>
      <description>&lt;p&gt;XenのハイパーバイザーにOSのライブマグレーションを統合し、約60msのダウンタイムでのOSのライブマイグレーションを実現した。
手法の焦点は、どれだけ短いダウンタイムや移行時間で、移行元と移行先のメモリの状態を同等にできるかにある。
ネットワークアドレスや物理デバイスの移行はあつかわない。
移行元と移行先は同じローカルネットワークにあり、マイグレーションによるIPアドレスのホストの移動を主にARP replyで通知でき、NASにデータを保存することを前提にし、ネットワークや物理デバイスの移行を考えなくてよいものとしている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Critique of ANSI SQL Isolation Levels</title>
      <link>https://nryotaro.dev/posts/a_critique_of_ansi_sql_isolation_levels/</link>
      <pubDate>Sat, 13 Nov 2021 19:34:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_critique_of_ansi_sql_isolation_levels/</guid>
      <description>&lt;p&gt;ANSI SQL-92規格は、トランザクション分離レベルを、Dirty Read, Non-Repeatable Reads, Phantomが発生する可能性で定義する。
著者は、トランザクション分離レベルを禁止する現象で定義する理由を、ロックなどの実装手段で定義すると規格が実装依存になるからだと考えている。
表題の論文は、規格の現象による定義があいまいであり、3つの異常が起きなくても望まない結果になる実行があることを例示した。
また、規格のトランザクション分離レベルが、商用データベースで採用されているトランザクション分離レベルにあてはまらない問題もある。
禁止する現象にDirty Writeを規格に加え、厳しく実行列を制限するように現象の定義を解釈し、現象の説明を自然言語から形式的な記述に変えることを提唱した。
さらに、規格がデータの版が単一であることを前提としていることを指摘した上で、多版型のトランザクションであるSnaphot Isolationを提案した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Overview of Data Warehousing and OLAP Technology</title>
      <link>https://nryotaro.dev/posts/an_overview_of_data_warehousing_and_olap_technology/</link>
      <pubDate>Sat, 06 Nov 2021 21:10:04 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_overview_of_data_warehousing_and_olap_technology/</guid>
      <description>&lt;p&gt;データウェアハウスの概要と発表時期の97年の関連技術を解説した論文で、データウェアハウスの理論を提唱したInmonにならい、データウェアハウスを、目的指向(subject-oriented)で、統合され、組織の意思決定に資する永続的なデータとらえている。
関連技術を、ETL処理、データ保存方法、保存したデータによる分析の3つに分けて整理する。
データベースの設計手法では、Kimballの提唱したスタースキーマ、その応用のスノーフレークスキーマ, 事実の星座(fact constellations)、インデックスについてビットマップインデックスを解説する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System</title>
      <link>https://nryotaro.dev/posts/bayou/</link>
      <pubDate>Sat, 06 Nov 2021 18:41:01 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bayou/</guid>
      <description>&lt;p&gt;Bayouは、モバイルコンピューティングむけに95年に発表されたストレージで、弱い一貫性とレプリケーションがある。
発表当時の不安定なモバイルネットワーク環境でもクライアントがストレージにアクセスできるように、一貫性を犠牲にしてクライアントにどんなレプリケーションへの読み書きも認める。
Bayouは、アプリケーションの知識を借りてデータの整合性を復元するアプローチを開拓したストレージとして知られ、書き込み時に開発者が実装した不整合を検知する処理を実行する。
不整合を検知したら、同じく開発者によって実装された不整合を解消する処理を実行する。
ただし、書き込みのログをほかのレプリケーションに伝搬することでデータを同期するため、どんなレプリケーションでも不整合の検知と解消の処理が同一の結果になるように、これらの処理は決定的でなければならない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Many Faces of Publish/Subscribe (2003)</title>
      <link>https://nryotaro.dev/posts/the_many_faces_of_publish_subscribe/</link>
      <pubDate>Sat, 30 Oct 2021 16:13:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_many_faces_of_publish_subscribe/</guid>
      <description>&lt;p&gt;分散システムの通信をpublish/subscribeモデルにするとシステム間の結合を疎にできる。
表題の論文は、publish/subscribeのサーベイであり、publish/subscribeモデルを空間、時間、同期の3種の結合度を下げる技術ととらえ、ほかの通信技術と比較する。
また、先行研究のpublish/subscribeモデルを、購読するイベントの宣言手段によって、topic-based, content-based, type-basedに分類する。
最後に、メッセージブローカーを使うかどうかなどpublish/subscribeモデルの設計や実装の利点や欠点を整理する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks</title>
      <link>https://nryotaro.dev/posts/dryad/</link>
      <pubDate>Sat, 23 Oct 2021 19:03:45 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dryad/</guid>
      <description>&lt;p&gt;DryadはMicrosoftで開発された分散コンピューティングエンジンであり、MapReduceに近い。
処理を点、処理結果を受け渡す通信経路を辺とするDAGで、分散処理を定義する。
通信経路にはファイル、TCP, FIFOを使え、実行環境は1台のマルチコアのコンピュータから数千台のコンピュータまでスケールできる。
MapReduceは入力と出力が一つでなければならないが、DryadはDAGであれば入力と出力が複数あってもかまわない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Chubby lock service for loosely-coupled distributed systems</title>
      <link>https://nryotaro.dev/posts/the_chubby_lock_service_for_loosely_coupled_distributed_systems/</link>
      <pubDate>Sat, 16 Oct 2021 15:16:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_chubby_lock_service_for_loosely_coupled_distributed_systems/</guid>
      <description>&lt;p&gt;Chubbyは、ファイルに助言ロックをかけられる低容量のストレージで、開発元のGoogleは分散システム向けのロックサービスとして使っている。
処理性能よりも高可用性と信頼性を重視し、ロックを保持する期間は数時間から数日にわたる長いものを想定する。
ロックされたファイルで情報を共有でき、例えばリーダーの選出に使える。
Chubbyは既存のアルゴリズムを集積であり、表題の論文は、新しいアルゴリズムよりも、Chubbyの設計や想定していた使用方法や想定外の使われ方に重点をおく。
Chubbyはロックサービスとして作られたが、ネームサーバとして最もよく使われる。
数千規模の互いに依存するプロセスがあり、プロセスが落ちたら新しいプロセスに置き換えるとする。
このとき、ネームサーバにDNSをつかうと、DNSは時間で失効するキャッシュ方式なので、TTLを短くし、落ちたプロセスにリクエストが届かないようにすると、DNSへの負荷が増える。
一方、Chubbyのキャッシュは時間ではなくファイルに変更されたときに無効になるので、Chubbyをネームサーバに使うことで、名前解決の負荷を減らすことができる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Congestion Avoidance and Control</title>
      <link>https://nryotaro.dev/posts/congestion_avoidance_and_control/</link>
      <pubDate>Sat, 09 Oct 2021 12:44:26 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/congestion_avoidance_and_control/</guid>
      <description>&lt;p&gt;86年の10月におきたインターネットの世界規模の輻輳の反省から、4BSDに7つのTCPの輻輳制御のアルゴリズムが導入された。
うち5つは、送信したパケットが到達や消失でネットワークに流れなくなるまで、ウィンドウサイズ以上の新しいパケットを送らない原則を守るためにある。
この原則をconservation of packets principleという。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Notion of Consistency and Predicate Locks in a Database System</title>
      <link>https://nryotaro.dev/posts/the_notion_of_consistency_and_predicate_locks_in_a_database_system/</link>
      <pubDate>Sat, 02 Oct 2021 16:24:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_notion_of_consistency_and_predicate_locks_in_a_database_system/</guid>
      <description>&lt;p&gt;トランザクションを並行実行するときに、直列実行した場合と同じ実行結果をえるには、各トランザクションがロックを解放後に新しいロックを獲得してはならないことを示した。
その場合もデッドロックがおきうるが、それは並列実行を終了するためにはいつロックをかければよいかという議論である。
ここでは、並列実行が終了した場合に直列実行と同じ結果になる一貫性を保証できるロックの順序を問う。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Parallel Database Systems: The Future of High Performance Database Systems (1992)</title>
      <link>https://nryotaro.dev/posts/parallel_database_systems/</link>
      <pubDate>Sat, 25 Sep 2021 13:09:34 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/parallel_database_systems/</guid>
      <description>&lt;p&gt;データベースの処理性能を上げる方法として、汎用的なコンピュータによる水平パーティションのシェアードナッシング構成を主張した。
特殊なハードウェアを使ったり、メモリ、ディスクを共有する複数のコンピュータを使ったりするよりも、単純で安く実装できる。
論文が発表された1992年の時点でTeradataやTandem NonStopなどのパーティンション化されたデータベースがある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chain Replication for Supporting High Throughput and Availability</title>
      <link>https://nryotaro.dev/posts/chain_replication_for_supporting_high_throughput_and_availability/</link>
      <pubDate>Sat, 18 Sep 2021 15:34:12 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/chain_replication_for_supporting_high_throughput_and_availability/</guid>
      <description>&lt;p&gt;ストレージサーバーに順番をつけ、順にオブジェクトを複製することで、ストレージサービスの一貫性を維持しつつスループットと可用性の向上をはかる。
ここでのストレージサービスは、オブジェクトを保存し、クエリに対して一つのオブジェクトを返し、アトミックに一つのオブジェクトを更新できるものをさす。
また、一貫性は、オブジェクトごとにクエリと更新を直列的に適用し、更新につづくクエリが更新内容になることを意味する。
末尾のサーバーがクエリを受信し、該当するオブジェクトをクライアントに返す。
先頭のサーバーが更新リクエストを受信し、順にサーバーのオブジェクトを更新し、末尾のサーバーが更新の結果をクライアントに返す。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Implementing Remote Procedure Calls</title>
      <link>https://nryotaro.dev/posts/implementing_remote_procedure_calls/</link>
      <pubDate>Sat, 11 Sep 2021 12:34:19 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/implementing_remote_procedure_calls/</guid>
      <description>&lt;p&gt;Xeroxのプログラミング環境&lt;a href=&#34;http://www.bitsavers.org/pdf/xerox/parc/techReports/CSL-80-10_Requirements_for_an_Experimental_Programming_Environment.pdf&#34;&gt;Cedar&lt;/a&gt;のために開発されたRPCを解説した84年の論文。
RPCの開発目的は、分散コンピューティングを簡単に実装できるようにする、通信時間を短くする、通信をセキュアにすることの3点にある。
RPCは大部分で&lt;a href=&#34;http://www.bitsavers.org/pdf/xerox/parc/techReports/CSL-79-3_Mesa_Language_Manual_Version_5.0.pdf&#34;&gt;Mesa&lt;/a&gt;が使われた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dremel: Interactive Analysis of Web-Scale Datasets</title>
      <link>https://nryotaro.dev/posts/dremel/</link>
      <pubDate>Sat, 04 Sep 2021 15:55:14 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dremel/</guid>
      <description>&lt;p&gt;Dremelは2006年からGoogle社内で利用されているDWHで、社外にはBigQueryとして提供されている。
列指向の形式でデータを保存し、SQLに似た言語のクエリでデータを検索できる。
Dremelは、サーバーのクラスタを木構造に組織し、ルートで受理したクエリの処理を下層のサーバに分配し、その結果を集約することで処理を分散し高速化をはかる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Is Rust Used Safely by Software Developers?</title>
      <link>https://nryotaro.dev/posts/is_rust_used_safely_by_software_developers/</link>
      <pubDate>Fri, 27 Aug 2021 17:51:26 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/is_rust_used_safely_by_software_developers/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://crates.io/&#34;&gt;crates.io&lt;/a&gt;に登録された15,097個のクレートにおける&lt;code&gt;unsafe&lt;/code&gt;キーワードの使用状況を調べた。
調査したクレートは、調査を開始した2018年9月時点で&lt;a href=&#34;https://crates.io/&#34;&gt;crates.io&lt;/a&gt;に登録された全クレートの81%にあたる。
&lt;code&gt;unsafe&lt;/code&gt;を含むクレートは、そのうちの29%だったが、依存するクレートにある&lt;code&gt;unsafe&lt;/code&gt;も対象にすると、50%におよぶ。
&lt;a href=&#34;https://crates.io/&#34;&gt;crates.io&lt;/a&gt;の総ダウンロード数のうち90%を占める473個の有名なクレートに限定すると、60%のクレート&lt;code&gt;unsafe&lt;/code&gt;が含まれる。
2018年9月から2019年6月までの10ヶ月間で&lt;code&gt;unsafe&lt;/code&gt;の使用傾向に変化はなく、&lt;code&gt;unsafe&lt;/code&gt;の数が少し増えただけであった。
&lt;code&gt;unsafe&lt;/code&gt;の用途の大半は&lt;code&gt;unsafe&lt;/code&gt;で修飾されたRustの関数を呼び出すためだった。
なお、コンパイラで生成された&lt;code&gt;unsafe&lt;/code&gt;キーワードは集計に含まれていない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Design Tradeoffs for SSD Performance</title>
      <link>https://nryotaro.dev/posts/design_tradeoffs_for_ssd_performance/</link>
      <pubDate>Mon, 23 Aug 2021 12:03:39 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/design_tradeoffs_for_ssd_performance/</guid>
      <description>&lt;p&gt;HDDの処理性能を測定するシミュレーションソフト&lt;a href=&#34;https://www.pdl.cmu.edu/DiskSim/index.shtml&#34;&gt;DiskSim&lt;/a&gt;をSSD向けに改造し、SSDの設計における選択肢の分類と選択にともなうトレードオフを報告した。
主要な選択肢は、論理アドレスと物理アドレスのマッピング、ページサイズ、オーバープロビジョニング、複数のpackageでcontrollerに接続するピンを共有するgangingがある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Program Comprehension and Code Complexity Metrics: An fMRI Study</title>
      <link>https://nryotaro.dev/posts/program_comprehension_and_code_complexity_metrics/</link>
      <pubDate>Sat, 21 Aug 2021 16:13:15 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/program_comprehension_and_code_complexity_metrics/</guid>
      <description>&lt;p&gt;コードの複雑さのメトリクスがコードを理解する難しさの指標になるかは疑問視されてきた。
19人の被験者に16個のソースコードを読ませ、関数の返り値を回答してもらい、作業中の脳の状態をfMRIで観察することで、メトリクスと回答時間、正答率、脳の活性状態の相関関係を調べた。
メトリクスと主観的な評価を比較するために、回答後に被験者にコードの複雑さを評価してもらった。
調べたメトリクスは、41種類あり、コードの行数(LOC), 語彙の多さ(Halstead), とりえる実行パスの数(McCabe), 依存するデータの数(DepDegree)の4種類に大別できる。
相関関係をケンドールの順位相関係数\(\tau\)で評価し、相関なし(\(\tau &amp;lt;0.1\)), 弱い(\(0.1 &amp;lt; \tau &amp;lt; 0.3\)), 中(\(0.3 &amp;lt; \tau &amp;lt;0.5\)), 強い(\(0.5&amp;lt;\tau\))とみなす。&lt;/p&gt;
&lt;p&gt;回答と脳の活性状態と相関関係にあったメトリクスはDepDegreeだったが、被験者の主観的な評価のほうが強い相関がみられた。
LOC, Halstead, DepDeegreeは回答時間や正答率と弱い〜中程度の相関があった。
この3つのメトリクスは脳の活性度合いと弱から中の相関があり、活性度合いと正答率には強い相関、回答時間には弱から中の相関があった。
一方、主観的な評価は、メトリクスと弱い相関があり、問題の正答率や脳の活性状態と強い相関関係があった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The UNIX Time-Sharing System</title>
      <link>https://nryotaro.dev/posts/the_unix_time_sharing_system/</link>
      <pubDate>Sat, 21 Aug 2021 13:11:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_unix_time_sharing_system/</guid>
      <description>&lt;p&gt;デニス・リッチーとケン・トンプソンによるPDP-11/40, /45, /70で採用されたUnixのファイルシステムとコマンドラインインターフェースの解説である。
PDP-11は1971年2月から運用がはじまり、当初はアセンブリ言語で実装されていたが、1973年の夏にCで再実装された。
ファイルシステムは、UNIXの最も重要な役割に位置づけられ、特殊ファイルによるI/Oデバイスの抽象化、外部ディスクのマウント、ファイルの権限、i-nodeをふくむ多くの特徴が今日まで引き継がれている。
2人は、UNIXの設計に影響したものとして、プログラマとして対話的なインターフェースを望んでいたこと、ハードウェアの低い性能ゆえにソフトウェアの設計を洗練させる必要があったこと、ソースコードをUNIX上で編集し簡単にプログラムを変更できたことをあげている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What helped, and what did not? An Evaluation of the Strategies to Improve Continuous Integration</title>
      <link>https://nryotaro.dev/posts/what_helped_and_what_did_not/</link>
      <pubDate>Fri, 20 Aug 2021 17:58:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/what_helped_and_what_did_not/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://travistorrent.testroots.org/&#34;&gt;TravisTorrent&lt;/a&gt;にある100件のプロジェクトで10種類のCIのテクニックを定量評価した。
テクニックは、不要なビルドやテストをスキップするか、実行順序を優先づけるものかに分かれる。
前者は計算資源の消費を減らすこと、後者は失敗するケースを早めに実行することを目的にする。
もっとも成功するテストやビルドをスキップできたテクニックは、コードに変更のないケースをスキップするものだったが、同時に失敗するテストを多く見落とした。
実行順序を優先付ける手法で最も性能のよかったテクニックは、&lt;a href=&#34;https://www.semanticscholar.org/paper/Static-test-case-prioritization-using-topic-models-Thomas-Hemmati/ad77a2776733c7fdc10c0b043f7504a14fce3b6e&#34;&gt;Thomasら&lt;/a&gt;のもので、シグネチャやコメントで学習したトピックモデルを使い直前に実行したテストと違うトピックのテストを実行する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Rumors Roble Bodegas Rumors</title>
      <link>https://nryotaro.dev/gallery/rumors/</link>
      <pubDate>Sun, 15 Aug 2021 23:00:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/rumors/</guid>
      <description>渋い。</description>
    </item>
    
    <item>
      <title>Don&#39;t Do That! Hunting Down Visual Design Smells in Complex UIs against Design Guidelines</title>
      <link>https://nryotaro.dev/posts/dont_do_that/</link>
      <pubDate>Sat, 14 Aug 2021 16:40:58 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dont_do_that/</guid>
      <description>&lt;p&gt;2020年5月時点のマテリアルデザインの公式ドキュメントから93種類の不吉な匂いを洗い出し、71種類の匂いを検出するツールUIS-Hunterを開発した。
文中に&amp;quot;don&amp;rsquo;t&amp;quot;や&amp;quot;caution&amp;quot;があることとUIの画像があることを条件に不吉な匂いを選び、9,286個のアンドロイドアプリにある7,497のUIを調べたところ、2,587個のアプリから1つ以上の不吉な匂いのあるUIが見つかった。
UIS-Hunterは、FigmaやAndroid Studio Layout EditorなどのモックアップやAndroid UI AutomatorやSeleniumのスクリーンショットから不吉な匂いを解析し、UIのソースコードを必要としない。
9,286個のアプリの60,756のUIで検出性能を評価したところ、precisionが0.81, recallが0.90だった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Empirical Analysis of UI-based Flaky Tests</title>
      <link>https://nryotaro.dev/posts/an_empirical_analysis_of_ui-based_flaky_tests/</link>
      <pubDate>Sat, 14 Aug 2021 16:33:14 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_empirical_analysis_of_ui-based_flaky_tests/</guid>
      <description>GitHub上の5つのWebアプリケーションと37のAndroidアプリケーションから集めた235件のUIのflaky tests（何度か実行すると成功する不安定なテスト）を調査し、原因と修正を分類した。
大きく原因を、非同期処理の待機、環境依存の動作、DOMのセレクタやテストライブラリの誤用、テスト対象の事前条件を満たしていないテストスクリプトの4つに大別した。 具体例をあげると、環境依存の動作には、IE固有のバグや予期していないレイアウトで画面が表示される場合、テストの事前条件については、テストの実行順序次第で誤ったテストデータが作られる場合がある。 最も多くのテストが分類され、全体の45%を占めたのは、非同期処理の不適切な待機方法だった。
修正のパターンには、待機時間の追加、テスト用APIの誤用の修正やAPIのアップグレード、テストスクリプトのリファクタリング、アニメーションの削除、不要なテストの削除がある。
論文をこちらからダウンロードできます。</description>
    </item>
    
    <item>
      <title>A Case Study of Onboarding in Software Teams: Tasks and Strategies</title>
      <link>https://nryotaro.dev/posts/a_case_study_of_onboarding_in_software_teams/</link>
      <pubDate>Sat, 14 Aug 2021 16:31:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_case_study_of_onboarding_in_software_teams/</guid>
      <description>&lt;p&gt;オンボーディングのためのタスクの選び方とタスクの効果を調査するために、マイクロソフトのエンジニアとマネージャーにインタビューした。
まず、新しいチームに入った32人のエンジニアとエンジニアを迎えた15人のマネージャーにインタビューし、特に、チームのことを知る、担当する役割を果たせる自信の醸成、メンバーとの交流の3つを重視し、これらに対するタスクの影響を調査した。
タスクの選び方は、大きく、割り当てるタスクを少しずつ複雑にする、優先度の高いものを選ぶ、曖昧なタスクを選ぶ、の3つがあった。
オンボーディングするエンジニアがジュニアであれば最初の選び方、シニアであれば最後の選び方、アジャイルを採用するチーム、新しいチーム、納期の厳しいチームは優先度でタスクが選ばれやすく、効果的であった。
以上の考察を189名のエンジニアと37名のマネージャに評価してもらい、妥当性を確認した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms</title>
      <link>https://nryotaro.dev/posts/a_differential_testing_approach_for_evaluating_ast_mapping_algorithms/</link>
      <pubDate>Sat, 14 Aug 2021 16:29:33 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_differential_testing_approach_for_evaluating_ast_mapping_algorithms/</guid>
      <description>&lt;p&gt;AST mappingは、コードの変更前後のASTを比べてノードの対応関係を見つける手法であり、変更差分検出に使われる。 現状、対応関係の精度を自動で評価する有効な方法はなく、評価には人手による手間がかかる。 多くのノードに1対1の対応関係があることに着目し、異なる2つのAST mappingを同じ変更に適用した結果を比べ、個別の文やトークンごとに、より正確な方を推定するアルゴリズムを提案した。 これを応用し、複数のAST mappingアルゴリズムに同じファイルの変更差分を入力し、アルゴリズムごとの不正確に検出した箇所を自動で推定できることを示した。 特定性能は、Precisionが0.98-1.00, Recallは0.65-0.75だった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>“Ignorance and Prejudice” in Software Fairness</title>
      <link>https://nryotaro.dev/posts/ignorance_and_prejudice_in_software_fairness/</link>
      <pubDate>Sat, 14 Aug 2021 16:23:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/ignorance_and_prejudice_in_software_fairness/</guid>
      <description>&lt;p&gt;特徴の種類を増やすと、機械学習の予測の公平性と精度を改善できることを5つのデータセットで例示した。 データセットのタスク内容は、性別、人種、年齢を特徴に含み、経済的な裕福さや再犯率を予測するもの。 他方、教師データの数を増やしても公平性は改善されなかった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Time, Clocks, and the Ordering of Events in a Distributed System</title>
      <link>https://nryotaro.dev/posts/time_clocks_and_the_ordering_of_events_in_a_distributed_system/</link>
      <pubDate>Sat, 14 Aug 2021 14:18:01 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/time_clocks_and_the_ordering_of_events_in_a_distributed_system/</guid>
      <description>&lt;p&gt;分散システムの各プロセスにおける送受信の半順序関係をすべてのプロセスで起きた送受信の全順序関係に拡張するアルゴリズムを提案し、アルゴリズムを同期処理に応用できることを例示した。
分散システムではプロセスの時刻が同期しているとはかぎらない。
各プロセスで起きたメッセージの送受信をプロセスでの時刻順に並べられても、その計測時刻を信じて全プロセスで起きたイベントを正しく発生順に並べることはできない。&lt;/p&gt;
&lt;p&gt;アルゴリズムは、プロセスごとに論理的なクロックをもたせる。
クロックは、メッセージを送信するときに時を進める。
メッセージを送信するプロセスは、送信時刻をメッセージにふくめる。
受信したプロセスは現在の時刻とメッセージにある送信元の時刻より先の時刻に時を進める。
以上の手続きで、異なるプロセス間の送受信であっても送信時刻が受信時刻より必ず前になり、全プロセスの送受信イベントに全順序関係を定義できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, unbounded, Out-of-Order Data Processing</title>
      <link>https://nryotaro.dev/posts/dataflow/</link>
      <pubDate>Sat, 07 Aug 2021 15:11:48 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dataflow/</guid>
      <description>&lt;p&gt;データ処理サービスは、処理の正確さ、遅延、システムの複雑さの間にトレードオフを抱える。
ストリーミング処理サービスのStorm, Samza, Pulsarは、(論文が発表された2015年時点では)メッセージ配信がexactly-onceではなく欠損や重複する。
MapReduceやSparkなどのバッチ処理サービスは、バッチ処理の単位までデータが集まらなければバッチ実行できない。
Lambda architectureは、システムの複雑化を許容し、2つのアーキテクチャを使い分けることで、処理の正確さと遅延のバランスをとる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Playing Planning Poker in Crowds: Human Computation of Software Effort Estimates</title>
      <link>https://nryotaro.dev/posts/playing_planning_poker_in_crowds/</link>
      <pubDate>Fri, 06 Aug 2021 13:44:03 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/playing_planning_poker_in_crowds/</guid>
      <description>&lt;p&gt;プランニングポーカーのように、ソフトウェア開発の工数を見積もる手法の前提には、プロジェクトに一定期間在籍する専門家がいることがある。
しかし、OSS開発はその限りではなく、見積りの対象になるイシューの数は多い。
各イシューの見積りに5分から10分時間をかけるなら、Linux Kernelのバックログを見積りきるのに半年がかかる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Villa Yambol Cabernet Sauvignon</title>
      <link>https://nryotaro.dev/gallery/villa_yambol/</link>
      <pubDate>Wed, 04 Aug 2021 13:09:11 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/villa_yambol/</guid>
      <description>渋みがなく、飲みやすい。飲みやすくて、記憶に残りにくいけどおいしい。</description>
    </item>
    
    <item>
      <title>The Byzantine Generals Problem</title>
      <link>https://nryotaro.dev/posts/the_byzantine_generals_problem/</link>
      <pubDate>Sat, 31 Jul 2021 11:07:42 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_byzantine_generals_problem/</guid>
      <description>&lt;p&gt;ビザンチン将軍問題は、故障したコンポーネントがほかのコンポーネントに誤ったメッセージを送りうるとき、コンポーネント間でメッセージを交換した結果、すべてのコンポーネントが一つの正しいメッセージを合意する問題をあつかう。
アルゴリズムは、署名付きメッセージを使うものと使わないものの2つがある。
署名付きメッセージを使うと、受信したメッセージが改ざんされたかをコンポーネントが判断できる。
署名を使わない場合、3分の2より多くのコンポーネントが正常でなければシステム全体が正しい1つのメッセージについて合意にできないことを示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chateau La Croix Blanche</title>
      <link>https://nryotaro.dev/gallery/chateau_la_croix_blanche2/</link>
      <pubDate>Mon, 26 Jul 2021 10:04:27 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/chateau_la_croix_blanche2/</guid>
      <description>酸味はあまりない。少し渋い。</description>
    </item>
    
    <item>
      <title>The Browsemaps: Collaborative Filtering at LinkedIn</title>
      <link>https://nryotaro.dev/posts/browsemaps/</link>
      <pubDate>Thu, 22 Jul 2021 13:01:15 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/browsemaps/</guid>
      <description>&lt;p&gt;Browsemapsは、LinkedInのアイテムベースで水平型の協調フィルタリングである。
Browsemapは、LinkedInの画面上にある推薦するコンテンツを並べたコンポーネントを意味する。
ここでの水平型は特徴の種類の違いによらず異なるエンティティを統一的に扱えることであり、実際に、人、仕事、会社など複数のエンティティをBrowsemapsでユーザに推薦している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Casal di Serra</title>
      <link>https://nryotaro.dev/gallery/casal_di_serra2/</link>
      <pubDate>Mon, 19 Jul 2021 23:19:41 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/casal_di_serra2/</guid>
      <description>やっぱり記憶に残らない。</description>
    </item>
    
    <item>
      <title>Linearizability: A Correctness Condition for Concurrent Objects</title>
      <link>https://nryotaro.dev/posts/linearizability/</link>
      <pubDate>Sat, 17 Jul 2021 13:31:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/linearizability/</guid>
      <description>&lt;p&gt;オブジェクトを読み書きする並行プロセスの実行系列があるとき、読み書き操作がアトミックであるように観測でき、かつ、プロセスの実行を仮に直列化したときと同じ実行結果をえられる条件を示し、これを線形化可能性とよんだ。
線形化可能性は、直列化可能性と同様に安全性の条件だが、直列化可能性にはないローカル性がある。
いいかえると、各オブジェクトが線形化可能かつそのとき限り、システム全体が線形化可能になる。
また、ローカル性だけでなく、ノンブロッキング性もあり、操作がオブジェクトの任意の状態において定義されていれば、受信したリクエストの操作を保留しているオブジェクトは別オブジェクトの保留した操作の完了を待たずに自分の操作を進行できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>END-TO-END ARGUMENTS IN SYSTEM DESIGN</title>
      <link>https://nryotaro.dev/posts/end_to_end_arguments_in_system_design/</link>
      <pubDate>Sat, 10 Jul 2021 15:15:39 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/end_to_end_arguments_in_system_design/</guid>
      <description>&lt;p&gt;分散システムのあるシステムから発せられた通信は、システムより低いレイヤーのネットワークを通り、ほかのシステムに到達する。
論文の発表当時から、分散システムの信頼性にかかわる機能には、途中の低レイヤーではなく高レイヤーの終端におくべきものがあると知られていた。
表題の論文は、それらを列挙し、END-TO-END ARGUMENTSと名付け、原則として提示する。
例えば、データの完全性の保証、メッセージの重複対応、メッセージが到達したことの保証が原則にあてはまる。
これらの機能を途中の低レイヤーにおくと、検証後の経路でメッセージが壊れた場合、送信元が重複メッセージをおくる場合、対向システムが受信したメッセージを処理できない場合に対処できない。
分散システム全体の信頼性をあげるのであれば、信頼性を上げる機能を低レイヤーではなく終端の高レイヤーにおくほうが効率がよい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Goose Bump</title>
      <link>https://nryotaro.dev/gallery/goose_bump/</link>
      <pubDate>Sat, 10 Jul 2021 15:02:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/goose_bump/</guid>
      <description>甘い。この一言につきる。こんなに甘い赤ワインがあるんだって驚き。</description>
    </item>
    
    <item>
      <title>Casal di Serra</title>
      <link>https://nryotaro.dev/gallery/casal_di_serra/</link>
      <pubDate>Sat, 03 Jul 2021 23:51:51 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/casal_di_serra/</guid>
      <description>辛くもなく、樽香もないワイン。抑揚のないワインで、嫌いではないが、記憶に残らないのでまた買いたいという気にはならない。</description>
    </item>
    
    <item>
      <title>Chateau La Croix Blanche</title>
      <link>https://nryotaro.dev/gallery/chateau_la_croix_blanche/</link>
      <pubDate>Sat, 03 Jul 2021 23:43:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/chateau_la_croix_blanche/</guid>
      <description>なぜかまとめて２つ買ってしまったやつ。酸味と渋みのどちらにも偏りがなくて飲みやすい。 飲みやすくておいしいのは分かるけど、あまりに個性がなくて「これを飲みたい」みたいな気にならないのが残念。 ただ、美味しいのは確か。</description>
    </item>
    
    <item>
      <title>A History and Evaluation of System R</title>
      <link>https://nryotaro.dev/posts/a_history_and_evaluation_of_systemr/</link>
      <pubDate>Sat, 03 Jul 2021 15:02:21 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_history_and_evaluation_of_systemr/</guid>
      <description>&lt;p&gt;System Rは、1970年に発表されたリレーショナルモデルを実装した初めてのリレーショナルデータベースである。
1981年に発表された表題の論文は、System Rの開発プロジェクトを3つのフェーズに分け各フェーズごとにえられた知見をまとめている。
Phase 0は、74年から75年にかけて、シングルユーザ向けにSQLサブセットと対話的なインターフェースを開発した期間にあたる。
Phase 1は、76年から77年の、Phase 0のコードを破棄し、マルチユーザに対応しSystem Rの機能を完全に実装するまでの期間である。
Phase 2は、78年から79年にSan Jose Research Laboratoryなどでの実用にもとづく評価期間にあたる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>beamerのスタイルファイル rei.sty</title>
      <link>https://nryotaro.dev/posts/rei/</link>
      <pubDate>Sat, 26 Jun 2021 17:15:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/rei/</guid>
      <description>&lt;p&gt;日本語用のbeamerスライドのデザインを作った。
飽きないように単純なデザインにした。
デモは&lt;a href=&#34;https://nryotaro.dev/rei.pdf&#34;&gt;こっち&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoder Beginner Contest 206 D KAIBUNsyo</title>
      <link>https://nryotaro.dev/posts/atcoder_abc206d/</link>
      <pubDate>Sun, 20 Jun 2021 12:59:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/atcoder_abc206d/</guid>
      <description>&lt;p&gt;Union Findを使うと通せる。すべての\(A_i\)と\(A_{N+1-i}\)が同じグループに所属するために必要なunionの回数を答えればいい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</title>
      <link>https://nryotaro.dev/posts/consistent_hashing_and_random_trees/</link>
      <pubDate>Sat, 19 Jun 2021 14:41:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/consistent_hashing_and_random_trees/</guid>
      <description>&lt;p&gt;各サーバーがネットワーク全体の情報を保持しない分散キャッシュネットワークにホットスポットを生まないためのキャッシュプロトコルを開発した。
表題のconsistent hashingは、プロトコルの基礎になるハッシュ関数で、関数の写像が値域の違いに影響されにくい。
この影響しにくさは、クライアントから見えるアクティブなサーバが入れ替わる可能性に対して機能する。
また、クライアントは一部のキャッシュサーバーにアクセスできればよく、クライアントごとにアクセス可能なキャッシュサーバーの集合は違ってよい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Navaelus Bodega Inurrieta</title>
      <link>https://nryotaro.dev/gallery/navaelus_bodega_inurrieta/</link>
      <pubDate>Sat, 19 Jun 2021 14:39:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/navaelus_bodega_inurrieta/</guid>
      <description>すごい飲みやすい。また買いたい。</description>
    </item>
    
    <item>
      <title>Vine In Flames Chardonnay Ville</title>
      <link>https://nryotaro.dev/gallery/vine_in_flames_chardonnay_ville/</link>
      <pubDate>Sat, 19 Jun 2021 14:32:27 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/vine_in_flames_chardonnay_ville/</guid>
      <description>辛口でもなく、樽香をつけてもないシャルドネそのままの感じがする。辛口ではないシャルドネの中では一番好きかもしれない。</description>
    </item>
    
    <item>
      <title>AtCoder Begineer Contest 051 D - Candidates of No Shortest Paths</title>
      <link>https://nryotaro.dev/posts/abc051d/</link>
      <pubDate>Mon, 14 Jun 2021 11:04:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/abc051d/</guid>
      <description>&lt;p&gt;ワーシャルフロイト法で全ての2頂点間の最短距離を求めた後、隣接する2頂点の辺のうち、2頂点の最短距離が辺の重みと異なる辺を数えれば通る。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoder Beginner Contest 205 D - Kth Excluded</title>
      <link>https://nryotaro.dev/posts/abc205d/</link>
      <pubDate>Mon, 14 Jun 2021 10:56:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/abc205d/</guid>
      <description>&lt;p&gt;\(K_q\)ごとに二部探索で答えを直接検索しても通すことができる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency</title>
      <link>https://nryotaro.dev/posts/windows_azure_storage/</link>
      <pubDate>Sat, 12 Jun 2021 17:21:52 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/windows_azure_storage/</guid>
      <description>&lt;p&gt;AzureのクラウドストレージサービスWindows Azure Storage(WAS) は、2008年の11月から本番運用されている。
保存できるデータの形式には、単なるファイル(Blob)だけでなく、テーブルのレコードとキューのメッセージがある。
ハードウェアの故障に備えたローカルでのレプリケーションだけでなく、地理的に離れたデータセンタにもレプリケーションを分散し、災害復旧にも備える。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Aves Del Sur</title>
      <link>https://nryotaro.dev/gallery/aves_del_sur/</link>
      <pubDate>Sat, 12 Jun 2021 17:11:25 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/aves_del_sur/</guid>
      <description>樽香が効いたチリワイン。開けたときの香りですぐにオークドシャルドネと分かるくらい香りがある。 美味しかった。</description>
    </item>
    
    <item>
      <title>Ilpasso</title>
      <link>https://nryotaro.dev/gallery/ilpasso/</link>
      <pubDate>Sat, 05 Jun 2021 17:03:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/ilpasso/</guid>
      <description>こういうモノクロの単純なラベルの赤に惹かれる。少し渋い感じがちょうどよくて好き。 寝る前の夜中の3時にあけて味わわずに飲んでしまったのが惜しい。</description>
    </item>
    
    <item>
      <title>Thrift: Scalable Cross-Language Services Implementation</title>
      <link>https://nryotaro.dev/posts/thrift/</link>
      <pubDate>Sun, 30 May 2021 19:59:48 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/thrift/</guid>
      <description>&lt;p&gt;ThriftはFacebookで開発されたRPCライブラリで、IDLからクライアントとサーバのコードを生成する。
Thrift自体は、大きくType, Transport, Protocol, Versioning, Processorの5つのコンポーネントからなる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The End of an Architectural Era (It&#39;s Time for a Complete Rewrite)</title>
      <link>https://nryotaro.dev/posts/the_end_of_an_architectural_era/</link>
      <pubDate>Sat, 29 May 2021 20:54:27 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_end_of_an_architectural_era/</guid>
      <description>&lt;p&gt;発表時期は2007年で、RDBMSの源流であるSystem Rが開発された70年代からハードウェアの性能や価格が変わったことを背景に、RDMSのアーキテクチャを既存技術の延長ではなく抜本から刷新し、いわゆるNoSQLを開発する必要性を主張している。
第一著者のMichael Stonebrakerは、後の2011年に今日全てのデータベースの要件をRDMSだけでは満たすことができないと主張する論文&lt;a href=&#34;https://cs.brown.edu/~ugur/fits_all.pdf&#34;&gt;&amp;ldquo;One Size Fits All&amp;rdquo;&lt;/a&gt;を出している。
表題の論文では、OLTPに特化したデータベースH-Storeを実装し、TPC-Cをベンチマークとして商用データベースと比較したところ、H-Storeの方が82倍高速だった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>C tes Du Roussillon Mas Las Cabes Domaine Gardi s</title>
      <link>https://nryotaro.dev/gallery/c_tes_du_roussillon_mas_las_cabes_domaine_gardi_s/</link>
      <pubDate>Sat, 29 May 2021 20:51:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/c_tes_du_roussillon_mas_las_cabes_domaine_gardi_s/</guid>
      <description>シラー。今までのんだシラーの中で一番好きかも。普段飲んでいるものより少し高いのが残念。</description>
    </item>
    
    <item>
      <title>Barista Chardonnay Bertus Fourie</title>
      <link>https://nryotaro.dev/gallery/barista_chardonnay_bertus_fourie/</link>
      <pubDate>Sat, 22 May 2021 19:05:11 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/barista_chardonnay_bertus_fourie/</guid>
      <description>樽香のあるシャルドネ。でもそこまで香りがきつくない。ちょうどいい。このくらいの香りがだめな人なら香りのついたシャルドネはダメかも。</description>
    </item>
    
    <item>
      <title>Burst Tries: A Fast, Efficient Data Structure for String Keys</title>
      <link>https://nryotaro.dev/posts/burst_tries/</link>
      <pubDate>Sat, 22 May 2021 02:32:18 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/burst_tries/</guid>
      <description>&lt;p&gt;trie木の一種のburst trieは、2分木よりもメモリ効率がよく、trie木と同じくらい速い。
内部は2種類のデータ構造からなり、trie木のほかに別のデータ構造もつかう。
どのデータ構造を使うかは要件次第で、実験では、連結リスト、二分探索木、スプレー木をつかった。
Burst trie内の別のデータ構造はcontainerとよばれる。
初期状態のburst trieは、1つのcontainerからなり、別のデータ構造そのものに等しい。
その後、containerの要素のアクセス頻度やヒット率にもとづくヒューリスティックな基準が満たされると、containerの要素をtrie木のノードにおきかえ、ノードの下に新しいcontainerをつくる。
新しくできたcontainerにも基準を適用し、適宜containerをtrieのノードにおきかえる。
以上の手続きより、与えられる文字列の頻度に合わせてtrie木と別のデータ構造を使い分けることができる。
性能はデータ分布の歪度に依存する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Suffix Arrays: A New Method for On-Line String Searches</title>
      <link>https://nryotaro.dev/posts/suffix_arrays/</link>
      <pubDate>Sun, 16 May 2021 01:45:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/suffix_arrays/</guid>
      <description>&lt;p&gt;接尾辞配列 Suffix Arrayは、長さ\(P\)の文字列が長さ\(W\)の文字列のどこに出現するかを時間計算量\(\mathcal{O}(P + \log N)\)で判定できるデータ構造で、接尾辞木 suffix treeよりもメモリ効率が3-5倍よい。
一方、検索の準備にかかる最悪時間計算量はsuffix treeよりも大きい。
suffix arrayの構築にかかる最悪時間計算量が\(\mathcal{O}(N\log N)\)に対して、suffix treeは\(\mathcal{O}(N)\)しかかからない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reserva Dos Amigos Touriga Nacional Vidigal Wines</title>
      <link>https://nryotaro.dev/gallery/reserva_dos_amigos_touriga_nacional_vidigal_wines/</link>
      <pubDate>Sun, 16 May 2021 01:39:11 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/reserva_dos_amigos_touriga_nacional_vidigal_wines/</guid>
      <description>ラベルは渋そうな感じだけど、少しスパイシーで果実味がある。おいしい。また買いたい。でもラベルがそんなに好みじゃない。</description>
    </item>
    
    <item>
      <title>リトル　ジェームズ　バスケット　プレス　ホワイト</title>
      <link>https://nryotaro.dev/gallery/little_james_basket_press_white/</link>
      <pubDate>Sun, 09 May 2021 11:09:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/little_james_basket_press_white/</guid>
      <description>辛口の白。変な樽香のない感じでおいしい。 ただ辛口の白で香りのないものはみんな同じ味に感じる。 そうなると値段で選んだほうがいいのかな。</description>
    </item>
    
    <item>
      <title>論文メモ HyperDex: A Distribute, searchable Key-Value Store</title>
      <link>https://nryotaro.dev/posts/hyperdex/</link>
      <pubDate>Fri, 07 May 2021 18:10:27 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/hyperdex/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dbdb.io/db/hyperdex&#34;&gt;HyperDex&lt;/a&gt;はキー以外の要素で値を検索できるキーバリューストアで、このキー以外の要素による検索速度がCassandraやMongoDBと比べて12-13倍速い。
キーバリューストアは、RDBMSとくらべて、処理速度は速く、検索条件の機能に乏しい。
HyperDexは、要素数と同数の次元からなるユークリッド空間を構成し、要素のハッシュ値で決まる座標に値をおくことで、キー以外でも高速に検索することを可能にする。
もとのドメインの順序関係を保持するハッシュ関数を使うため、範囲検索もあつかえる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Honoro Vera Organically Bodegas Juan Gil</title>
      <link>https://nryotaro.dev/gallery/honoro_vera_organically_bodegas_juan_gil/</link>
      <pubDate>Wed, 05 May 2021 13:03:39 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/honoro_vera_organically_bodegas_juan_gil/</guid>
      <description>オーガニックななんちゃらかんちゃら。少し渋い。</description>
    </item>
    
    <item>
      <title>50° Riesling trocken</title>
      <link>https://nryotaro.dev/gallery/50_riesling_trocken/</link>
      <pubDate>Mon, 03 May 2021 15:59:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/50_riesling_trocken/</guid>
      <description>リースリングだけど辛すぎない。辛いのが好きな人には物足りないかも。見かけによらず炭酸はない。</description>
    </item>
    
    <item>
      <title>論文メモ Integrating the UB-Tree into a Database System Kernel</title>
      <link>https://nryotaro.dev/posts/integrating_the_ub_tree_into_a_database_system_kernel/</link>
      <pubDate>Thu, 29 Apr 2021 16:30:04 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/integrating_the_ub_tree_into_a_database_system_kernel/</guid>
      <description>&lt;p&gt;商用DBMS TransBaseのインデックスをB木から&lt;a href=&#34;https://link.springer.com/chapter/10.1007/3-540-63343-X_48&#34;&gt;UB木&lt;/a&gt;に変え、 多次元アクセス（複数の列の範囲を指定するクエリ）を高速化した。
一見、商用DBMSのインデックスの拡張は複雑かつ高コストのように思える。
UB木は、B木をベースにしたインデックスで、B木のキーの生成とページ分割方法を細工し、範囲クエリを高速化できる。
範囲クエリに向きB木に似たUB木を使うことで、既存のDBMSの範囲クエリを人々の想定よりも簡単に高速化できることを例示した。
また、既存のインデックスの拡張用に提供されたインターフェースを使わず、インデックスを置き換えカーネルにUB木を密結合することで、クエリのオプティマイザを継続して利用できた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Designing Access Methods: The RUM Conjecture</title>
      <link>https://nryotaro.dev/posts/designing_access_methods/</link>
      <pubDate>Mon, 26 Apr 2021 10:29:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/designing_access_methods/</guid>
      <description>&lt;p&gt;データ構造は読み込み(Read, R), 更新(Update, U), 所要メモリ容量(Memory, M)にトリレンマを抱え、いかなるデータ構造でも、2つを最適化すると残る1つのオーバヘッドが悪化すると予想した。
Read, Update, Memoryの頭文字から、これをRUM Conjectureと名付けた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ドメーヌ・ダンデゾン　コート・デュ・ローヌ・ルージュ　ヴィエイユ・ヴィーニュ　2018</title>
      <link>https://nryotaro.dev/gallery/cotes_du_rhone_rouge/</link>
      <pubDate>Mon, 19 Apr 2021 15:04:04 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/cotes_du_rhone_rouge/</guid>
      <description>コルクが割れたけど、割れた位置が浅かったから深く刺しなおして、まっすぐ上にひっぱったらきれいに抜けた。 渋みも酸味もなく、果実の味がそのままする。</description>
    </item>
    
    <item>
      <title>論文メモ A comparison of Fractal Trees to Log-Structured Merge (LSM) Trees</title>
      <link>https://nryotaro.dev/posts/a_comparision_of_fractal_trees_to_lsm_trees/</link>
      <pubDate>Mon, 19 Apr 2021 14:59:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_comparision_of_fractal_trees_to_lsm_trees/</guid>
      <description>&lt;p&gt;Fractal Treeは、B+木のルートと節にバッファをもたせるデータ構造にあたる。
そのFractal TreeのamplificationをB+木やLSM Treeのそれと比較した。
議論になるamplificationは、write, read, spaceの3つで、write amplificationはアプリケーションが書き込むデータ量に対して実際にストレージに書き込まれたデータ量を表す。
read amplificationはクエリの実行に必要なI/Oの回数、space amplificationは仕組み上避けられない断片化や一時的なデータのコピーに該当する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Fast Intersection Algorithms for Sorted Sequences</title>
      <link>https://nryotaro.dev/posts/fast_intersection_algorithms_for_sorted_sequences/</link>
      <pubDate>Mon, 12 Apr 2021 13:27:26 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/fast_intersection_algorithms_for_sorted_sequences/</guid>
      <description>&lt;p&gt;ソートされたシーケンスの直積を高速に求めるアルゴリズム Double Binary Searchを示した。
2つのシーケンス\(D\), \(Q\)があり、\(\mid D\mid=n\), \(\mid Q\mid=m\), \(n &amp;gt;= m\)であれば、平均と最悪時間計算量が、それぞれ、\(\mathcal{O}(m\log(n/m))\), \(m\)になる。
本アルゴリズムは、Web検索エンジンで大きなシーケンスの直積を高速に求めるために開発された。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ドメーヌ・ポール・マス・シェ・マス・赤[2019]</title>
      <link>https://nryotaro.dev/gallery/domaine_paul_mas_chai_mas_rouge2019/</link>
      <pubDate>Sat, 10 Apr 2021 16:41:50 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/domaine_paul_mas_chai_mas_rouge2019/</guid>
      <description>派手なラベルを裏切るような感じ。最近飲んだものの中で一番酸味と渋みがバランスがとれてピーキーな感じがしなかった。</description>
    </item>
    
    <item>
      <title>La Vie Pinot Noir Domeniile Sahateni</title>
      <link>https://nryotaro.dev/gallery/lavie20210403/</link>
      <pubDate>Sat, 03 Apr 2021 23:12:57 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/lavie20210403/</guid>
      <description>4回目。でもいつも買っているリアルワインガイドの通販で売り切れていたからしばらく飲めない。あと1本だけ家にある。</description>
    </item>
    
    <item>
      <title>論文メモ The Log-Structured Merge-Tree (LSM-Tree)</title>
      <link>https://nryotaro.dev/posts/lsmtree/</link>
      <pubDate>Sat, 03 Apr 2021 22:26:27 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/lsmtree/</guid>
      <description>&lt;p&gt;LSM-Treeは、検索より挿入や削除が多い用途に向いたインデックス構造であり、例えば履歴テーブルやログの保存につかえる。
メモリにある1つ木とディスク上の1つ以上の木からなり、直近の挿入や削除をメモリの木で管理する。
メモリの木の大きさがしきい値を超えたとき、メモリの木の葉をディスクの木に移す。
移動時は、ディスク上の木の葉とメモリの葉をマージソートの要領でソートし、ソートした葉をディスクの新しい連続領域に書き込む。
連続領域に書き込み、アームの移動やディスクの回転を減らすことで、高速に挿入や削除ができる。
一方、検索速度は、複数の木を探索しなければならないために、1つの木でインデックスを構成するB木に劣る。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ The Design and Implementation of a Log-Structured File System</title>
      <link>https://nryotaro.dev/posts/the_design_and_implementation_of_a_log_structured_file_system/</link>
      <pubDate>Mon, 29 Mar 2021 16:18:21 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_design_and_implementation_of_a_log_structured_file_system/</guid>
      <description>&lt;p&gt;Log-structured file system(LFS)は、91年に提唱されたHDD向けのファイルシステムであり、バッファリングした変更をディスクの連続領域に一度に書き込むことで、小さなファイルを大量に高速で書き込める。
また、クラッシュからのリカバリも、ディスク全体ではなくチェックポイント以降に追記された箇所だけを検査すればよいので、高速になる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>シャトー・プティ・フレロン・キュヴェ・サラ[2016]</title>
      <link>https://nryotaro.dev/gallery/chateau_petit_freylon/</link>
      <pubDate>Sun, 28 Mar 2021 21:48:00 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/chateau_petit_freylon/</guid>
      <description>ラベルの高級感のとおり、ちょっと渋い。また飲んでもいい。</description>
    </item>
    
    <item>
      <title>ピノ・グリ（白） [&#39;20] ボートシェッド・ベイ</title>
      <link>https://nryotaro.dev/gallery/2020_boatshed_bay_marlborugh_pinot_gris/</link>
      <pubDate>Sun, 28 Mar 2021 21:43:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/2020_boatshed_bay_marlborugh_pinot_gris/</guid>
      <description>炭酸入り。味をあまり覚えていない。嫌いじゃないけどまた飲みたいという感じじゃない。 多分、炭酸そんなに好きじゃないのかも。</description>
    </item>
    
    <item>
      <title>論文メモ ARIES/IM: An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging</title>
      <link>https://nryotaro.dev/posts/ariesim/</link>
      <pubDate>Sun, 21 Mar 2021 12:08:11 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/ariesim/</guid>
      <description>&lt;p&gt;ARIES/IMは、B+木への直列化可能性のある並行処理とログ先行書き込み(Write-Ahead Logging, WAL)による復元を実現する。
ARIES/IMのベースには、先行研究の&lt;a href=&#34;https://web.stanford.edu/class/cs345d-01/rl/aries.pdf&#34;&gt;ARIES&lt;/a&gt;がある。
復元手順はARIESとほとんど変わらない。
キーの参照先にあるデータのレコードとキーを同じロックで保護したり、ロックの代わりにラッチを使ったりすることで、ロックの頻度を下げ、B+木の高速化を実現する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ベルフォンテーヌ Bellefontaine Pays d&#39;Oc Rouge</title>
      <link>https://nryotaro.dev/gallery/bellefontaine_rouge/</link>
      <pubDate>Fri, 19 Mar 2021 19:22:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/bellefontaine_rouge/</guid>
      <description>かなり好き。果実味が強く、渋みのない味でLa Vieと似た感じ。La Vieが1300円くらいで、こっちは800円くらい。すごい。また飲みたい。</description>
    </item>
    
    <item>
      <title>論文メモ The Ubiquitous B-Tree</title>
      <link>https://nryotaro.dev/posts/the_ubiquitous_b_tree/</link>
      <pubDate>Fri, 19 Mar 2021 19:17:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_ubiquitous_b_tree/</guid>
      <description>&lt;p&gt;B木とその派生データ構造のサーベイ論文で、とくにB+木に重点がおかれている。
B+木は、全てのキーを葉におき、葉を隣接リストでつなぐことで、B木が不得手なシーケンシャルなアクセスを改善する。
これにより、定数オーダーの時間と空間計算量で逐次的にキーにアクセスにできる。
ランダムな検索、挿入、削除にかかる時間、空間計算量はB木と変わらない。
以降はB木のデータ構造を前提としてB+木の概要を説明する。B木について知りたい場合は、&lt;a href=&#34;https://nryotaro.dev/posts/organization_and_maintenance_of_large_orderred_indexes/&#34;&gt;B木のオリジナル論文の解説記事&lt;/a&gt;を見てほしい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>エリソン マラン 2019</title>
      <link>https://nryotaro.dev/gallery/herisson_malin/</link>
      <pubDate>Sat, 13 Mar 2021 02:40:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/herisson_malin/</guid>
      <description>シャルドネの辛口。樽香で変な香りをつけておらず、少し炭酸があって、辛口。のみやすくて好き。</description>
    </item>
    
    <item>
      <title>論文メモ Robust Random Cut Forest Based Anomaly Detection On Streams</title>
      <link>https://nryotaro.dev/posts/robust_random_cut_forest_based_anomaly_detection_on_streams/</link>
      <pubDate>Fri, 12 Mar 2021 20:30:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/robust_random_cut_forest_based_anomaly_detection_on_streams/</guid>
      <description>&lt;p&gt;Robust Random Cut Forest(RRCT)はストリームデータ向きの異常検知の手法で、&lt;a href=&#34;https://www.aclweb.org/anthology/P16-1162/&#34;&gt;Amazon SageMaker&lt;/a&gt;から提供されている。
バッチデータと違い、ストリームデータは新たなデータが随時追加、修正される。
RRCTは、特徴の値でデータを2分するノードからなる木の集合であり、データを追加したときに木の集合であるモデルを大きく複雑するものを異常と判定する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Coteaux du Pont du Gard Cuvee des Galets</title>
      <link>https://nryotaro.dev/gallery/coteaux_du_pont_du_gard_cuvee_des_galets/</link>
      <pubDate>Sun, 07 Mar 2021 13:04:28 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/coteaux_du_pont_du_gard_cuvee_des_galets/</guid>
      <description>ラベルがかわいい。味はLa Vieに似てた。好き。</description>
    </item>
    
    <item>
      <title>La Vie その3</title>
      <link>https://nryotaro.dev/gallery/lavie3/</link>
      <pubDate>Sun, 07 Mar 2021 13:03:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/lavie3/</guid>
      <description>おいしかったので、3回目。</description>
    </item>
    
    <item>
      <title>Feudo Arancio Inzolia</title>
      <link>https://nryotaro.dev/gallery/feudo_arancio_inzolia/</link>
      <pubDate>Sun, 28 Feb 2021 12:16:50 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/feudo_arancio_inzolia/</guid>
      <description>辛口で変な香りのない素直な感じ。ラベルがいいよね。好き。</description>
    </item>
    
    <item>
      <title>論文メモ Organization and Maintenance of Large Ordered indexes</title>
      <link>https://nryotaro.dev/posts/organization_and_maintenance_of_large_orderred_indexes/</link>
      <pubDate>Sat, 27 Feb 2021 19:54:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/organization_and_maintenance_of_large_orderred_indexes/</guid>
      <description>&lt;p&gt;1972年に発表されたB木のオリジナルの論文。
\(I\)をインデックスの大きさ、\(k\)をハードウェア依存の値とすると、検索、挿入、削除の時間計算量は\(\log_kI\)になる。
ここでのインデックスは、固定長の\((x, \alpha)\)を要素とする連想配列で、最大\(2k\)個のキーを格納できる連続したページ上にある。
\(\alpha\)には、データを保存した1つ以上のレコードへのポインタが格納される。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Space/Time Trade-offs in Hash Coding with Allowable Errors</title>
      <link>https://nryotaro.dev/posts/space_time_trade-offs_in_hash_coding_with_allowable_errors/</link>
      <pubDate>Sat, 20 Feb 2021 16:46:44 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/space_time_trade-offs_in_hash_coding_with_allowable_errors/</guid>
      <description>&lt;p&gt;要素がハッシュ空間にあるかを高速に調べる手法で、ブルームフィルタの名で知られる。
偽陽性を許容し、ハッシュ値の保持に必要な空間を小さすることで、管理できるデータ件数を増やす。
例えば、&lt;a href=&#34;https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/&#34;&gt;データベースのストレージで使われており&lt;/a&gt;、検索するキーがデータベースに存在するか事前に確かめ、存在しないキーのための不要なディスクの読み取りを省いている。
偽陰性の誤りはない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>La Vie その2</title>
      <link>https://nryotaro.dev/gallery/la_vie2/</link>
      <pubDate>Sat, 20 Feb 2021 13:33:48 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/la_vie2/</guid>
      <description>おいしかったので、2回目。</description>
    </item>
    
    <item>
      <title>ヴィニウス・ヴィンヤーズ IGP ペイ・ドック・シャルドネ（白） [&#39;19] ジャン・クロード・マス　エステーツ＆ブランズ</title>
      <link>https://nryotaro.dev/gallery/vinus/</link>
      <pubDate>Sat, 20 Feb 2021 13:33:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/vinus/</guid>
      <description>香りがついた感じ。たぶん香りのつけられたシャルドネあまりすきじゃない。</description>
    </item>
    
    <item>
      <title>論文メモ Tree Indexing on Solid State Drives</title>
      <link>https://nryotaro.dev/posts/tree_index_on_solid_state_drives/</link>
      <pubDate>Mon, 15 Feb 2021 15:53:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/tree_index_on_solid_state_drives/</guid>
      <description>&lt;p&gt;SSDのランダムな書き込みを高速化するために、インデックスのためのデータ構造FD-treeを提案した。
FD-treeのエントリ数が\(n\), ページサイズが\(B\)であれば、ランダムな読み込みと書き込みの計算量はともに\(\mathcal{O}(\log_B(n))\)になる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IGP ドック・dA・メルロ[2017] ドメーヌ・アストラック</title>
      <link>https://nryotaro.dev/gallery/d_a_merlot/</link>
      <pubDate>Sun, 14 Feb 2021 11:31:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/d_a_merlot/</guid>
      <description>前回飲んだものの赤の方。 最後に飲んだ赤がLa Vieの薄めの飲みやすいものだったけど、こっちは別に薄くない。 おいしい。</description>
    </item>
    
    <item>
      <title>論文メモ Isolation Forest</title>
      <link>https://nryotaro.dev/posts/isolation_forest/</link>
      <pubDate>Sat, 13 Feb 2021 19:52:34 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/isolation_forest/</guid>
      <description>&lt;p&gt;Isolation Forestは、完全二分木による異常検知の手法で、iForestともよばれる。時間計算量が線形で、ハイパーパラメタがわずか2つで、メモリ効率がよい。
時間、空間計算量が少なく高次元のデータに向く。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IGP ドック・dA・シャルドネ（白）[2018] </title>
      <link>https://nryotaro.dev/gallery/d_a_chardonnay/</link>
      <pubDate>Sun, 07 Feb 2021 13:33:13 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/d_a_chardonnay/</guid>
      <description>IGP ドック・dA・シャルドネ（白）[2018] ドメーヌ・アストラック。 前回飲んだ白のDARK HORSEがわざとらしく香りをつけていたのに対して、妙な自己主張のない自然な辛口の白。 こっちのほうが断然おいしい。自分は変に樽香をつけたシャルドネが苦手かもしれない。 これはまた買うつもり。</description>
    </item>
    
    <item>
      <title>論文メモ TAO: Facebook&#39;s Distributed Data Store for the Social Graph</title>
      <link>https://nryotaro.dev/posts/tao_facebooks_distributed_data_store_for_social_graph/</link>
      <pubDate>Thu, 04 Feb 2021 18:47:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/tao_facebooks_distributed_data_store_for_social_graph/</guid>
      <description>&lt;p&gt;TAOは、Facebookで開発されたソーシャルグラフのためのマルチリージョンの分散システムで、秒間10億件の読み込みと数百万件の書き込みの性能を発揮する。
Facebookは、もともとソーシャルグラフを、MySQLに保存し、memcacheでキャッシュし、PHPで問いあわせるシステムで構成していた。
TAOは、そのシステムの現状を引きつぎ、MySQLをストレージに採用している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>La Vie PINOT NOIR 2018</title>
      <link>https://nryotaro.dev/gallery/la_vie/</link>
      <pubDate>Sun, 31 Jan 2021 00:58:40 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/la_vie/</guid>
      <description>某所でおすすめされたワイン。香りがよく、飲みやすい。 1300円とは信じられないくらいおいしい。 明日3本くらい追加で買うつもり。</description>
    </item>
    
    <item>
      <title>論文メモ The Daily Life of Software Engineers during the COVID-19 Pandemic</title>
      <link>https://nryotaro.dev/posts/the_daily_life_of_software_engineers_during_the_covid_19_pandemic/</link>
      <pubDate>Sat, 30 Jan 2021 13:19:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_daily_life_of_software_engineers_during_the_covid_19_pandemic/</guid>
      <description>&lt;p&gt;COVID19でソフトウェア開発者が在宅稼動(WFH)をはじめたことによる稼動時間の使い方、良好性(Well-being), 生産性の変化を調査した。
ロックダウンを実施した国々のエンジニア500名の中から、2020年4月20日から26日の一波について192名、2020年5月4日から10日の二波について184名を選び、サンプルを集めた。
結果、会社での勤務とWFHの間で、稼動時間の使い方はほぼ変わっていないかった。
また、一波において休憩と生産性の間に負の相関がみられたが、それ以外では良好性、生産性、社会性、心理の4つと特定の業務内容の間に相関関係はみられなかった。
結果、組織やエンジニアにとってWFHそれ自体が課題になるわけではないと結論づけている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DARK HORSE CHARDONNAY 2018</title>
      <link>https://nryotaro.dev/gallery/darkhorse/</link>
      <pubDate>Tue, 26 Jan 2021 01:04:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/darkhorse/</guid>
      <description>アメリカのシャネルドネ。1本自宅近くのスーパーで買ったあとに、飲んでもないのに、Amazonでもう一本買った。 飲んだ瞬間、シャルドネらしからぬ強い香りがした。商品説明には濃いとあったが、ここまで強いと思っておらず、正直きついと感じた。たぶん二度と買わない。</description>
    </item>
    
    <item>
      <title>論文メモ Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</title>
      <link>https://nryotaro.dev/posts/dapper/</link>
      <pubDate>Sat, 23 Jan 2021 14:50:16 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dapper/</guid>
      <description>&lt;p&gt;分散トレーシングシステムDapperをGoogle社内で開発、デプロイ、運用した知見がまとめられている。
運用期間は2年にわたる。
Dapperをもとに、TwitterはZipkinを、UberはJaegerを&lt;a href=&#34;https://www.oreilly.co.jp/books/9784873119038/&#34;&gt;開発した&lt;/a&gt;。
Dapper以前に開発された&lt;a href=&#34;https://www.usenix.org/legacy/publications/library/proceedings/hotos03/tech/full_papers/barham/barham_html/paper.html&#34;&gt;Magpie&lt;/a&gt;や&lt;a href=&#34;https://www.usenix.org/conference/nsdi-07/x-trace-pervasive-network-tracing-framework&#34;&gt;X-Trace&lt;/a&gt;との違いは、サンプリングされたリクエストのみをトレースし負荷を下げるられたり、少数の共通ライブラリだけを測定対象したりする点にある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Convolutional 2D Knowledge Graph Embeddings</title>
      <link>https://nryotaro.dev/posts/convolutional_2d_knowledge_graph_embeddings/</link>
      <pubDate>Mon, 11 Jan 2021 17:24:16 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/convolutional_2d_knowledge_graph_embeddings/</guid>
      <description>&lt;p&gt;ナレッジグラフの未知のリンクを予測するモデルは、一般に大きなグラフをあつかえるようにネットワークを浅くし、処理性能の高速化をはかる。一方、代償として層の深いモデルと比べて表現力を欠く。
提唱されるネットワークConvEは、畳み込み層をつかった深めのネットワークで予測性能の向上をはかる。
層が深くなると計算コストの増加や過学習が課題になるが、先行研究の&lt;a href=&#34;https://arxiv.org/abs/1412.6575&#34;&gt;DistMult&lt;/a&gt;や&lt;a href=&#34;https://arxiv.org/abs/1703.06103&#34;&gt;R-GCN&lt;/a&gt;と比べたConvEのパラメタ数は1/8や1/17であり、パラメタ効率がよい。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Multiversion Concurrency Control - Theory and Algorithms</title>
      <link>https://nryotaro.dev/posts/mvcc/</link>
      <pubDate>Wed, 06 Jan 2021 16:02:37 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/mvcc/</guid>
      <description>&lt;p&gt;データベースのトランザクション制御Multiversion Concurrency Control(MVCC, 多版型同時実行制御) 下のトランザクションが逐次実行と等価な結果になる条件を定義した。
また、その定義を既存のMVCCのアルゴリズム3つにあてはめ、アルゴリズムの正しさを確かめた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Paxos Made Simple</title>
      <link>https://nryotaro.dev/posts/paxos_made_simple/</link>
      <pubDate>Thu, 31 Dec 2020 19:17:13 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/paxos_made_simple/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf&#34;&gt;The Part-Time Parliament&lt;/a&gt;で提唱された分散含意アルゴリズムPaxosをLamport自身が平易に解説した。
エージェントの処理速度やメッセージが配信されるまでの長さに仮定はない。
メッセージは複製、喪失してもよい。
他方で、ビザンチン将軍問題は扱わず、メッセージが壊れることは考えない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Impossibility of Distributed Consensus with One Faulty Process</title>
      <link>https://nryotaro.dev/posts/impossibility_of_distributed_consensus_with_one_faulty_process/</link>
      <pubDate>Fri, 18 Dec 2020 22:18:59 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/impossibility_of_distributed_consensus_with_one_faulty_process/</guid>
      <description>&lt;p&gt;プロセスが1つでもクラッシュしうる場合には常に含意を保証できる完全な非同期アルゴリズムは存在しないことを示した。
この定理はFLP帰結とよばれる。
FLPは、著者Fischer, Lynch, Patersonの頭文字に由来する。
ここでの完全は、プロセスの処理速度やメッセージの配信遅延に仮定をおかず、同期クロックがないためにタイムアウトを使えず、ほかのプロセスが別のプロセスの障害を検知できないことを意味する。
いいかえると、含意アルゴリズムを実装するには上のいずれかを仮定しなければないことを示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Availability in Globally Distributed Storage Systems</title>
      <link>https://nryotaro.dev/posts/availability_in_globally_distributed_storage_systems/</link>
      <pubDate>Sat, 12 Dec 2020 20:53:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/availability_in_globally_distributed_storage_systems/</guid>
      <description>&lt;p&gt;Googleの分散ストレージで生じた障害の統計をとり、ストレージの可用性の予測モデルを提唱した。
ディスク、ノード、ラックなどハードウェアの粒度を変えて、粒度ごとの平均故障間隔を計測し、故障原因を分類した。
2分のウィンドウで生じた障害をグループにまとめると、ほとんどの障害が同時多発的な障害の一部であった。
20以上のノードを巻き込む大きな障害では、別々のラックにあるノードに障害が起きるよりも、特定のラックのノードに障害が起きることが多かった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ What You Always Wanted to Know About Datalog</title>
      <link>https://nryotaro.dev/posts/what_you_always_wanted_to_know_about_datalog/</link>
      <pubDate>Sat, 12 Dec 2020 18:02:01 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/what_you_always_wanted_to_know_about_datalog/</guid>
      <description>&lt;p&gt;論理型のデータベースのクエリ言語Datalogの構文、意味論、最適化が解説されている。
最適化の節はサーベイ論文の形式で、最適化を分類し、各種類の先行研究に案内がある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Translating Embeddings for Modeling Multi-relational Data</title>
      <link>https://nryotaro.dev/posts/translating_embeddings_for_modeling_multi_relational_data/</link>
      <pubDate>Sat, 05 Dec 2020 20:21:25 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/translating_embeddings_for_modeling_multi_relational_data/</guid>
      <description>&lt;p&gt;ナレッジグラフを低次元のベクトル空間に埋め込むアルゴリズムTransEを提案した。
エンティティは複数種類のラベルをもってよく、埋め込まれたエンティティやラベルの距離を計算することで、入力されたグラフに欠けているリンクを推定できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ The Declarative Imperative</title>
      <link>https://nryotaro.dev/posts/the_declarative_imperative/</link>
      <pubDate>Sat, 05 Dec 2020 16:19:30 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_declarative_imperative/</guid>
      <description>&lt;p&gt;宣言型言語Datalogを拡張した言語でネットワークプロトコルと分散システムを開発した7年間の経験から、次世代の並列分散プログラミング言語の基礎になる理論上の予想を4つ提唱した。
Datalogは、論理プログラミング言語Prologのサブセットの構文をもった言語である。
予想の説明に使われるDatalogの拡張言語&lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-173.pdf&#34;&gt;Dedalus&lt;/a&gt;は、分散システムのノードが互いの時刻を直接的に推論できない性質を、送受信するデータから離れたノードの時刻を推論する問題に帰着する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Bigtable: A Distributed Storage System for Structured Data</title>
      <link>https://nryotaro.dev/posts/bigtable/</link>
      <pubDate>Mon, 30 Nov 2020 20:18:18 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bigtable/</guid>
      <description>&lt;p&gt;Bigtableは、数千のコモディティサーバ上でペタバイト級のデータをホスティングできる分散ストレージであり、行、列、タイムスタンプをキーとして値の文字列を保存する。
論文の発表された2005年時点で、検索エンジンのインデックス, Google Earth, Google Financeなどの多様なアプリケーションの実装に使用されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Aerospike: Architecture of a Real-Time Operational Database</title>
      <link>https://nryotaro.dev/posts/aerospike/</link>
      <pubDate>Sat, 28 Nov 2020 21:22:34 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/aerospike/</guid>
      <description>&lt;p&gt;Aerospikeは、高い対障害性をもった分散データベースで、正式にはCitrusleafという。
CPU, DRAM, HDDやSSDを搭載したコモディティなサーバでクラスタを組める。
ノード同士はTCP/IPで通信し、シェアードナッシングなクラスタを構成する。
処理性能の向上のために、スケールアウトだけでなく、スケールアップも重視しており、ハードウェアを意識した実装で高速化をはかっている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Spanner: Google&#39;s Globally Distributed Database</title>
      <link>https://nryotaro.dev/posts/spanner/</link>
      <pubDate>Fri, 27 Nov 2020 16:56:57 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/spanner/</guid>
      <description>&lt;p&gt;Spannerは、世界中のデータセンタにデータを複製する高可用な分散データベースで、&lt;a href=&#34;http://bitsavers.trailing-edge.com/pdf/xerox/parc/techReports/CSL-81-8_Information_Storage_in_a_Decentralized_Computer_System.pdf&#34;&gt;外部整合性&lt;/a&gt;のある分散トランザクションを保証する。
ユーザからみると半リレーショナルなデータモデルのデータベースであり、各テーブルに一つ以上の順序つき主キーが必要なところがリレーショナルデータモデルと違う。
一方、内部は文字列とタイムスタンプの組をキーにしたキーバリューストアであり、Single Paxos状態機械でデータの一貫性を守りながら複数のデータセンタにデータを複製する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ A Relational Model of Data for Large Shared Data Banks</title>
      <link>https://nryotaro.dev/posts/a_relational_model_of_data_for_large_shared_data_banks/</link>
      <pubDate>Sat, 21 Nov 2020 13:51:57 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_relational_model_of_data_for_large_shared_data_banks/</guid>
      <description>&lt;p&gt;Coddがリレーショナルデータモデルを提唱した70年の論文。
データをリレーション(SQLでのテーブル)として束ね、リレーションを順序なしのタプル(SQLの行)で構成する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ On Designing and Deploying Internet-Scale Services</title>
      <link>https://nryotaro.dev/posts/on_designing_and_deploying_internet-scale_services/</link>
      <pubDate>Fri, 20 Nov 2020 18:55:51 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/on_designing_and_deploying_internet-scale_services/</guid>
      <description>&lt;p&gt;MSNとWindows Liveで培われたシステム管理者による運用負担を減らすためのベストプラクティス集。
07年に発表された。
プラクティス集は、10のグループに分かれ、それぞれ複数のアドバイスからなる。
特徴的な内容に絞って以下に要約する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Weighted Voting for Replicated Data</title>
      <link>https://nryotaro.dev/posts/weighted_voting_for_replicated_data/</link>
      <pubDate>Thu, 19 Nov 2020 21:21:33 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/weighted_voting_for_replicated_data/</guid>
      <description>&lt;p&gt;1979年に発表されたレプリケーション管理のクオーラムモデルのアルゴリズムの論文で、以前紹介したように&lt;a href=&#34;https://awsmedia.awsstatic-china.com/blog/2017/aurora-design-considerations-paper.pdf&#34;&gt;Amazon Aurora&lt;/a&gt;や&lt;a href=&#34;https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf&#34;&gt;Dynamo&lt;/a&gt;で採用されている。
ファイルの読み込みや書き込みのトランザクションは、複製されたファイルのもつ票を集め、所定の数、クオーラムを越えたときのみ実行される。
これにより、読み込み、書き込みの線形の一貫性が保証され、実行中のトランザクションが高々一つであるかのように見せかけられる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases</title>
      <link>https://nryotaro.dev/posts/amazon_aurora/</link>
      <pubDate>Fri, 13 Nov 2020 17:51:53 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/amazon_aurora/</guid>
      <description>&lt;p&gt;AWSで提供されるRDBM, Amazon Auroraのアーキテクチャを解説した論文。
分散システムをクラウドにおく場合、計算やIOはノードに分散され、ボトルネックではなくなる。
そして、ボトルネックは、DBインスタンスとストレージ間のネットワークになる。
この仮説もと、プライマリインスタンスが、別テナントのストレージに直接Redoログを送ることで、レプリカインスタンスとストレージ間の負荷を減らし、処理性能の向上をはかる。
また、レプリケーションのために、MySQLがRedoログだけでなくバイナリログなど複数種類のログをスレーブに送るのに対し、AuroraはRedoログだけを転送する。
これにより、リカバリや縮退、フェールオーバの性能も向上している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ &#34;One Size Fits All&#34;: An Idea Whose Time Has Come and Gone</title>
      <link>https://nryotaro.dev/posts/one_size_fit_all/</link>
      <pubDate>Fri, 13 Nov 2020 16:05:23 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/one_size_fit_all/</guid>
      <description>&lt;p&gt;2011年に発表された論文で、これまでのようにDBMSを様々なデータ中心のアプリケーションに利用することがデータベース市場で受け入れられなくなったと主張する。
データウェアハウスとストリーミング処理を例にとり、これらに特化したデータベースをDBMSで代用することの限界が説明されている。
表題の&amp;quot;One Size Fits All&amp;quot;はフリーサイズ、転じて、万能、汎用的を意味する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Dynamo: Amazon&#39;s Highly Available Key-value Store</title>
      <link>https://nryotaro.dev/posts/dynamo_amazons_highly_available_key_value_store/</link>
      <pubDate>Fri, 06 Nov 2020 17:06:40 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/dynamo_amazons_highly_available_key_value_store/</guid>
      <description>&lt;p&gt;Amazonで社内運用されている高可用性のKVS, Dynamoのアーキテクチャを解説している。
まぎらわしいが、Dynamoは、AWSサービスのDynamo DBとは違う&lt;a href=&#34;https://www.oreilly.co.jp/books/9784873118703/&#34;&gt;*&lt;/a&gt;。
Dynamoは、リーダーレスレプリケーションモデルで、Dynamo DBはシングルリーダレプリケーションモデルを採用している。
Dynamoは、高信頼性が必要なシステムの状態管理に使用される。
その用途から、トランザクション分離レベルのサポートは不要で、可用性を優先するために結果整合性を許容する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Zero-shot Word Sense Disambiguation using Sense Definition Embeddings</title>
      <link>https://nryotaro.dev/posts/zero_shot_word_sense_disambiguation_using_sense_definition_embeddings/</link>
      <pubDate>Fri, 30 Oct 2020 18:58:39 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/zero_shot_word_sense_disambiguation_using_sense_definition_embeddings/</guid>
      <description>&lt;p&gt;語義曖昧性解消のためのアーキテクチャ, Extended WSD Incorporating Sense Embeddings(EWISE)を発表した。
EWISEは単語の意味をアノテーションしあテキストと辞書を教師データにもちいる。
実験では、辞書にWordNetをつかい、概念同士の上下関係や関係を示す分散表現を獲得する。
学習であたえられていない意味を推定するために、離散値ではなく分散表現でラベルの意味を表現する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Simple Testing Can Prevent Most Critical Failures</title>
      <link>https://nryotaro.dev/posts/simple_testing_can_prevent_most_critical_failures/</link>
      <pubDate>Thu, 29 Oct 2020 20:53:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/simple_testing_can_prevent_most_critical_failures/</guid>
      <description>&lt;p&gt;5つの分散システムのバグのうち198件を無作為に抽出、調査したところ、エラーハンドリングに対する単純なテストが有効であることが分かった。
198件のうちの48件は、論文でcatastrophic failuresと形容された、多くのユーザに影響を与える障害が占めた。
調査対象は、Cassandra, HBase, HDFS, Hadoop MapReduce, Redisの5つである。
catastrophic failuresの35%の原因は、エラーハンドラがログの出力だけしかしていない、過剰に上位の例外クラスが宣言されたcatch構文で例外を処理していること、例外に&lt;code&gt;FIXME&lt;/code&gt;, &lt;code&gt;TODO&lt;/code&gt;コメントがある、の3パターンに分類された。
Javaのバイトコードから以上の3パターンを検出するツールを実装し、9種類の分散システムに適用したことで、121件の未知のバグを特定することができた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ From Diversity by Numbers to Diversity as Process</title>
      <link>https://nryotaro.dev/posts/from_diversity_by_numbers_to_diversity_as_process/</link>
      <pubDate>Fri, 16 Oct 2020 23:58:44 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/from_diversity_by_numbers_to_diversity_as_process/</guid>
      <description>開発におけるブレーンストーミングが、マイノリティに属する開発者の満足度の向上に貢献することを実験的に示した。 ここでの開発は、ハッカソンのような短時間かつ集中が求められるものが想定されている。 満足度は、開発プロセスと成果物に対するもので分けて扱われ、どちらの観点でもブレーンストーミングは満足度に対してよい効果をもたらした。
  論文をこちらからダウンロードできます。  </description>
    </item>
    
    <item>
      <title>論文メモ End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</title>
      <link>https://nryotaro.dev/posts/end_to_end_sequence_labeling_via_bi_directional_lstm_cnns_crf/</link>
      <pubDate>Fri, 16 Oct 2020 22:17:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/end_to_end_sequence_labeling_via_bi_directional_lstm_cnns_crf/</guid>
      <description>&lt;p&gt;タスク固有の特徴を使わないEnd to Endの系列ラベリングのためのネットワークアーキテクチャを発表した。
実験では、Penn Treebank WSJの品詞タグ付けで97.55%のaccuracy, CoNLL 2003の固有表現抽出で91.21%のF1値を発揮し、発表当時の先行研究を上まわる性能を示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Becoming Agile: A Grounded Theory of Agile Transitions in Practice</title>
      <link>https://nryotaro.dev/posts/becoming_agile/</link>
      <pubDate>Fri, 16 Oct 2020 17:34:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/becoming_agile/</guid>
      <description>&lt;p&gt;アジャイル開発に熟練する過程でチームに生じる変化をグラウンデッドセオリーで調査した。
調査のために、ニュージーランド、オーストラリア、アメリカ、インド、ポルトガルの5カ国から18のチームを選び、その中の31名に半構造化された約1時間の面接を実施した。
面接では、職歴、自己組織化の実践、仕事のわりあて方の3つを話してもらった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Software Development Waste</title>
      <link>https://nryotaro.dev/posts/software_development_waste/</link>
      <pubDate>Sun, 11 Oct 2020 15:02:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/software_development_waste/</guid>
      <description>&lt;p&gt;Pivotal Labs(Pivoital社の一部門、PivotalはSpring Frameworkを開発している会社。昨年VM Wareに買収された？)における8プロジェクトを、グラウンデットセオリーにしたがって参与観察し、ソフトウェア開発においる無駄を特定し、無駄を9つの区分に分類した。
論文では、無駄は「リソースを使っのに顧客にとっての価値を生みださなかった活動」と定義されている。
調査期間は2年5ヶ月で、調査結果は、ソフトウェア開発者、インタラクションデザイナ、プロダクトマネジャーからなる33名のステークホルダに面接した結果もふまえてある。
分類のほかに、無駄を生みだす二項対立や原因にも言及されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ End-to-end Neural Coreference Resolution</title>
      <link>https://nryotaro.dev/posts/end_to_end_neural_coreference_resolution/</link>
      <pubDate>Sat, 10 Oct 2020 00:12:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/end_to_end_neural_coreference_resolution/</guid>
      <description>&lt;p&gt;ニューラルネットワークによる共参照解析の手法で、End-to-Endとあるように、構文解析やルールベースの参照表現に頼らず、先行研究を上回る性能を発揮した。
文書中の全ての単語系列を参照表現の候補とみなし、ある単語系列の組が照応関係にある確率の分布を学習する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Build it yourself! Homegrown Tools in a Large Software Company</title>
      <link>https://nryotaro.dev/posts/build_it_yourself/</link>
      <pubDate>Fri, 02 Oct 2020 20:22:20 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/build_it_yourself/</guid>
      <description>&lt;p&gt;マイクロソフトにおいて、誰が(RQ1)、どんな(RQ2)自作ツールを、どのような動機(RQ3)で、いつ(RQ3)開発し、どのように普及するのか(RQ4)を調査した。
調査結果では、大多数の開発者はツールを自作し、そのほとんどは所属するチームの外までは普及せす、ツールの使用者と共同開発者は一人以上いることが多かった。
受容的な組織文化はツールの自作を促進し、また、自作ツールは組織に大きな影響をあたえる可能性があることを示唆している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Universal Sentence Encoder</title>
      <link>https://nryotaro.dev/posts/universal_sentence_encoder/</link>
      <pubDate>Fri, 02 Oct 2020 18:20:29 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/universal_sentence_encoder/</guid>
      <description>&lt;p&gt;転移学習のための文の分散表現を獲得するモデルを提案した。
提案されたモデルは2つで、両者には精度と計算量の間にトレードオフがある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ How much Up-Front? A Grounded Theory of Agile Architecture</title>
      <link>https://nryotaro.dev/posts/how_much_up-front/</link>
      <pubDate>Fri, 25 Sep 2020 20:40:50 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/how_much_up-front/</guid>
      <description>&lt;p&gt;「アジャイル開発において、実装前のアーキテクチャ設計にどれだけ工数を割くべきか？」という問いの回答指針を、44名のソフトウェア開発者に面接し、その結果からグラウンデッド・セオリーで提案した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Character-level Convolutional Networks for Text Classification</title>
      <link>https://nryotaro.dev/posts/character_level_convolutional_networks_for_text_classification/</link>
      <pubDate>Fri, 25 Sep 2020 18:10:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/character_level_convolutional_networks_for_text_classification/</guid>
      <description>&lt;p&gt;文字単位のCNNによる文書の分類を、ほかのモデルと比較して評価した。
先行研究より、CNNを訓練するときは大量の教師データが必要になると分かっている。
のため、比較のためのデータセットは大きく、訓練データの事例数は最低でも12万件におよぶ。
文字単位のCNNに大量の訓練データをあたえれば、別途単語の意味をモデルにあたえずとも性能を発揮することを示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ What Makes A Great Software Engineer?</title>
      <link>https://nryotaro.dev/posts/what_makes_a_great_software_engineer/</link>
      <pubDate>Fri, 18 Sep 2020 20:19:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/what_makes_a_great_software_engineer/</guid>
      <description>&lt;p&gt;優れたエンジニアの特徴を知るために、マイクロソフト社のアーキテクト59名と半構造化された面接をし、グラウンデッドセオリーで内容から特徴を抽出した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
      <link>https://nryotaro.dev/posts/maml/</link>
      <pubDate>Fri, 18 Sep 2020 17:54:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/maml/</guid>
      <description>&lt;p&gt;表題の略称MAMLで知られるメタ学習であり、少ない教師データで新しいタスクに対応することを目的としている。
Model-Agnosticとあるように、MAMLの汎用性は高く、勾配法をもちいるモデルであえば適用可能である。
論文には、教師あり学習だけでなく強化学習の事例もある。
さまざまなタスクに適した初期パラメタを見つけ、データ件数の削減をねらう。
目的のパラメタを求めるためには、複数のタスクを用意し、これらの損失関数の合計値を最小にするように勾配法でパラメタの更新を繰り返す。
最後に更新されたパラメタをモデルの初期値に設定する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Truth in Advertising: The Hidden Cost of Mobile Ads for Software Developers</title>
      <link>https://nryotaro.dev/posts/truth_in_advertising/</link>
      <pubDate>Fri, 11 Sep 2020 20:01:30 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/truth_in_advertising/</guid>
      <description>&lt;p&gt;モバイルアプリを無料で提供し、アプリ内の広告で収益をえるビジネスが普及している。
このビジネスモデルでは、一見、ユーザには、画面に広告を表示されること以外のコストがないように思える。
表題の論文は、それ以外のコストを確かめるために、CPU, 電力, ネットワーク, 広告関連のためのリリース, アプリケーションへのユーザからの評価へ広告が及ぼす影響を調査した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Skip-Thought Vectors</title>
      <link>https://nryotaro.dev/posts/skip_thought_vectors/</link>
      <pubDate>Fri, 11 Sep 2020 16:09:15 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/skip_thought_vectors/</guid>
      <description>&lt;p&gt;様々なタスクで性能を発揮できる文の分散表現を生成するモデルSkip-Thougtを提案した。
表題のSkip-Tought Vectorsは、Skip-Toughtで生成されるベクトルである。
Skip-Thoughtは、文書を入力とする教師なし学習であり、与えられた文から隣接する左右の文を推定できるようにパラメタを学習する。
学習後は、語彙を増やすために、Word2Vecの単語のベクトルからSkip-Toughtのベクトルを推定するための正則化なしの回帰モデルを学習させる。
学習データにない単語のベクトルを回帰モデルの推定結果で代用し、未知の単語を含む文の分散表現もつくれるようにする。
Skip-Toughtを8タスクに適用し、汎用性を確かめた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Beyond Accuracy: Behavioral Testing of NLP Models with CHECKLIST</title>
      <link>https://nryotaro.dev/posts/beyond_accuracy/</link>
      <pubDate>Fri, 04 Sep 2020 21:54:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/beyond_accuracy/</guid>
      <description>&lt;p&gt;ホールドアウト法にかわる自然言語処理モデルの汎化性能を評価するための手法CHECKLISTを提案した。
テストデータと訓練データが同じ方法で集められたときなど、ホールドアウト法はモデルを過大評価することがある。
CHECKLISTは、ソフトウェア開発のブラックボックステストにならい、半自動生成したテストデータで汎化性能を評価する。
CHECKLISTの汎用性と性能を評価するために、感情分析、Quoraの重複質問検出、読解の3タスクについて、商用やSoTAに近いモデルを学習させ、CHECKLISTでモデルがあつかえない入力パターンをどれだけ生成できるか実験した。
感情分析の評価には、Microsoft, Google, AmazonのAPIとBERT, RoBERTaを使い、重複検出にはBERTとRoBERTa, 読解にはBERTを使用した。
CHECKLISTはOSSとして&lt;a href=&#34;https://github.com/marcotcr/checklist&#34;&gt;公開&lt;/a&gt;されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Why Good Developers Write Bad Code: An Observational Case Study of the Impacts of Organizational Factors on Software Quality</title>
      <link>https://nryotaro.dev/posts/why_good_developers_write_bad_code/</link>
      <pubDate>Fri, 04 Sep 2020 20:54:26 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/why_good_developers_write_bad_code/</guid>
      <description>&lt;p&gt;創立40年以上の電気通信系企業における内製システムのリプレースプロジェクトを観察し、コードに悪影響をおよぼす10の組織的な要因をまとめた。
表題には、&amp;ldquo;Good Developers&amp;quot;とあるが、開発者のスキルの高さは議論されていない。
列挙された要因は、包括的ではなく、また、あくまでコードの品質を悪化させるものであり、プロジェクトの失敗に直結するわけではない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ A Comparative Study of Programming Lanugages in Rosetta Code</title>
      <link>https://nryotaro.dev/posts/a_comparative_study_of_programming_languages_in_rosetta_code/</link>
      <pubDate>Fri, 28 Aug 2020 20:19:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_comparative_study_of_programming_languages_in_rosetta_code/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://rosettacode.org/wiki/Rosetta_Code&#34;&gt;Rosetta Code&lt;/a&gt;に投稿された実装で言語の性能を比較した。
比較した言語は、Ruby, F#, Java, C#, Go, C, Python, Haskellの8つである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ SQuAD: 100,000&#43; Questions for Machine Comprehension of Text</title>
      <link>https://nryotaro.dev/posts/squad/</link>
      <pubDate>Fri, 28 Aug 2020 19:36:42 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/squad/</guid>
      <description>&lt;p&gt;読解タスクのテストデータセットSQuADをつくり、ロジスティック回帰で難易度を評価した。
難易度は、ベースラインのF1スコアが20%, 強いモデルで51.0%, 人間で86.8%程度である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Extensible Effects An Alternative to Monad Transformers</title>
      <link>https://nryotaro.dev/posts/extensible_effects/</link>
      <pubDate>Sun, 23 Aug 2020 13:34:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/extensible_effects/</guid>
      <description>&lt;p&gt;モナド変換子にかわるモナドの合成方法Extensible Effectsの実装を示す。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ When and Why Your Code Starts to Smell Bad</title>
      <link>https://nryotaro.dev/posts/when_and_why_your_code_starts_to_smell_bad/</link>
      <pubDate>Fri, 21 Aug 2020 18:03:14 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/when_and_why_your_code_starts_to_smell_bad/</guid>
      <description>&lt;p&gt;200件のAndroid, Apache, EclipseのOSSプロジェクトのコミット履歴を調査し、不吉な匂いが生じる原因と理由を調査した。
常識では、改修の繰返しによって匂いのない既存のコードに匂いが生じると考えられているが、これに反して、不吉な匂いのするコードのほとんどが作成時点で不吉な匂いを出していたことを明らかにした。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Extracting and Composing Robust Features with Denoising Autoencoders</title>
      <link>https://nryotaro.dev/posts/extracting_and_composing_robust_features_with_denoising_autoencoders/</link>
      <pubDate>Fri, 21 Aug 2020 17:04:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/extracting_and_composing_robust_features_with_denoising_autoencoders/</guid>
      <description>&lt;p&gt;ノイズを含む入力からノイズのない入力を復元するように学習すると、次元圧縮の性能を向上できることを示した。
層の深いautoencoderを学習するには、良い初期値を与えなければらないことが知られていた。
&lt;a href=&#34;https://www.cs.toronto.edu/~hinton/science.pdf&#34;&gt;先行研究&lt;/a&gt;は、各中間層を個別に学習することで、良い初期値を求められることを示した。
具体的には、各中間層について、前の層の入力から次の層の出力を推定できるよう個別に学習させる。
一方で、何が良い初期値をなすのかは知られていなかった。
表題の論文は、その条件は入力に含まれるノイズに対して頑強であると仮説をおき、ノイズを除去できるように目的関数を設定することで、次元圧縮の性能が上がることを示し、仮説の正しさを確かめた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Statistical Errors in Software Engineering Experiments: A Preliminary Literature Review</title>
      <link>https://nryotaro.dev/posts/statistical_errors_in_software_engineering_experiments/</link>
      <pubDate>Fri, 14 Aug 2020 18:39:35 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/statistical_errors_in_software_engineering_experiments/</guid>
      <description>&lt;p&gt;ソフトウェア工学の実験において、統計をもちいた手法がどれだけ誤用されているかを調査した。
薬学や心理学の実験では、統計による手法が時に誤って使われていることが知られている。
一方で、ソフトウェア工学では、どの程度誤用がみられるのかは分かっていない。
著者らは、2006から2015年のソフトウェア工学のトップ会議ICSEで発表された論文770件から、実験や評価に統計的手法をもちいたものを選び、10の観点からなる判断基準で、手法の妥当性を評価した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Semi Supervised Learning with Ladder Networks</title>
      <link>https://nryotaro.dev/posts/semi-supervised_learning_with_ladder_networks/</link>
      <pubDate>Fri, 14 Aug 2020 17:01:02 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/semi-supervised_learning_with_ladder_networks/</guid>
      <description>&lt;p&gt;Ladder Networkを半教師あり学習に応用する。
Ladder Networkは、2015年に、著者の一人Valpolaによって教師なし学習のためのネットワークとして発表されている&lt;a href=&#34;https://arxiv.org/abs/1411.7783&#34;&gt;*&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ An Empirical Study On Program Failures On Deep Learning Jobs</title>
      <link>https://nryotaro.dev/posts/an_empirical_study_on_program_failures_on_deep_learning_jobs/</link>
      <pubDate>Fri, 07 Aug 2020 18:50:12 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/an_empirical_study_on_program_failures_on_deep_learning_jobs/</guid>
      <description>&lt;p&gt;Microsoftの社内では深層学習のプラットフォームPhillyが運用されており、そこで起きた4960件のジョブの失敗原因を調査した。
調査では、失敗の原因を20のカテゴリに分類し、カテゴリごとに失敗の件数を集計した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Improving Language Understanding by Generative Pre-Training</title>
      <link>https://nryotaro.dev/posts/improving_language_understanding_by_generative_pre_training/</link>
      <pubDate>Fri, 07 Aug 2020 16:45:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/improving_language_understanding_by_generative_pre_training/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;GPTの略称で知られる教師なしの事前学習である。
評価実験では、全結合層を1層追加したファインチューニングで、12の自然言語処理タスクのうち9つについて、当時のSoTAを上まわる性能を発揮した。
ネットワークには&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Transformer&lt;/a&gt;のデコーダー部分が使われている。
ファインチューニング時には、事前学習の効果を生かすために、特徴の入力をモデルのアーキテクチャに合わせ、アーキテクチャの変更を最小限におさえる。
推論タスクであれば前提や仮定、テキストの類似性判定であれば比較する2つのテキストといった異種の特徴をデリミタで連結した系列を作り、モデルに入力する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ A Tale from the Trenches: Cognitive Biases and Software Development</title>
      <link>https://nryotaro.dev/posts/a_tale_from_the_trenches/</link>
      <pubDate>Fri, 31 Jul 2020 18:55:54 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_tale_from_the_trenches/</guid>
      <description>&lt;p&gt;エンジニア10人の普段の開発状況から、認知バイアスが開発者にあたえる影響やバイアスの頻度、対策方法について調査した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Factorization Machines</title>
      <link>https://nryotaro.dev/posts/factorization_machines/</link>
      <pubDate>Fri, 31 Jul 2020 16:50:25 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/factorization_machines/</guid>
      <description>&lt;p&gt;Factorization Machineは、Matrix factorizationのようなFactorization modelとSVMの両方の利点をもつ。
Matrix modelには疎な特徴を入力することができるが、予測のモデルに使うには汎用性に欠ける。
一方、SVMは、汎用的であるが、推薦システムで使われるような疎な特徴を扱うことができない。
Factorizatiom Machineは、両者の利点をそなえており、疎な任意の実数を要素にもつ特徴ベクトルを扱うことができる。
また、予測の計算量が線形であり、必要なパラメタの数も線形であるため、SVMのサポートベクタのように訓練データをモデルに持たせる必要がない。
そのために、大量の訓練データを使う学習も可能となる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploration of Technical Debt in Start-ups</title>
      <link>https://nryotaro.dev/posts/exploration_of_technical_debt_in_start_ups/</link>
      <pubDate>Fri, 24 Jul 2020 15:26:42 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/exploration_of_technical_debt_in_start_ups/</guid>
      <description>&lt;p&gt;スタートアップ86社を調査し、スタートアップにおける技術的負債を招く要因(precedents)、負債を抱える側面(dimentions)、その影響(outcomes)について調査した。
チームの人数の多さと熟練度の低さが負債の要因を誘発し、負債はテストの不足によくみられた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Deep Learning Recommendation Model for Personalization and Recoomendation Systems</title>
      <link>https://nryotaro.dev/posts/deep_learning_recommendation_model_for_personalization_and_recommendation_systems/</link>
      <pubDate>Fri, 24 Jul 2020 13:53:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/deep_learning_recommendation_model_for_personalization_and_recommendation_systems/</guid>
      <description>&lt;p&gt;協調フィルタリングのような推薦システムのためのネットワークアーキテクチャを提案した。
特徴の疎・密にかかわらず入力として与えることができる。
論文の例題では、個人の選好を示すアイテムとユーザからなる疎な行列を受け取り、ユーザがアイテムをクリックする確率を推定するタスクが使われている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Explaining Pair Programming Session Dynamics from Knowledge Gaps</title>
      <link>https://nryotaro.dev/posts/explaining_pair_programming_session_dynamics_from_knowledge_gaps/</link>
      <pubDate>Fri, 17 Jul 2020 19:20:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/explaining_pair_programming_session_dynamics_from_knowledge_gaps/</guid>
      <description>&lt;p&gt;ペアプログラミングによる知識移転の効果を知るために、9社の社員からなる26組のペアプログラミングを、グラウンデッド・セオリーで調査した。
従来は、熟練度合いで開発者を分けて、ペアプログラミングを分析・評価することが多い。
今回の調査では、システムの要件・仕様のようなシステム固有の知識と、言語、デザインパターン、開発ツールなどの開発全般の知識の2軸で、開発者と知識を分ける重要性を定性的に示した。
一方がシステム固有の知識に欠け、他方が開発全般の知識に欠けるときに、最も知識の伝達が活発だった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Bag of Tricks for Efficient Text Classification</title>
      <link>https://nryotaro.dev/posts/bag_of_trics_for_efficient_text_classification/</link>
      <pubDate>Fri, 17 Jul 2020 17:09:26 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bag_of_trics_for_efficient_text_classification/</guid>
      <description>&lt;p&gt;2016年における分類のSOTAと互角の精度でありながら、格段に高速に学習、推定可能なモデルを&lt;code&gt;fastText&lt;/code&gt;で構築できることを示した。
評価実験には、&lt;a href=&#34;http://projects.dfki.uni-kl.de/yfcc100m/&#34;&gt;YFCC100M&lt;/a&gt;のキャプションとタイトルからタグを予測するタスク、8つのデータセットによる感情分析が採用された。
タグの予測では、312116個のユニークなタグをつかい、大きなクラス数でもモデルがうまくはたらくことが確かめられた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Recognizing Developers&#39; Emotions while Programming</title>
      <link>https://nryotaro.dev/posts/recognizing_developers_emotions_while_programming/</link>
      <pubDate>Sat, 11 Jul 2020 12:45:00 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/recognizing_developers_emotions_while_programming/</guid>
      <description>&lt;p&gt;23人の被検者に30分間のプログラミングをさせ、その間5分ごとに進捗と感情を自己申告してもらい、その結果から感情と進捗の関係を調査した。
作業後にインタービューを実施し、感情が変化する要因と良くない感情への対処法をヒアリングした。
感情を測定するため最低限必要な非侵襲的器具を調べるために、被検者には、脳波、皮膚電位、脈波、心拍数を測定する器具を装着して開発してもらった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Pytorch: An Imperative Style, High-Performance Deep Learning Library</title>
      <link>https://nryotaro.dev/posts/pytorch_an_imperative_style_high_performance_deep_learning_library/</link>
      <pubDate>Sat, 11 Jul 2020 01:47:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/pytorch_an_imperative_style_high_performance_deep_learning_library/</guid>
      <description>&lt;p&gt;Pytorchの使い勝手と実行速度について解説した論文である。
ここでの使い勝手は、命令型かつPythonらしいコードでPytorchのAPIを呼びだせることを意味する。
Pytorchは、4つの設計原則として、PythonらしいAPI、機械学習の複雑な処理をPytorch内に隠蔽する、使い勝手のために過度にパフォーマンスを犠牲にしない、完璧な解決策よりも実装の単純さを重視する、をかかげる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?</title>
      <link>https://nryotaro.dev/posts/here_we_go_again/</link>
      <pubDate>Sat, 04 Jul 2020 13:55:02 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/here_we_go_again/</guid>
      <description>&lt;p&gt;既知のプログラミング言語の知識は新しい言語を覚える役に立つ。
一方で、新しい言語の学習の妨げにもなることをStack Overflowの質問とプログラマへのインタビューで明らかにした。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Automatic differentiation in Pytorch</title>
      <link>https://nryotaro.dev/posts/automatic_differentiation_in_pytorch/</link>
      <pubDate>Sat, 04 Jul 2020 12:06:25 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/automatic_differentiation_in_pytorch/</guid>
      <description>&lt;p&gt;Pytorchの自動微分を解説したプレプリントのショートペーパである。
Pytorchの自動微分特徴として、in-placeアルゴリズム、微分の導出に不要な計算を省く仕組みのあるテープ、C++による実装をあげている。
&lt;a href=&#34;https://pytorch.org/blog/pytorch-0_4_0-migration-guide/&#34;&gt;0.4.0での仕様変更&lt;/a&gt;によって&lt;code&gt;Tensors&lt;/code&gt;と&lt;code&gt;Variables&lt;/code&gt;がマージされたことや&lt;code&gt;volatile&lt;/code&gt;が非推奨になったことをふまえていない。
読むときは、以上の点をはじめとする現在のAPIとの差異に注意する必要がある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ ROUGE: A Package for Automatic Evaluation of Summaries</title>
      <link>https://nryotaro.dev/posts/rouge/</link>
      <pubDate>Sat, 27 Jun 2020 12:55:45 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/rouge/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;生成された要約を機械的に評価するための指標, Recall-Oriented Understudy for Gisting Evaluation(ROUGE)を提案した論文である。
人が作成した複数の要約文書との再現率で要約文書を評価する。
ROUGEは、ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, ROUGE-SUの5つの指標の総称である。
同じ要約へのROUGEスコアと人の評価の相関によって、ROUGEの指標としての有用性を評価した。
その結果、ROUGE-2, ROUGE-L, ROUGE-W, ROUGE-Sは、文書の要約の評価に向き、ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, ROUGE-SU9はヘッドラインほどの短い要約文の評価に向いていることがわかった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Do Developers Discover New Tools On The Toilet?</title>
      <link>https://nryotaro.dev/posts/do_developers_discover_new_tools_on_the_toilet/</link>
      <pubDate>Sat, 20 Jun 2020 22:33:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/do_developers_discover_new_tools_on_the_toilet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://mike-bland.com/2011/10/25/testing-on-the-toilet.html&#34;&gt;Testing on the Toilet&lt;/a&gt;の効果を&lt;a href=&#34;https://research.google/pubs/pub41854/&#34;&gt;CausalImpact&lt;/a&gt;で示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ A Neural Attention Model for Sentence Summarization</title>
      <link>https://nryotaro.dev/posts/a_neural_attention_model_for_sentence_summarization/</link>
      <pubDate>Sat, 20 Jun 2020 16:23:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_neural_attention_model_for_sentence_summarization/</guid>
      <description>&lt;p&gt;注意機構による深層学習で文を要約する手法である。
もとの文にない単語を含む要約文を生成できるが、生成前に文の長さを決めておかなければならない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ BLEU: a Method for Automatic Evaluation of Machine Translation</title>
      <link>https://nryotaro.dev/posts/bleu/</link>
      <pubDate>Sat, 13 Jun 2020 13:48:07 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bleu/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;自動翻訳を定量的に評価するための指標BLEUを提案した論文である。
指標は、専門家の翻訳に翻訳に高い評価をあたえるよう設計されている。
BLEUは、ひとつの候補訳に対する1つ以上の参照訳をあたえ、0から1の値をとるスコアを出力する。
スコアは高いほどよい。
BLEUは、参照訳にある単語を過剰に含むことや文の短さにペナルティをあたえ、適合率で候補訳を評価する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Sequence to Sequence Learning with Nueral Networks</title>
      <link>https://nryotaro.dev/posts/sequence_to_sequence_learning_with_neural_networks/</link>
      <pubDate>Sat, 06 Jun 2020 00:18:16 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/sequence_to_sequence_learning_with_neural_networks/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Sequence to Sequenceの論文。
入出力が系列データを学習する場合、入力と出力の長さが等しかったり対応関係にある箇所が系列の方向に単調でなければならなかったりする。
これらの制約に対処するために、Sequence to Sequenceでは、入力全体を固定長のベクトルに一度変換し、そのベクトルをもとに出力を予測する。
2種類のLSTMをもち、入力を与えるLSTMの最終層の隠れ状態で、固定長ベクトルをつくる。
固定長のベクトルは、単調の制約を緩めるはたらきをする。
このベクトルは、もう一方のLSTMにあたえられ、その主力が最終的な出力になる。
実験では、入力系列を反転してあたえると、入力と出力の対応関係にある箇所の距離が近づき、予測性能が上がることが確認された。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Google&#39;s Neural Machine Transltation System: Bridging the Gap between Human and Machine Translation</title>
      <link>https://nryotaro.dev/posts/google_neural_machine_translation_system/</link>
      <pubDate>Sat, 30 May 2020 16:09:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/google_neural_machine_translation_system/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ニューラルネットワークをもちいた機械翻訳システムの論文である。
解決したい問題として、学習と推論時の処理時間の長さ、低頻出の単語を翻訳する難しさ、入力文の一部が翻訳されないことをあげ、注意機構でつながれたEncoderとDecoderからなるアーキテクチャを提案した。
学習時間を短縮するために、Decoderの最初の層とEncodeerの出力層から注意をつくる注意機構を採用し、Decoderを並列に学習できるようにしている。
また、量子化によって推論時間を短縮をしている。
低頻出の単語でも翻訳できるようにwordpieceでエンコードされた入力をうけとる。
入力文の一部が翻訳されない問題に対しては、短い出力文に罰則を課すビームサーチで出力文の候補を探索する仕組みが導入されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Zero-Shot Learning with Semantic Output Codes</title>
      <link>https://nryotaro.dev/posts/zero_shot_learning_with_semantic_output_codes/</link>
      <pubDate>Sat, 23 May 2020 14:04:24 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/zero_shot_learning_with_semantic_output_codes/</guid>
      <description>&lt;p&gt;学習データにないラベルを推定する問題に対してzero-shot leanringと名づけ、ラベルを推定できる確率と条件を形式化した論文である。
形式化するモデルは、複数の二値分類器と1つの最近傍探索器からなる。
最近傍探索は、2値分類器の出力を要素とするベクトルをうけとり、最近傍のラベルに対応するベクトルを探す。
PACフレームワークにもとづく必要な学習データの件数を示し、そのデータで訓練されたモデルが学習データにないラベルを推定できる確率を示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</title>
      <link>https://nryotaro.dev/posts/a_structured_self_attentivesentence_embedding/</link>
      <pubDate>Sat, 16 May 2020 14:05:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_structured_self_attentivesentence_embedding/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;自己注意機構をもちいて、可変長の文を埋め込み行列に変換するアーキテクチャを発表した論文である。
埋め込み行列の各行は、それぞれ文中の異なる箇所の意味を反映する。
アーキテクチャは2つの構成からなり、入力から出力にむかい双方向LSTMを、次に自己注意機構をもつ。
自己注意機構を導入した背景は、回帰結合型のネットワークでは、全ての時刻わたって入力の意味を保持することは難しく、また不要であるという著者らの仮説である。
3つの実験により、文の分散表現を獲得する先行研究と比較し、自己注意機構の効果が確認された。
注意機構は複数のベクトルのどれを重視するかを学習できるため、埋め込まれた文の箇所を可視化できることも示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Poincaré Embeddings for Learning Hierarchical Representations</title>
      <link>https://nryotaro.dev/posts/poincare_embeddings/</link>
      <pubDate>Sat, 09 May 2020 14:42:32 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/poincare_embeddings/</guid>
      <description>概要 単語のように上位下位関係のある記号を、ポアンカレ球体模型という双曲空間に埋め込む手法を発表した論文である。 ユークリッド空間よりも、記号間の類似度や上位下位関係が保たれていることを実験的に示した。 記号を木のノードとして配置し関係を表現するとき、ノード数は深さ\(l\)対して指数関数的に増加する。 双曲幾何学では、円板の面積や周は半径\(r\)に対して指数関数的に増大するため、木を2次元でモデル化できる。 たとえば、深さ\(l\)以下のノードを半径\(r \varpropto l \)の空間に配置することができる。 一方、2次元のユークリッド空間の場合、半径\(r\)に対する円周は線形、円の面積は2次関数的であるため、モデル化が難しい。 実験では、次元数が少ないほど、ポアンカレ球体模型とユークリッド空間の間で、上下関係や類似度の表現力に差があった。
損失関数 埋め込みたい上下関係\(\mathcal{D}=\{(u, v)\}\)を記号の数を\(n\)として入力すると、アルゴリズムは、埋め込みベクトルの集合\({\rm \Theta}=\{\boldsymbol{\theta}_i\}^n_{i=1}\)を出力する。 ただし、\(\boldsymbol{\theta}\in \mathcal{B}^d\), \(\mathcal{B}^d=\{\boldsymbol{x}\in \mathbb{R}^d\mid ||\boldsymbol{x}||&amp;lt;1\}\)とする。 学習では、次の損失関数\(\mathcal{L}(\Theta)\)をもちいる。
$$ \mathcal{L}(\Theta)=\sum_{(u, v)\in \mathcal{D}}\log\frac{e^{-d(\boldsymbol{u}, \boldsymbol{v})}}{\sum_{\boldsymbol{v}&#39;\in \mathcal{N}(u)}e^{-d(\boldsymbol{u}, \boldsymbol{v}&#39;)}} $$ \(\mathcal{N}(u)=\{v&#39;\mid (u, v&#39;)\notin \mathcal{D}\} \cup \{v\}\)は\(v\)を含んだ\(u\)に対する負例である。 実験では、正例に対して10の負例をサンプリングしていた。 \(d\)は、\(\boldsymbol{u}, \boldsymbol{v}\in \mathcal{B}^d\)の距離であり、次の式であたえらえる。
$$ d(\boldsymbol{u}, \boldsymbol{v}) = \mathrm{arccosh}\left(1+2\frac{||\boldsymbol{u}-\boldsymbol{v}||^2}{(1-||\boldsymbol{u}||^2)(1-||\boldsymbol{v}||^2)}\right) $$
最適化 RSGDやRSVRGで損失関数の値を最小化する埋め込みベクトルを探す。 ここでは、RSGDについて説明する。 RSGDでは、次のパラメタの更新式をとる。
$$ \boldsymbol{\theta}_{t+1} = \mathfrak{R}_{\theta_t}(-\eta_t\nabla_R\mathcal{L}(\boldsymbol{\theta}_t)) $$ \(\mathfrak{R}_{\theta_t}\)はレトラクションで、ここでは\(\mathfrak{R}_\theta(\boldsymbol{v})=\boldsymbol{\theta}+\boldsymbol{v}\)をもちいる。 \(\eta_t\)は時刻\(t\)の学習率をさす。 \(\nabla_R\)はリーマン多様体上の勾配であり、ユークリッド空間上の勾配\(\nabla_E\)とは
$$ \nabla_R = \frac{(1-||\boldsymbol{\theta_t}||^2)^2}{4}\nabla_E $$ の関係がある。 以上より、更新式は
$$ \mathrm{proj}(\boldsymbol{\theta})= \begin{cases} \boldsymbol{\theta}/||\boldsymbol{\theta}|| - \epsilon &amp;amp;\mathrm{if}\ ||\boldsymbol{\theta}||\ge 1 \\ \boldsymbol{\theta}&amp;amp;\mathrm{otherwise.</description>
    </item>
    
    <item>
      <title>論文 メモ Learning Joint Multilingual Sentence Representations with Neural Machine Translation</title>
      <link>https://nryotaro.dev/posts/learning_joint_multilingual_sentence_representations_with_neural_machine_translation/</link>
      <pubDate>Wed, 29 Apr 2020 14:40:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_joint_multilingual_sentence_representations_with_neural_machine_translation/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;多言語の文をあつかう分散表現モデルを発表した論文である。
異なる言語の文であっても、意味が同じであれば、同様の分散表現に変換される。
モデルのアーキテクチャにはseq2seqを、入力と出力には対訳コーパスをつかう。
ミニバッチごとに、入力または出力の言語をいれかえ、言語に依存しない文の意味の分散表現への変換方法を学習する。
本論文の成果は多言語に対応する分散表現のモデルのライブラリ&lt;a href=&#34;https://github.com/facebookresearch/LASER&#34;&gt;LASER&lt;/a&gt;に応用されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</title>
      <link>https://nryotaro.dev/posts/albert/</link>
      <pubDate>Sat, 25 Apr 2020 13:12:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/albert/</guid>
      <description>&lt;p&gt;BERTのパラメタ数を削減し、学習時間の短縮と正則化による予測性能の向上を両立したモデルALBERTを提案し、GLUE, RACE, SQuADでSoTAを実現した。
BERT-largeと比べると、ALBERT-largeのパラメタ数は約5.3%の18Mであり、学習時間は1.7倍速い。
パラメタを削減するために、単語のOne-hotベクトルをあたえられる単語埋め込み行列の次元を減らし、隠れ層の順伝播ネットワークや注意機構のパラメタを層の間で共有した。
また、Next Sentence Prediction(NSP)による学習を、与えられた2文の前後関係を判定する学習Sentence Order Prediction(SOP)におきかえ、主タスクの予測性能を向上をはかった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Random Walks in recommender Systems: Exact Computation and Simulations</title>
      <link>https://nryotaro.dev/posts/random_walks_in_recommender_systems/</link>
      <pubDate>Sat, 18 Apr 2020 01:25:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/random_walks_in_recommender_systems/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/4072747&#34;&gt;F. Foussら&lt;/a&gt;や&lt;a href=&#34;https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-444.pdf&#34;&gt;M. Goriら&lt;/a&gt;のランダムウォークによる推薦システムの先行研究を、質や計算量について比較した論文である。
比較対象には、著者らの用意したも含まれる。
実験には、MovieLensのデータセットが使われた。
F. Foussらの実験で使われた評価指標や上位kの推薦結果のヒット数で評価したところ、著者らの用意した単純な手法\(P^s\)やその拡張\(P_\alpha^s\)が質と計算量の両方で最も優れた結果を残した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Domain Adversarial Training of Neural Networks</title>
      <link>https://nryotaro.dev/posts/domain_adversarial_training_of_neural_networks/</link>
      <pubDate>Sat, 11 Apr 2020 15:07:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/domain_adversarial_training_of_neural_networks/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ニューラルネットワークをもちいたドメイン適用の論文である。
ソースドメインのラベルつきデータと目標ドメインのラベルのないデータでモデルを訓練し、目標ドメインに対する分類性能を引きあげる。
目的関数は、ソースドメインの分類器の目的関数とデータのドメインを判定する識別器の目的関数からなる。
後者は、前者の正則化項としてはたらく。
これにより、ドメイン間に共通する特徴からソースドメインのデータのラベルを高い性能で予測できるようになる。
目標関数から、ドメイン間のデータの分布が近いほど、目標ドメインのデータでも高い分類性能を発揮する。
先行研究との違いは、できるだけ共通するする特徴で分類するという着想を、通常の分類と同じく、確率的勾配降下法で実現したところにある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Character-Aware Neural Language Models</title>
      <link>https://nryotaro.dev/posts/character_aware_neural_language_models/</link>
      <pubDate>Sat, 04 Apr 2020 16:46:56 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/character_aware_neural_language_models/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;文字単位の入力から次に出現する単語を予測するニューラル言語モデルの論文である。
アーキテクチャは入力から近い順にCNN, highway network, LSTMからなる。
実験データにPenn Treebankを、評価指標にPerplexityを採用してモデルを評価したところ、
論文が発表された2016年時点での&lt;a href=&#34;https://arxiv.org/abs/1409.2329&#34;&gt;SOTA&lt;/a&gt;の60%程度のパラメタしかないモデルでありながら、これに匹敵する性能を発揮した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Deep contextualized word representations</title>
      <link>https://nryotaro.dev/posts/deep_contextualized_word_representations/</link>
      <pubDate>Tue, 24 Mar 2020 01:59:00 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/deep_contextualized_word_representations/</guid>
      <description>&lt;p&gt;文脈をふまえた単語の分散表現を生成する手法を提案し、教師あり学習に応用することで評価した論文である。
文字単位の学習済み双方向LSTM言語モデルへの入力と各層の出力から分散表現をつくる。
言語モデルの入力やどの層をどれだけ重視するかは、教師あり学習のときに更新するパラメタのひとつになる。
実験では、構文にかかわるタスクであれば入力層に近い層が、意味にかかわるものであれば出力層に近い層が、重視された。
モデルは、Embeddings from Language Modelsにちなみ、ELMoと名付けられた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ The Seven Sins: Security Smells in Infrastructure as Code Scripts</title>
      <link>https://nryotaro.dev/posts/the_seven_sins/</link>
      <pubDate>Fri, 20 Mar 2020 15:14:44 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_seven_sins/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;OSSの調査にもとづき、Infrastrucure as Code(IaC)スクリプトに潜む主要なセキュリティ上の不吉な匂い(Security Smells)を7つ列挙し、これらを検出するツールを実装した論文である。
論文のねらいは、開発者がIaCスクリプトに不吉な匂いを混ぜないようにすることにある。
著者らは、本論文で、ICSE2019のDistinguished Paper Awardを受賞した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Semi-supervised Sequence Learning(2015)</title>
      <link>https://nryotaro.dev/posts/semi_supervised_sequence_learning/</link>
      <pubDate>Sat, 14 Mar 2020 23:46:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/semi_supervised_sequence_learning/</guid>
      <description>&lt;p&gt;系列データの教師あり学習において、ラベルのないデータを学習した言語モデルやオートエンコーダーの重みでLSTMを初期化することの有用性を実験的に示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>In Search of an Understandable Consensus Algorithm(2014)</title>
      <link>https://nryotaro.dev/posts/raft/</link>
      <pubDate>Mon, 09 Mar 2020 02:17:22 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/raft/</guid>
      <description>&lt;p&gt;コンセンサスアルゴリズムRaftを提案した論文である。
Raftは、Multi Paxosと同様の実行結果をもたらす。
実行するコマンドのログをサーバ間で交換することで、状態を同期し、サーバの一部が落ちてもシステムを継続することができる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</title>
      <link>https://nryotaro.dev/posts/sentence_piece/</link>
      <pubDate>Sat, 29 Feb 2020 21:15:17 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/sentence_piece/</guid>
      <description>&lt;p&gt;SentencePieceは、深層学習向けのトークナイザ・脱トークナイザである。
特定の言語を意識した処理がないため、あらゆるテキストに利用できる。
本論文では、C++やPythonによる&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;実装&lt;/a&gt;と翻訳への適用実験について書かれている。
アルゴリズムの解説は、&lt;a href=&#34;https://www.aclweb.org/anthology/P16-1162.pdf&#34;&gt;Sennrich et al.&lt;/a&gt;や&lt;a href=&#34;https://arxiv.org/pdf/1804.10959.pdf&#34;&gt;Kudo.&lt;/a&gt;にゆずられている。
これらの論文について2019年7月13日の&lt;a href=&#34;../neural_machine_translation_of_rate_words/&#34;&gt;記事&lt;/a&gt;と2019年7月17日の&lt;a href=&#34;./subword_regularization/&#34;&gt;記事&lt;/a&gt;で解説している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Latent Dirichlet Allocation(2003)</title>
      <link>https://nryotaro.dev/posts/latent_dirichlet_allocation/</link>
      <pubDate>Sun, 23 Feb 2020 21:54:59 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/latent_dirichlet_allocation/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;トピックモデルの潜在的ディリクレ配分法(LDA)の原論文である。
LDAは、テキストコーパスのような離散データの確率的生成モデルである。
意味のあるデータのまとまりに対する端的な説明を与える情報を見つけることを目的としている。
3つの階層からなる階層ベイズモデルである。
データの要素は、各トピックを表すモデルの混合モデルから生成される。
トピックもまた混合モデルから確率的に生成される。
推論にはベイズ変分法を、パラメタの推定にはEMアルゴリズムをもちいる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unsupervised Pretraining for Sequence to Sequence Learning(2017)</title>
      <link>https://nryotaro.dev/posts/unsupervised_pretraining_for_sequence_to_sequence_learning/</link>
      <pubDate>Sun, 16 Feb 2020 14:21:55 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/unsupervised_pretraining_for_sequence_to_sequence_learning/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;事前学習とファインチューニングにより&lt;a href=&#34;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;seq2seq&lt;/a&gt;の汎化性能を改善する手法を提案した論文である。
encoderの重みを学習済み言語モデルの重みで初期化する。
decoderについても、encoderと別の言語モデルを用意し、その重みで初期化する。
ただし、工夫のないファインチューニングをすると&lt;a href=&#34;https://arxiv.org/pdf/1312.6211.pdf&#34;&gt;破滅的忘却&lt;/a&gt;が生じてしまう。
そこで、ファインチューニングでは言語モデルとseq2seqの目的関数の両方を学習につかうことで、過学習をさけ、汎化性能を確保する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Playing Atari with Deep Reinforcement Learning(2013)</title>
      <link>https://nryotaro.dev/posts/playing_atari_with_deep_reinforcement_learning/</link>
      <pubDate>Sun, 09 Feb 2020 17:05:30 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/playing_atari_with_deep_reinforcement_learning/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;深層強化学習をAtari2600の7つのゲームに応用し、うち6つについて先行手法の性能を超えたDeep Q-Networks(DQN)を提案した論文である。
ピクセルデータを直接入力として与え、深層学習で方策を学習する手法としては初めて提案された。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>context2vec: Learning Generic Context Embedding with Bidirectional LSTM (2016)</title>
      <link>https://nryotaro.dev/posts/context2vec/</link>
      <pubDate>Sun, 02 Feb 2020 19:19:46 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/context2vec/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;文書の文脈の分散表現を獲得するニューラルネットワークのアーキテクチャ&lt;em&gt;context2vec&lt;/em&gt;を提案、評価した論文である。
アーキテクチャの基本構造は&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;CBOW&lt;/a&gt;と同様で、周辺の単語から中心の単語を当てられるようにコーパスをもとにモデルを訓練する。
CBOWとの違いは、文脈の算出方法にある。
CBOWは、ウィンドウ内のベクトルの平均値で文脈の分散表現を求める。
一方、&lt;em&gt;context2vec&lt;/em&gt;では、双方向LSTMの出力をもとに算出する。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
    <item>
      <title>NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015)</title>
      <link>https://nryotaro.dev/posts/neural_machine_translation_by_jointly_learning_to_align_and_translate/</link>
      <pubDate>Sat, 01 Feb 2020 17:21:41 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/neural_machine_translation_by_jointly_learning_to_align_and_translate/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Decoderに注意機構を採用したencoder-decoderモデルを提案したICLR2015の論文である。
論文の発表当時、encoder-decoderモデルによる翻訳の多くは、encoderが入力文を固定長ベクトルに変換し、固定長ベクトルから翻訳された文を出力していた。
著者らは、固定長ベクトルへの変換が長い文の翻訳性能を下げていると考え、固定長ベクトルを注意機構におきかえたencoder-decoderモデルを提案した。
モデルは、翻訳に加え、生成する単語と入力文の箇所の関係を学習する。
推定時には、まず、次に生成する単語に関係する入力文の箇所を推定する。
次に、推定された箇所と生成済の単語列をもとに、単語を生成する。
特に長い文書の翻訳において、固定長ベクトルをつかうモデルよりも、提案手法が優れていることを実験的に示した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Why Should I Trust You? Explaining the Predictions of Any Classifier</title>
      <link>https://nryotaro.dev/posts/why_should_i_trust_you/</link>
      <pubDate>Sun, 26 Jan 2020 01:38:57 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/why_should_i_trust_you/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;モデルの予測に説明をあたえる手法、Local Interpretable Model-agnostic Explanations (LIME)を提案する。
モデルが回帰や分類器であれば、アルゴリズムによらずLIMEを適用できる。
説明を与えたい事例近くにある事例を解釈可能なモデルに学習させ、解釈可能なモデルで予測を説明する。
また、個別の予測ではなく、モデル自体をよく説明する事例を集める手法Submodullar Pick (SP)-LIMEを提案する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Active Learning for Ranking through Expected Loss Optimization</title>
      <link>https://nryotaro.dev/posts/active_learning_for_ranking_through_expected_loss_optimization/</link>
      <pubDate>Sun, 19 Jan 2020 18:23:41 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/active_learning_for_ranking_through_expected_loss_optimization/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Yahoo! Labsで開発されたランキングのための能動学習の論文である。
提案手法は、Yahoo!検索エンジンでの&lt;a href=&#34;https://www.kdd.org/kdd2016/papers/files/adf0361-yinA.pdf&#34;&gt;採用実績&lt;/a&gt;がある。
手法は、Expected Loss Optimization(ELO)とよばれ、ベイズ決定則によって識別したときの損失の期待値が最大になるデータを選ぶ。
ELOに用いる損失関数にDCGを採用したExpected DCG Loss Optimization(ELO-DCG)を提案し、実験で評価した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AtCoderに提出したコードをテストするためのDockerイメージ</title>
      <link>https://nryotaro.dev/posts/docker-cmake-clang/</link>
      <pubDate>Tue, 14 Jan 2020 00:29:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/docker-cmake-clang/</guid>
      <description>AtCoderに提出したコードをテストするためのDockerイメージを実装した。 イメージのDockerfileはこちらにある。 AtCoderで提出したコードをgithubで管理していて、これをテストするために作った。</description>
    </item>
    
    <item>
      <title>Unsupervised Models for Named Entity Classification(1999)</title>
      <link>https://nryotaro.dev/posts/unsupervised_models_for_named_entity_classification/</link>
      <pubDate>Mon, 13 Jan 2020 19:54:03 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/unsupervised_models_for_named_entity_classification/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;1999年に発表された教師なしの固有表現抽出の手法である。
発表時期が古いことに注意してほしい。
2つの手法が提案されている。
ひとつは、DL-CoTrainと呼ばれるルールベースの手法であり、教師なしデータに既存のルールを適用、適用結果から導出したルールを既存のルールに追加、をくりかえしてルールを増やす。
もう一方は、AdaBoostを応用したCoBoostとよばれる手法である。
ルールベースの手法のほうがCoBoostよりもよい実験結果であったので、前者のみを説明する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Multilabel Classification with Label Correlations and Missing Labels(2014)</title>
      <link>https://nryotaro.dev/posts/multilabel_classification_with_label_correlations_and_missing_labels/</link>
      <pubDate>Mon, 06 Jan 2020 22:27:05 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/multilabel_classification_with_label_correlations_and_missing_labels/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ラベルの相関関係を学習し推論に利用するマルチラベルの線形モデルを提案した論文である。
相関関係のあるラベル集合を相関関係のないラベル集合に変換し、ラベルごとに分けて学習する手法、Label transformationを応用する。
分類器は、相関関係だけなく、学習データに与えられていないラベルを推定するように拡張できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>大砲ラーメン</title>
      <link>https://nryotaro.dev/gallery/taihoh/</link>
      <pubDate>Sun, 05 Jan 2020 13:25:04 +0900</pubDate>
      
      <guid>https://nryotaro.dev/gallery/taihoh/</guid>
      <description>帰省して食べに行きました。 中高校生から食べに行っていました。 そのころは、ラーメン大が600円くらいだった記憶でしたが、帰省の度に高くなって今は800円くらいしました。 隔世の感があります。</description>
    </item>
    
    <item>
      <title>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data(2013)</title>
      <link>https://nryotaro.dev/posts/learning_deep_structured_semantics_models_for_web_search_using_clickthrough_data/</link>
      <pubDate>Sat, 04 Jan 2020 23:25:20 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_deep_structured_semantics_models_for_web_search_using_clickthrough_data/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;クエリと文書を同じ低次元の空間に射影する深層学習のモデルを提案した論文である。
クエリと文書は、適合度合いが高いほど、近くに配置される。
教師データは、クエリと文書の組からなる教師データである。
実験では、商用検索エンジンから抽出した16510件のクエリと対応するWeサイトのタイトルがつかわれる。
Web文書の大量の語彙をあつかうために、語彙の増加に対して次元数を抑えるbag-of-wordsの手法、word hasingも提案した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Classification in the Presence of Label Noise: a Survey(2013)</title>
      <link>https://nryotaro.dev/posts/classification_in_the_presence_of_label_noise/</link>
      <pubDate>Mon, 30 Dec 2019 16:07:40 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/classification_in_the_presence_of_label_noise/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ノイズのある教師データによるクラス分類の&lt;a href=&#34;https://romisatriawahono.net/lecture/rm/survey/machine%20learning/Frenay%20-%20Classification%20in%20the%20Presence%20of%20Label%20Noise%20-%202014.pdf&#34;&gt;サーベイ論文&lt;/a&gt;である。発表時期は、&lt;a href=&#34;https://ieeexplore.ieee.org/document/6685834&#34;&gt;2013年の12月&lt;/a&gt;である。
主な内容は、ノイズの分類、ノイズが分類に及ぼす影響、ノイズへの対策である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Distributed Representations of Sentences and Documents(2014)</title>
      <link>https://nryotaro.dev/posts/distributed_representations_of_sentences_and_documents/</link>
      <pubDate>Sat, 28 Dec 2019 00:56:36 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/distributed_representations_of_sentences_and_documents/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://radimrehurek.com/gensim/models/doc2vec.html&#34;&gt;Doc2Vec&lt;/a&gt;のアルゴリズムとして採用されたニューラル言語モデルParagraph Vectorを提案した論文である。
bag of wordsは、文書の単語順を記憶せず、また、似た意味の単語ベクトルと無関係なベクトルを単語にわりあてる。
Paragraph Vectorは、文脈中の単語と抽出元のパラグラフから文脈の中心の単語をあてられるように学習することで、可変長文字列から固定長の文書埋め込みベクトルを生成できるようになる。
これにより、単語順と単語の意味を記憶したベクトルの生成を実現する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GloVe: Global Vectors for Word Representation(2014)</title>
      <link>https://nryotaro.dev/posts/glove_vectors_for_word_representation/</link>
      <pubDate>Sat, 21 Dec 2019 23:53:36 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/glove_vectors_for_word_representation/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D14-1162.pdf&#34;&gt;GloVe&lt;/a&gt;は,コーパスに出現する単語の共起回数を学習するニューラル言語モデルである。
既存手法を単語の出現頻度の統計値つかう手法と対数双線形モデルに分類し、両者の長所を備え短所を補う手法として、GloVeを提案する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Lnaguages Understaing(2018)</title>
      <link>https://nryotaro.dev/posts/bert/</link>
      <pubDate>Sat, 14 Dec 2019 17:39:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bert/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;BERTは&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;にあるTransformerをアーキテクチャに導入した分散表現のモデルであり、本稿は、事前学習済みのBERTにファインチューニングを適用しQAタスクや自然言語推論のベンチマークにおいて既存研究を上回る結果を示している。
なお、アーキテクチャに関する説明は少なく、子細に知りたい場合は&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Attention Is All You Need&lt;/a&gt;や&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;The Annotated Transformer&lt;/a&gt;を参照するように案内されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com(2019)</title>
      <link>https://nryotaro.dev/posts/150_successfuly_machine_learning_modes/</link>
      <pubDate>Sat, 14 Dec 2019 13:25:04 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/150_successfuly_machine_learning_modes/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;宿泊予約サービス&lt;a href=&#34;http://booking.com/&#34;&gt;Booking.com&lt;/a&gt;におけるモデルの開発運用でえられた教訓を6つにまとめたKDD2019の論文である。
教訓の主眼を収益におき、6つの教訓を通して、実運用環境における仮説と実験を反復する重要性を強調する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ranking Relevance In Yahoo Search(2016)</title>
      <link>https://nryotaro.dev/posts/ranking_relevance_in_yahoo_search/</link>
      <pubDate>Sat, 07 Dec 2019 15:10:51 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/ranking_relevance_in_yahoo_search/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Yahooの検索エンジンを解説するKDD16の論文である。
論文におけるランキングの課題は、クエリと文書の語彙がことなること、ほとんどのクエリは滅多に入力されないこと、クエリの意味の解釈が難しいことである。
これらの課題に対する手法として、ランキングのモデル、特徴のつくりかた、クエリを文書によせる翻訳モデルを解説する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Dual Embedding Space Model for Document Ranking</title>
      <link>https://nryotaro.dev/posts/a_dual_embedding_space_model_for_document_ranking/</link>
      <pubDate>Sat, 30 Nov 2019 08:18:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_dual_embedding_space_model_for_document_ranking/</guid>
      <description>&lt;p&gt;Dual Embedding Space Model(DESM)は、word2vecによるランキング学習である。
word2vecは、単語ごとに、入力と出力それぞれに近い重みから、2つの分散表現を生成できる。
DESMは、入力側の重みでクエリを、出力側の重みで文書を、それぞれ分散表現に変換する。&lt;/p&gt;
&lt;p&gt;実験では、BM25と比較して評価した。
DESMだけで順位づけをすると偽陽性が高くなるが、DESMとBM25の加重平均をとるとBM25よりも高いNDCG値になった。
アルゴリズムを実装し&lt;a href=&#34;https://github.com/nryotaro/desm&#34;&gt;公開&lt;/a&gt;した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ On Calibration of Modern Neural Networks(2017)</title>
      <link>https://nryotaro.dev/posts/on_calibration_of_modern_neural_networks/</link>
      <pubDate>Sat, 23 Nov 2019 14:37:30 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/on_calibration_of_modern_neural_networks/</guid>
      <description>&lt;p&gt;ネットワークの複雑化、バッチ正則化、重み減衰を使わない、負の対数尤度の過学習が汎化精度を上げるが、予測確率と精度のズレを大きくすることを実験的に示した。
予測確率を補正する6つの手法を19種類のクラス分類のデータセットに適用した結果、
最も補正できたものは、温度つきソフトマックスの出力を予測確率にする場合であった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Trinary-Projection Trees for Approximate Nearest Neighbor Search(2012)</title>
      <link>https://nryotaro.dev/posts/trinary_projection_trees/</link>
      <pubDate>Sat, 16 Nov 2019 13:12:52 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/trinary_projection_trees/</guid>
      <description>&lt;p&gt;Trinary-Projection Trees(TP trees)は、kd木のように、ユークリッド空間の分割を二分木で表現できるデータ構造である。
超平面は1または-1の重みのついた少数の座標軸で定義される。
これにより、探索時の分岐にかかる計算が、加算と減算だけからなる\(O(1)\)となる。
また、射影されたデータの分散の大きい超平面を探し、同じ分割にある点同士の距離を小さくすることで、精度を向上させている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>論文メモ Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers(2018)</title>
      <link>https://nryotaro.dev/posts/get_another_label/</link>
      <pubDate>Sat, 09 Nov 2019 21:46:12 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/get_another_label/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ある確率でデータに誤ったラベルをふるlabelerでデータにラベルをふるときに、
既にラベルのあるデータに重ねてラベルをふるべきか調査した。
12種類のラベルつきデータセットを使い、
正解ラベルを誤ったラベルに置換する割合や同一のデータのもつラベルの数を変化させ、モデルの精度の違いを観察した。
加えて、ラベルをふるべきデータを推定する手法も提案している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ ActiveClean: Interactive Data Cleaning For Statistical Modeling(2016)</title>
      <link>https://nryotaro.dev/posts/active_clean/</link>
      <pubDate>Sat, 09 Nov 2019 15:37:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/active_clean/</guid>
      <description>&lt;p&gt;ActiveCleanは、教師データの誤りを修正し、モデルの精度を改善する手法である。
優先して修正すべきデータを推定し、データが修正されたら修正されたデータでモデルを学習する。
この修正と学習を条件を満たすまでくりかえす。
反復的な学習で大域的最適解をえられるモデルであれば、最適解への収束が保証される。
データの修正件数が等しい場合に、先行研究と比べて最大2.5倍の精度改善を達成した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 WebTables: Exploring the Power of Tables on the Web(2008)</title>
      <link>https://nryotaro.dev/posts/web_tables/</link>
      <pubDate>Thu, 31 Oct 2019 21:09:38 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/web_tables/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Web上の表から抽出した大量の関係モデルを対象にした検索を提案・評価した。
検索の他にも、一部の属性を入力とするスキーマの補完、入力した属性ないしスキーマに類似のものを推定するアルゴリズムの議論もある。
ここのスキーマは属性のリストである。論文の著者らは研究時にGoogleに在籍しており、論文で使われたコーパスはグーグルの汎用ウェブクローラで集めた141億のHTMLの表から抽出した高精度な154百万の関係モデルである。
コーパスに使うものはHTML形式の表から抽出した関係モデルのみである。
手法の新規性は、1億以上もの大量のテーブルを対象にしていることにある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ A Survey on Data Collection for Machine Learning(2018)</title>
      <link>https://nryotaro.dev/posts/a_survey_on_data_collection_for_machine_learning/</link>
      <pubDate>Sat, 26 Oct 2019 14:27:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_survey_on_data_collection_for_machine_learning/</guid>
      <description>&lt;p&gt;機械学習に使う教師データに関するサーベイ論文であり、機械学習や自然言語処理などのデータの応用分野だけでなく、データの管理にまつわる分野の調査も含まれているところに特徴がある。
データの管理に着目している理由は、深層学習の発展によって必要な教師データが増えたことで、データの管理の課題が顕在化してきたからである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses(2012)</title>
      <link>https://nryotaro.dev/posts/structure_estimation_for_dscrete_graphical_models/</link>
      <pubDate>Sat, 19 Oct 2019 16:21:31 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/structure_estimation_for_dscrete_graphical_models/</guid>
      <description>&lt;p&gt;表題の論文は、マルコフ確率場をなす無向グラフとグラフの構造を反映した逆共分散行列の間の対応関係を証明し、
観測した確率変数の値からグラフの構造を復元する実験を通じて、対応関係を確認した。
この手法は&lt;a href=&#34;https://arxiv.org/pdf/1810.02840.pdf&#34;&gt;Snorkel&lt;/a&gt;というWeak supervisionの手法において、
正解データのない環境で、ノイズつきの教師データを生成する異なるソース間の相関関係を推定するために応用された。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Feature Selection for Text Categorization on Imbalanced Data(2004)</title>
      <link>https://nryotaro.dev/posts/feature_selection_for_text_categorization_on_imbalance_data/</link>
      <pubDate>Sat, 12 Oct 2019 15:59:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/feature_selection_for_text_categorization_on_imbalance_data/</guid>
      <description>&lt;p&gt;分類すべきデータには、正例と負例どちらか一方にのみ顕著にみられる特徴がある。
特徴選択をする場合、正例だけでなく負例の選択された顕著な特徴の割合も性能に影響する。
不均衡データの文書分類で、選択する割合を調整するほうが、予測性能を向上できたことを実験的に示した。
情報利得やオッズ比など単変量統計にもとづく特徴選択の場合、統計量の値によって暗黙的に決められた割合と異なる割合の場合の方が予測性能が高かった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale(2019)</title>
      <link>https://nryotaro.dev/posts/snorkel_drybell_case_study/</link>
      <pubDate>Sat, 05 Oct 2019 00:45:00 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/snorkel_drybell_case_study/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Weak Supervisionは、人手によるルールベースよりも効率良くノイズ交じりの教師データを生成する手法である。
表題の論文は、Weak Supervisionの手法である&lt;a href=&#34;https://www.snorkel.org&#34;&gt;Snorkel&lt;/a&gt;をGoogleで適用したケーススタディである。
Snorkelは、サンプルから推定したラベルを返す関数（ラベリング関数）を複数用意し、それをラベルなしデータに適用する。
その結果から関数の精度を予測し、個々の関数よりも高い精度でデータにラベルを与える。この関数をラベリング関数という。
Snorkel DryBellは、Snorkelのアルゴリズムをスケールさせるために、MapReduceに適用できるように修正された主要である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Training Complex Models with Multi-Task Weak Supervision(2018)</title>
      <link>https://nryotaro.dev/posts/training_complex_models_with_multi_task_weak_supervision/</link>
      <pubDate>Sat, 28 Sep 2019 14:53:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/training_complex_models_with_multi_task_weak_supervision/</guid>
      <description>&lt;p&gt;ソース間の粒度や精度が揃っていることを前提とせず、&lt;a href=&#34;https://arxiv.org/pdf/1212.0478.pdf&#34;&gt;LohとWainwrightらの手法&lt;/a&gt;でソースの精度を推定し、ソースのラベルよりも精度が高いラベルをデータに与えるWeak supervsionの手法である。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Snorkel: Rapid Traning Data Creation with Weak Supervision (2020)</title>
      <link>https://nryotaro.dev/posts/snorkel_rapid_training_data_creation_with_weak_supervision/</link>
      <pubDate>Sat, 21 Sep 2019 12:01:51 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/snorkel_rapid_training_data_creation_with_weak_supervision/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Snorkelは、不正確なラベルを許容した上で半手動で大量の教師データを生成するWeak supervisionの一種である。
Snorkelは、データを受け取りラベルの予測値か予測不可能であることを示す値を返す関数を使い、データにラベルを割りあてる。
このラベル付けする関数は、ラベリング関数と呼ばれ、データとともにSnorkelに与えられる。
ラベリング関数は人間が実装したルールベースな関数でよい。
Snorkelは、入力された複数のラベリング関数をデータに適用し、データ件数xラベリング関数の個数をサイズとするラベルの予測値の行列を作る。その後、真のラベルを予測するモデルを行列にあてはめ、サンプルにラベルごとの真のラベルについての確信度を出力する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Software Engineering for Machine Learning: A Case Study</title>
      <link>https://nryotaro.dev/posts/software_engineering_for_machine_learning/</link>
      <pubDate>Sat, 14 Sep 2019 11:26:41 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/software_engineering_for_machine_learning/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;マイクロソフトにおける機械学習を応用したアプリケーションの開発についての調査結果をまとている。
著者らは、9人中の8名がMicrosoft Researchに所属し、本論文で2019年のICSEのSoftware Engineering in Practiceの分野でBest Paper Awardを受賞した。
主な貢献は、各チームにおける開発フローを9のステージに分けて解説したこと、機械学習を応用するアプリケーションやプラットフォームの開発におけるベストプラクティスを示したこと、機械学習を応用するアプリケーションの開発に固有の課題にまつわる論考の3つである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Class Imbalance, Redux(2012)</title>
      <link>https://nryotaro.dev/posts/class_imbalance_redux/</link>
      <pubDate>Sat, 07 Sep 2019 15:34:06 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/class_imbalance_redux/</guid>
      <description>&lt;p&gt;不均衡データの特徴が二値分類の予測性能に及ぼす影響を理論づける論文である。
理論と実験を通じて、多くのケースにおいて、アンダーサンプリングによる均衡データのバギングで予測性能があがったことを示している。
バギングは、アンダーサンプリングによる偏りを抑えて予測性能を安定させるために使われる。
論文で扱うモデルは、SVMなどの学習で経験損失を最小化するモデルに限定されている。発表学会は2011年のICDMである。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Learning on the Border: Active Learning in Imbalanced Data Classification(2007)</title>
      <link>https://nryotaro.dev/posts/learning_on_border/</link>
      <pubDate>Sat, 31 Aug 2019 02:05:53 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_on_border/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;不均衡データに対する二値分類の予測性能を能動学習で改善する。
学習器のマージン付近では正例と負例がよりバランスしていると仮定し、
マージン付近にあるデータを集めることで、均衡のとれたデータセットをあつめる。
具体的には、ラベルづけしたデータでSVMをオンライン学習し、無作為に抽出されたラベルのないデータの中で最も超平面に近いデータにラベルをつける手順をマージンの中にあるデータ数が変わらなくなるまでくりかえす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Distilling the Knowledge in a Neural Network(2015)</title>
      <link>https://nryotaro.dev/posts/distilling_the_knowledge_in_a_neural_network/</link>
      <pubDate>Sat, 24 Aug 2019 23:04:39 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/distilling_the_knowledge_in_a_neural_network/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;蒸留(Distilling)は、既存のモデルの予測性能をできるだけ落とさずに、より小さいモデルを作るための手法である。
複数のモデルからなるモデルや正則化された大きなモデルのように予測性能は高いが、同時に計算コストも高くつく。
蒸留の目的は本番運用に耐えられるデプロイ可能なモデル作ることにである。
出力層の活性化関数に温度つきソフトマックスを使った多クラス分類のモデルを蒸留する手法を提案、評価した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FastXML: A Fast, Accurate and Stable Tree-classifier for eXtreme Multi-label Learning(2014)</title>
      <link>https://nryotaro.dev/posts/fastxml/</link>
      <pubDate>Sat, 24 Aug 2019 15:45:40 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/fastxml/</guid>
      <description>&lt;p&gt;Extreme multi-label classificationの手法である。
Extreme multi-label classificationは、大量のラベルの候補からデータに関連する複数のラベルを推定するタスクである。
FastXMLは、決定木を弱学習器にしたアンサンブル学習であり、ノードを分割するための評価関数にnDCGを採用することで、学習時間を短縮し、予測精度を向上させた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Advantages and Disadvantages of a Monolithic Repository(2018)</title>
      <link>https://nryotaro.dev/posts/advantages_and_disadvantages_of_a_monolithic_repository/</link>
      <pubDate>Sat, 17 Aug 2019 14:49:48 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/advantages_and_disadvantages_of_a_monolithic_repository/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;マルチリポジトリと比べたときのモノリシックリポジトリの長所と短所を調査した。
論文は2018年のICSEでGoogleから発表された。
調査方法はGoogle社のエンジニアへのアンケートとエンジニアの行動ログの分析が採用されている。
Googleではモノリシックリポジトリが採用されており、エンジニアがこれまで経験したマルチリポジトリが比較対象となっている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 CatBoost: unbaiased boosting with categorical features(2017)</title>
      <link>https://nryotaro.dev/posts/cat_boost/</link>
      <pubDate>Sat, 10 Aug 2019 23:27:29 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/cat_boost/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;表題はNeurIPS 2018で発表されたCatBoostという勾配ブースティングの手法を論文にちなむ。
Target Statisticsというカテゴリカル特徴量の前処理と勾配ブースティングの学習時に生じる一種のleakageが起きることを示し、leakageをさけて前処理と学習をする手法を示した。
CatBoostは二進木の決定木を弱識別器に用いる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches(2019)</title>
      <link>https://nryotaro.dev/posts/are_we_really_making_much_progress/</link>
      <pubDate>Sat, 10 Aug 2019 14:39:11 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/are_we_really_making_much_progress/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;ニューラルネットワークを用いた推薦システムを提案、評価した論文における実験の再現性と予測性能の再評価した。
発表学会は、2019年のRecSys。
著者らは、以下の2つのRQに回答するためにトップ会議で発表された18の論文を調査した。
その結果、実験を再現できた論文は7稿であり、その中でも単純な手法を上回る性能が認められたのは1稿だけだった。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RQ1: ニューラルネットワークを用いた推薦システムの研究の再現性はどの程度か&lt;/li&gt;
&lt;li&gt;RQ2: 最近発表されたアルゴリズムは、ハイパーパラメタチューニングされた単純な手法と比べてどの程度性能がいいか&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>概要 Gaussian Processes for Regression(1996)</title>
      <link>https://nryotaro.dev/posts/gaussian_processes_for_regression/</link>
      <pubDate>Sat, 03 Aug 2019 21:54:03 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/gaussian_processes_for_regression/</guid>
      <description>&lt;p&gt;ガウス過程を回帰の問題に応用した。
著者らは、scikit-learnの&lt;a href=&#34;https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process&#34;&gt;ガウス過程回帰&lt;/a&gt;の
元になっている&lt;a href=&#34;http://www.gaussianprocess.org/gpml/chapters/RW.pdf&#34;&gt;Gaussian Processes for Machine Learning&lt;/a&gt;の著者と同じ。
論文の構成は、ガウス過程回帰の予測分布の式、ハイパーパラメタ推定方法、実験による評価からなる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Google Vizier: A Service for Black-Box Optimization(2017)</title>
      <link>https://nryotaro.dev/posts/google_vizier/</link>
      <pubDate>Sat, 03 Aug 2019 17:18:36 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/google_vizier/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;VizierはGoogleで開発されたブラックボックス最適化のためのサービスである。
論文は、Vizierのシステムアーキテクチャの構成とアルゴリズムの説明とその評価からなる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Learning Active Learning from Data(2017)</title>
      <link>https://nryotaro.dev/posts/learning_active_learning_from_data/</link>
      <pubDate>Sat, 27 Jul 2019 16:40:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/learning_active_learning_from_data/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;能動学習は、できるだけ少数のサンプルでモデルの予測性能を向上できる学習データセットを集める技術である。
論文は、2値分類問題のための能動学習で、サンプルを教師データに追加したときの汎化誤差の減少値を予測し、追加すべきサンプルを推定する。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 TextRank: Bringing Order into Texts(2004)</title>
      <link>https://nryotaro.dev/posts/textrank/</link>
      <pubDate>Sat, 20 Jul 2019 17:49:36 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/textrank/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;TextRankは、ドキュメントからキーワードとキーセンテンスを抽出するためのグラフベースのアルゴリズムである。
TextRankは、単語を頂点、文書をグラフとみなすことで、PageRankを応用する。
頂点の重要度を、頂点の内容のような局所的な情報ではなく、他の頂点との辺の接続関係を含むグラフ全体の大域的な情報から決定する。
TextRankは、PageRankと違い、辺ごとに重みを設定できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
      <link>https://nryotaro.dev/posts/subword_regularization/</link>
      <pubDate>Wed, 17 Jul 2019 12:51:29 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/subword_regularization/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;過去に&lt;a href=&#34;../neural_machine_translation_of_rate_words/&#34;&gt;紹介&lt;/a&gt;した&lt;a href=&#34;https://www.aclweb.org/anthology/P16-1162&#34;&gt;論文&lt;/a&gt;と同様、SentencePiece（ニューラルネットワークを用いた言語処理向けのトークナイザ・脱トークナイザ）の元になった論文である。
ノイズに対するロバストさを上げるために、単語のサブワード（部分文字列）を生成するユニグラム言語モデルの学習方法と、モデルから生成されたサブワード列を入力とする機械翻訳モデルの学習方法を提案した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Neural Machine Translation of Rare Words with Subword Units (2016)</title>
      <link>https://nryotaro.dev/posts/neural_machine_translation_of_rate_words/</link>
      <pubDate>Sat, 13 Jul 2019 17:19:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/neural_machine_translation_of_rate_words/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;SentencePiece（ニューラルネットワークを用いた言語処理向けのトークナイザ・脱トークナイザ）のトークナイズのアルゴリズムである。
単語をサブワード（単語の部分文字列）に分割し、サブワードを組み合わせて珍しい単語や未知語を表現することで、これらの出現頻度の低い単語の翻訳上げるというもの。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Annotated Transformer (2018)</title>
      <link>https://nryotaro.dev/posts/the_annotated_transformer/</link>
      <pubDate>Mon, 01 Jul 2019 12:57:09 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_annotated_transformer/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt;で提案されたTransformerのアーキテクチャを、サンプルコードとオリジナルの論文の引用を交えて解説している。 PyTorchで実装されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Embedding Logical Queries on Knowledge Graphs(2018)</title>
      <link>https://nryotaro.dev/posts/embedding_logical_queries_on_knowledge_graphs/</link>
      <pubDate>Sun, 17 Feb 2019 17:03:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/embedding_logical_queries_on_knowledge_graphs/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;一階述語論理式で表現されたクエリを満たすノードを、分散表現に変換し、ナレッジグラフの中から計算時間上効率よく見つけるアルゴリズムを提案した。
クエリに現れるエッジの数に対して計算時間が線形であることが特徴。
ただし、クエリには、存在量化と連接を使えるが、全称量化、選択、否定を使うことができない制約がある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Regularizing and Optimizing LSTM Language Models(2017)</title>
      <link>https://nryotaro.dev/posts/regularizing_and_optimizing_lstm_language_models/</link>
      <pubDate>Fri, 23 Nov 2018 19:27:00 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/regularizing_and_optimizing_lstm_language_models/</guid>
      <description>&lt;p&gt;LSTMをつかった言語モデルに正規化と最適化を適用し、実験でperplexityを評価した。
LSTMの実装に変更を加えない手法なので、NVIDIAやcuDNNなどの高速でブラックボックスなライブラリで実装できる。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Deep Joint Entity Disambiguation with Local Neural Attention(2017)</title>
      <link>https://nryotaro.dev/posts/deep_joint_entity_disambiguation/</link>
      <pubDate>Fri, 09 Nov 2018 21:11:50 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/deep_joint_entity_disambiguation/</guid>
      <description>&lt;p&gt;当ページで紹介した&lt;a href=&#34;https://aclweb.org/anthology/K18-1050&#34;&gt;End-to-End Neural Entity Linking&lt;/a&gt;(End-to-End) の著者らの先行研究にあたる。
End-to-EndがEntity LinkingのMention Detection(MD)とEntity Disambiguation(ED)の両方をアプローチの対象にしているのに対し、今回の論文はEDのみが対象となっている。
したがって、文章からmention（参照表現）が抽出されていることが前提にあり、提案の中心は、参照表現に対応するエンティティを候補の中から正しく求める手法にある。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 End-to-end Neural Entity Linking(2018)</title>
      <link>https://nryotaro.dev/posts/end_to_end_neural_entity_linking/</link>
      <pubDate>Fri, 02 Nov 2018 16:59:14 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/end_to_end_neural_entity_linking/</guid>
      <description>&lt;h3 id=&#34;背景&#34;&gt;背景&lt;/h3&gt;
&lt;p&gt;End to EndなEntity Linkingモデルのアーキテクチャを提案し、予測性能の評価実験で有用性を評価した。
実験のデータセットには、Entity annotationの評価に使える様々なデータセットを集めた&lt;a href=&#34;https://github.com/dice-group/gerbil/wiki&#34;&gt;Gerbil Platform&lt;/a&gt;が使われている。そのうちの&lt;a href=&#34;https://natural-language-understanding.fandom.com/wiki/AIDA-CoNLL&#34;&gt;AIDA/CoNLL&lt;/a&gt;データセットにおいて、提案手法は既存手法を超える予測性能を発揮した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 DeepType: Multilingual Entity Linking by Neural Type System Evolution(2018)</title>
      <link>https://nryotaro.dev/posts/deeptype/</link>
      <pubDate>Fri, 26 Oct 2018 20:53:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/deeptype/</guid>
      <description>&lt;p&gt;既存のオントロジから型システムを構築するアルゴリズムと型システムによるEntity Linking(EL)を提案した。
DeepTypeにおける型は、Wikipediaのようなオントロジにおける関係を意味する。
たとえば、オントロジに&lt;code&gt;Human&lt;/code&gt;という根から&lt;code&gt;instance of&lt;/code&gt;で結ばれる子ノードがあれば、&lt;code&gt;IsHuman&lt;/code&gt;を型とみなす。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
      <link>https://nryotaro.dev/posts/conditional_random_fields_probabilistic_models_for_segmenting_and_labeling_sequence/</link>
      <pubDate>Fri, 12 Oct 2018 18:39:08 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/conditional_random_fields_probabilistic_models_for_segmenting_and_labeling_sequence/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;条件付き確率場（Conditional Random Fields, CRF）を提案し、品詞タグづけにおけるerror rateをもとに評価した。
評価の比較対象には、Maximum entropy Markov models(MEMMs)が採用されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Bidirectional LSTM-CRF Models for Sequence Tagging(2015)</title>
      <link>https://nryotaro.dev/posts/bidirectional_lstm_crf_models_for_sequence_tagging/</link>
      <pubDate>Fri, 05 Oct 2018 18:46:36 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/bidirectional_lstm_crf_models_for_sequence_tagging/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;NLPにおける系列ラベリングためのニューラルネットワークアーキテクチャの提案と評価がなされている。
このアーキテクチャは、当サイトで以前紹介した&lt;a href=&#34;https://aclweb.org/anthology/papers/C/C18/C18-1139/&#34;&gt;Contextual String Embeddings for Sequence Labeling&lt;/a&gt;で応用されている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>抄訳 Contextual String Embeddings for Sequence Labeling(2018)</title>
      <link>https://nryotaro.dev/posts/context_string_embeddings_for_sequence_labeling/</link>
      <pubDate>Fri, 28 Sep 2018 16:29:46 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/context_string_embeddings_for_sequence_labeling/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;表題の論文は、&lt;a href=&#34;https://github.com/flairNLP/flair&#34;&gt;flair&lt;/a&gt;のアルゴリズムを提案、評価したもの。
論文は、テキストの系列ラベリングに向いた単語の分散表現モデルを提案し、
提案手法が予測性能において既存手法より優れいたことを実験的に示した。
本手法における単語の分散表現は、単語の字面だけでなく、文中における単語の出現位置によって決まる。
いいかえると、同じ単語であっても、文中における出現位置が異なれば、単語は異なる分散表現に変換される。
著者らは、分散表現に文脈の情報を含められることを強調して、提案手法をContextual String Embeddingsと名付けた。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ Universal Language Model Fine-tuning for Text Classification</title>
      <link>https://nryotaro.dev/posts/universal_language_model_fine_tuning_for_text_classification/</link>
      <pubDate>Fri, 14 Sep 2018 16:33:43 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/universal_language_model_fine_tuning_for_text_classification/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;UMLFiTという、様々なNLPの問題に適用可能なファインチューニングの手法を提案、評価した。
評価手段として、6種のテキスト分類のタスクにおける既存手法とのエラー率の比較が採られている。
主要な評価として、100件のラベル付きデータだけでその100倍のデータを要した事前学習を用いない手法と同等の予測性能が出たことを報告している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
      <link>https://nryotaro.dev/posts/metapath2vec/</link>
      <pubDate>Fri, 07 Sep 2018 18:31:47 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/metapath2vec/</guid>
      <description>&lt;p&gt;異種混合ネットワークから、ノード数x次元数の分散表現を獲得するための手法。
異種混合とは、企業、業界、ニュースなど複数の種類の概念がグラフのノードとして扱われていることを意味する。
獲得した分散表現を訓練データとして分類、クラスタリング、検索に応用し、既存手法と比較している。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Applying Deep Learning To Airbnb Search</title>
      <link>https://nryotaro.dev/posts/applying_deep_learning_to_airbnb_search/</link>
      <pubDate>Fri, 31 Aug 2018 19:21:10 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/applying_deep_learning_to_airbnb_search/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;論文では、Airbnbが深層学習を宿泊先検索に適用した時の試行錯誤と結果を紹介している。
採用したモデルのアルゴリズムと特徴量エンジニアリングの説明が本稿の大部分を占める。
深層学習を試す以前はGBDTを採用おり、以下の順にアルゴリズムを変えていった。
当初は、アルゴリズムを段階的に高度にしていくつもりはなく、1.以前には複雑なアルゴリズムをいきなり試したが、失敗に終わっている。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>メモ The Relationship Between Precision-Recall and ROC Curve</title>
      <link>https://nryotaro.dev/posts/the_relationship_between_precision_recall_and_roc_curve/</link>
      <pubDate>Sat, 25 Aug 2018 00:56:13 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/the_relationship_between_precision_recall_and_roc_curve/</guid>
      <description>ROCとPrecision Recallの関係を示した論文。
 1 Recallが0でなければ、ROC曲線には一対一に対応するPR曲線がある。 2 PR曲線AがPR曲線Bに対して常に優位であることとは、ROC曲線においてAがBより常に優位であることの必要十分条件。 3 1,2よりROC空間上の凸包に対応するPR曲線は、他の妥当なPR曲線よりも優位な曲線になる。 4 線形補間でROC曲線を描くことは妥当。一方で、Recallの分母は固定値であるが、Precisionの分母の値はRecallが上がると増える。そのため、PR曲線を線形補間でプロットすると、評価の甘い曲線になる。   論文はこちらからダウンロードできます。</description>
    </item>
    
    <item>
      <title>メモ Enriching Word Vectors with Subword Information</title>
      <link>https://nryotaro.dev/posts/enriching_word_vectors_with_subword_information/</link>
      <pubDate>Fri, 10 Aug 2018 17:29:44 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/enriching_word_vectors_with_subword_information/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;Fasttextの論文。
Character n-gramsを入力としてskip-gramのモデルを作る方法を提案、評価した。
単語の部分文字列（subword）を使わない手法や形態素解析に頼る手法よりも提案手法が優れていることを実験で示した。
単語のベクトルは部分文字列のベクトルの和である。
実験の考察では、そのために、未知語の部分文字列が学習データにあれば、未知語に対しても妥当な分散表現を与えることができるとあった。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 When Do Changes Induce Fixes?</title>
      <link>https://nryotaro.dev/posts/when_do_changes_induce_fixes/</link>
      <pubDate>Fri, 03 Aug 2018 21:47:03 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/when_do_changes_induce_fixes/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;バージョン管理ツールで追跡されている変更とバグを紐付ける手法を提案した。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>要約 A Simple Semi-supervised Algorithm For Named Entity Recognition</title>
      <link>https://nryotaro.dev/posts/a_simple_semi_supervised_for_ner/</link>
      <pubDate>Tue, 17 Jul 2018 12:51:29 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/a_simple_semi_supervised_for_ner/</guid>
      <description>&lt;h3 id=&#34;概要&#34;&gt;概要&lt;/h3&gt;
&lt;p&gt;CRFに入力する学習データを集めるための半教師学習の手法を提案と評価した論文。
本手法はCRFに与える学習データを集めるための手法であり、CRFのアルゴリズム自体に変更を加えることはない。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>要約 Applying Conditional Random Fields to Japanese Morphological Analysis</title>
      <link>https://nryotaro.dev/posts/applying_crf_to_japanese_morph_analysis/</link>
      <pubDate>Tue, 17 Jul 2018 12:51:29 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/applying_crf_to_japanese_morph_analysis/</guid>
      <description>&lt;p&gt;Mecabの中の人の&lt;a href=&#34;http://chasen.naist.jp/chaki/t/2009-09-30/doc/mecab-cabocha-nlp-seminar-2009.pdf&#34;&gt;資料&lt;/a&gt;で紹介でされている、Mecabのアルゴリズムを提案・評価した論文。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>概要 Text Classification from Labeled and Unlabeled Documents using EM</title>
      <link>https://nryotaro.dev/posts/text_cls_from_lbl_and_unlbl_doc_em/</link>
      <pubDate>Sun, 08 Jul 2018 01:23:49 +0900</pubDate>
      
      <guid>https://nryotaro.dev/posts/text_cls_from_lbl_and_unlbl_doc_em/</guid>
      <description>&lt;h3 id=&#34;アルゴリズム&#34;&gt;アルゴリズム&lt;/h3&gt;
&lt;p&gt;提案手法は、Naive BayesとEMアルゴリズムを組み合わせたもの。
ラベル付きデータが\(D^l\)でラベルなしデータが\(D^u\)で表されるとき、対数尤度\(\log P(D^l)P(D^u)\)を最大化する問題を解く。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://nryotaro.dev/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nryotaro.dev/about/</guid>
      <description>免責 当ウェブサイトに掲載されている情報の正確性については万全を期していますが、当ウェブサイトの著者は利用者が当ホームページの情報を用いて行う一切の行為について、何らの責任を負うものではありません。</description>
    </item>
    
  </channel>
</rss>
