<!DOCTYPE html>
<html>
  <head>
	<meta name="generator" content="Hugo 0.119.0">
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.min.87b8f4076c47000c129b0c8b240a71216448e3aedd7144174c55776680a783b0.css">
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.ja.min.66816a64bc4ce50ec1c4b76199ca9d69163fc8a69f7abc6ea758d1968d8618b0.css">
    

<link rel="stylesheet" href="https://nryotaro.dev/scss/articles.min.ad090cad4f6d9fadf06a509c7ea790bd1fc13b24802f351ad71342659fdebf9f.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Blanket</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="https://nryotaro.dev/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="https://nryotaro.dev/about">About</a></li>
          <li><a href="https://nryotaro.dev/posts">Posts</a></li>
          <li><a href="https://nryotaro.dev/tags">Tags</a></li>
	  
	  <li><a href="https://nryotaro.dev/en/">en</a></li>
	  
        </ul>
      </nav>
    </header>
    
<main>
    <h1>Posts</h1>
    
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/multilabel_classification_with_label_correlations_and_missing_labels/">Multilabel Classification with Label Correlations and Missing Labels(2014)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>ラベルの相関関係を学習し推論に利用するマルチラベルの線形モデルを提案した論文である。
相関関係のあるラベル集合を相関関係のないラベル集合に変換し、ラベルごとに分けて学習する手法、Label transformationを応用する。
分類器は、相関関係だけなく、学習データに与えられていないラベルを推定するように拡張できる。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A9%E3%83%99%E3%83%AB%E5%88%86%E9%A1%9E/">
		#マルチラベル分類
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">January 6, 2020</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/learning_deep_structured_semantics_models_for_web_search_using_clickthrough_data/">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data(2013)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>クエリと文書を同じ低次元の空間に射影する深層学習のモデルを提案した論文である。
クエリと文書は、適合度合いが高いほど、近くに配置される。
教師データは、クエリと文書の組からなる教師データである。
実験では、商用検索エンジンから抽出した16510件のクエリと対応するWeサイトのタイトルがつかわれる。
Web文書の大量の語彙をあつかうために、語彙の増加に対して次元数を抑えるbag-of-wordsの手法、word hasingも提案した。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%83%A9%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0%E5%AD%A6%E7%BF%92/">
		#ランキング学習
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">January 4, 2020</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/classification_in_the_presence_of_label_noise/">Classification in the Presence of Label Noise: a Survey(2013)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>ノイズのある教師データによるクラス分類の<a href="https://romisatriawahono.net/lecture/rm/survey/machine%20learning/Frenay%20-%20Classification%20in%20the%20Presence%20of%20Label%20Noise%20-%202014.pdf">サーベイ論文</a>である。発表時期は、<a href="https://ieeexplore.ieee.org/document/6685834">2013年の12月</a>である。
主な内容は、ノイズの分類、ノイズが分類に及ぼす影響、ノイズへの対策である。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/label-noise/">
		#Label noise
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87/">
		#サーベイ論文
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 30, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/distributed_representations_of_sentences_and_documents/">Distributed Representations of Sentences and Documents(2014)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p><a href="https://radimrehurek.com/gensim/models/doc2vec.html">Doc2Vec</a>のアルゴリズムとして採用されたニューラル言語モデルParagraph Vectorを提案した論文である。
bag of wordsは、文書の単語順を記憶せず、また、似た意味の単語ベクトルと無関係なベクトルを単語にわりあてる。
Paragraph Vectorは、文脈中の単語と抽出元のパラグラフから文脈の中心の単語をあてられるように学習することで、可変長文字列から固定長の文書埋め込みベクトルを生成できるようになる。
これにより、単語順と単語の意味を記憶したベクトルの生成を実現する。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/embedding/">
		#Embedding
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 28, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/glove_vectors_for_word_representation/">GloVe: Global Vectors for Word Representation(2014)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p><a href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe</a>は,コーパスに出現する単語の共起回数を学習するニューラル言語モデルである。
既存手法を単語の出現頻度の統計値つかう手法と対数双線形モデルに分類し、両者の長所を備え短所を補う手法として、GloVeを提案する。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/embedding/">
		#Embedding
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 21, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/bert/">BERT: Pre-training of Deep Bidirectional Transformers for Lnaguages Understaing(2018)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>BERTは<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>にあるTransformerをアーキテクチャに導入した分散表現のモデルであり、本稿は、事前学習済みのBERTにファインチューニングを適用しQAタスクや自然言語推論のベンチマークにおいて既存研究を上回る結果を示している。
なお、アーキテクチャに関する説明は少なく、子細に知りたい場合は<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>や<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>を参照するように案内されている。</p></div>
        <div class="article-footer">
	  
          <span class="date">December 14, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/150_successfuly_machine_learning_modes/">150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com(2019)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>宿泊予約サービス<a href="http://booking.com/">Booking.com</a>におけるモデルの開発運用でえられた教訓を6つにまとめたKDD2019の論文である。
教訓の主眼を収益におき、6つの教訓を通して、実運用環境における仮説と実験を反復する重要性を強調する。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/empirical-research/">
		#Empirical research
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/mlops/">
		#MLOps
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 14, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/ranking_relevance_in_yahoo_search/">Ranking Relevance In Yahoo Search(2016)</a></h3>
        <div class="summary"><h3 id="概要">概要</h3>
<p>Yahooの検索エンジンを解説するKDD16の論文である。
論文におけるランキングの課題は、クエリと文書の語彙がことなること、ほとんどのクエリは滅多に入力されないこと、クエリの意味の解釈が難しいことである。
これらの課題に対する手法として、ランキングのモデル、特徴のつくりかた、クエリを文書によせる翻訳モデルを解説する。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%83%A9%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0%E5%AD%A6%E7%BF%92/">
		#ランキング学習
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 7, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/a_dual_embedding_space_model_for_document_ranking/">A Dual Embedding Space Model for Document Ranking</a></h3>
        <div class="summary"><p>Dual Embedding Space Model(DESM)は、word2vecによるランキング学習である。
word2vecは、単語ごとに、入力と出力それぞれに近い重みから、2つの分散表現を生成できる。
DESMは、入力側の重みでクエリを、出力側の重みで文書を、それぞれ分散表現に変換する。</p>
<p>実験では、BM25と比較して評価した。
DESMだけで順位づけをすると偽陽性が高くなるが、DESMとBM25の加重平均をとるとBM25よりも高いNDCG値になった。
アルゴリズムを実装し<a href="https://github.com/nryotaro/desm">公開</a>した。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%83%A9%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0%E5%AD%A6%E7%BF%92/">
		#ランキング学習
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/embedding/">
		#Embedding
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 30, 2019</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/on_calibration_of_modern_neural_networks/">論文メモ On Calibration of Modern Neural Networks(2017)</a></h3>
        <div class="summary"><p>ネットワークの複雑化、バッチ正則化、重み減衰を使わない、負の対数尤度の過学習が汎化精度を上げるが、予測確率と精度のズレを大きくすることを実験的に示した。
予測確率を補正する6つの手法を19種類のクラス分類のデータセットに適用した結果、
最も補正できたものは、温度つきソフトマックスの出力を予測確率にする場合であった。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/confidence-calibration/">
		#Confidence calibration
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 23, 2019</span>
        </div>
    </article>
    
</main>

    

<ul class="pagination">  
    
    <li>
      <a href="/">
	<i class="fa-solid fa-xl fa-angles-left"></i>
      </a>
    </li>
    <li>
      <a href="/page/24/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/page/26/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    <li>
      <a href="/page/30/">
	<i class="fa-solid fa-xl fa-angles-right"></i>
      </a>
    </li>
    
</ul>

    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/nryotaro">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="https://nryotaro.dev/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
