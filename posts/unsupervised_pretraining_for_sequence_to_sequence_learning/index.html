<!DOCTYPE html>
<html>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=38897&amp;path=livereload" data-no-instant defer></script>
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="http://localhost:38897/scss/base.min.87b8f4076c47000c129b0c8b240a71216448e3aedd7144174c55776680a783b0.css">
    
    <link rel="stylesheet" href="http://localhost:38897/scss/base.ja.min.66816a64bc4ce50ec1c4b76199ca9d69163fc8a69f7abc6ea758d1968d8618b0.css">
    

<link rel="stylesheet" href="http://localhost:38897/scss/single.min.6706f2a051a6ab63f5377cb6e9a19e35f0265340ff70caf700b282f1962c59e1.css">

<link rel="stylesheet" href="http://localhost:38897/scss/posts/single.min.c8bed992e3e72febf0bd227ae4d39d51e4a5a169e0a19ca4cef950e3ab79cfe0.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Unsupervised Pretraining for Sequence to Sequence Learning(2017)</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="http://localhost:38897/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="http://localhost:38897/about">About</a></li>
          <li><a href="http://localhost:38897/posts">Posts</a></li>
          <li><a href="http://localhost:38897/tags">Tags</a></li>
	  
        </ul>
      </nav>
    </header>
    
<main>
  <h1>Unsupervised Pretraining for Sequence to Sequence Learning(2017)</h1>
  <ul class="tags">
    
    <li class="tag">
      <a href="http://localhost:38897/tags/seq2seq/">
	#Seq2seq
      </a>
    </li>
    
    <li class="tag">
      <a href="http://localhost:38897/tags/pretraining/">
	#Pretraining
      </a>
    </li>
    
  </ul>  
  <span class="date">February 16, 2020</span>    
  <div>
    <h3 id="概要">概要</h3>
<p>事前学習とファインチューニングにより<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">seq2seq</a>の汎化性能を改善する手法を提案した論文である。
encoderの重みを学習済み言語モデルの重みで初期化する。
decoderについても、encoderと別の言語モデルを用意し、その重みで初期化する。
ただし、工夫のないファインチューニングをすると<a href="https://arxiv.org/pdf/1312.6211.pdf">破滅的忘却</a>が生じてしまう。
そこで、ファインチューニングでは言語モデルとseq2seqの目的関数の両方を学習につかうことで、過学習をさけ、汎化性能を確保する。</p>
<h3 id="アーキテクチャ">アーキテクチャ</h3>
<p>言語モデルで重みを初期化する着想は、encoderのないdecoderや出力層のないencoderが言語モデルに似ているという観察に由来する。
実験では、言語モデルに1層のLSTMが採用されている。
encoderとdecoderのembedding層と1層目のLSTMを各言語モデルの重みで初期化する。
seq2seqには3層のLSTMが使われる。
事前学習のほか、後述のresidual connectionsと注意機構を導入することで、わずかであるが一貫した改善がみられた。</p>
<h4 id="residual-connections">Residual connections</h4>
<p>decoderの2層目以降は無作為に初期化されるため、ソフトマックス層へ入力されるベクトルは無作為になる。
これを避けるために、以下の図(a)のように、1層目のLSTMの出力からソウトマックス層へのresidual connectionを設ける。
青の背景は、事前学習でえられた重みで初期化された範囲を示す。
<img alt="fig1" src="/unsupervised_pretraining_for_sequence_to_sequence_learning/fig1.png"></p>
<h4 id="multi-layer-attention">Multi-layer attention</h4>
<p>図の(b)のように、encoderにおけるLSTMの最初と最後の層の状態とdecoderのソフトマックスへの入力ベクトル\(q_t\)から文脈ベクトルをつくる。
encoderの1層目の状態を\(h^{1}_1 , \dots , h^1_{T}\), 最後の層の状態を\(h^{L}_1 , \dots , h^L_{T}\)とすると文脈ベクトル\(c_t\)を次のように算出する。
$$
\alpha_i = \frac{\exp (q_t \cdot h^{L}_i)}{\sum^T_{j=1}\exp (q_t \cdot h^L_j)}
$$
$$
c^1_{t}=\sum^{T}_{i=1}\alpha_ih^1_{i}
$$
$$
c^L_{t}=\sum^T_{i=1}\alpha_ih^L_{i}
$$
$$
c_t=[c^1_{t};c^L_t]
$$</p>
<hr>
<ul>
<li>論文は<a href="https://www.aclweb.org/anthology/D17-1039.pdf">こちら</a>からダウンロードできます。</li>
<li>図はすべて論文から引用されています。</li>
</ul>
  </div>
</main>

    
<ul class="pagination">
    
    <li>
      <a href="/posts/playing_atari_with_deep_reinforcement_learning/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/posts/latent_dirichlet_allocation/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    
</ul>


    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/ryotaro612">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/ryotaro612/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="http://localhost:38897/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
