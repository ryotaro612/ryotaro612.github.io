<!DOCTYPE html>
<html>
  <head>
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.min.852c1c9119a48dcf6a4274cb71a00bd3b5d55b5040e4d37d525e743de1d0e8da.css">
    

<link rel="stylesheet" href="https://nryotaro.dev/scss/single.min.78272bdd99a3c4f03ee4c826bbe3a970cda497ed85b5da23b0af6dbaf9f30522.css">

<link rel="stylesheet" href="https://nryotaro.dev/scss/posts/single.min.c8bed992e3e72febf0bd227ae4d39d51e4a5a169e0a19ca4cef950e3ab79cfe0.css">
<link rel="stylesheet" href="/css/syntax.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>    
    
    <title>論文メモ Latent Dirichlet Allocation</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="/about">About me</a></li>
          <li><a href="/posts">Posts</a></li>
          <li><a href="/tags">Tags</a></li>
        </ul>
      </nav>
    </header>
    
<main>
  <h1>論文メモ Latent Dirichlet Allocation</h1>
  <ul class="tags">
    
  </ul>  
  <span class="date">February 23, 2020</span>    
  <div>
    <h3 id="概要">概要</h3>
<p>トピックモデルの潜在的ディリクレ配分法(LDA)の原論文である。
LDAは、テキストコーパスのような離散データの確率的生成モデルである。
意味のあるデータのまとまりに対する端的な説明を与える情報を見つけることを目的としている。
3つの階層からなる階層ベイズモデルである。
、データの要素は、各トピックを表すモデルの混合モデルから生成される。
トピックもまた混合モデルから確率的に生成される。
推論にはベイズ変分法を、パラメタの推定にはEMアルゴリズムをもちいらる。</p>
<h3 id="モデル">モデル</h3>
<p>テキストコーパスをLDAでモデル化する場合を考える。
ただし、文書をbag-of-wordsでとらえ、単語の並びは考慮しない。
単語\(w\)を語彙数\(V\)のOne-Hotベクトルで扱う。
文書\(\boldsymbol{\rm w}\)は\(N\)個の単語列\((w_1, w_2, \dots , w_N)\)からなり、
コーパスは\(M\)個の文書からなる\(\mathcal{D}=\{\boldsymbol{\rm w}_1, \boldsymbol{\rm w}_2, \dots ,\boldsymbol{\rm w}_M\}\)とする。
このとき、LDAは、隠れた複数のトピックから文書がなりたち、トピックは単語の分布で表わされると仮定する。
すなはち、コーパス\(\mathcal{D}\)の文書\(\boldsymbol{\rm w}\)が次の過程で生成されるとみなす。</p>
<ul>
<li>\(N\sim \text{Poissson}(\xi)\)を選ぶ</li>
<li>\(\theta\sim \text{Dir}(\alpha)\)を選ぶ</li>
<li>単語\(w_n\)\((1\le n\le N)\)について
<ul>
<li>トピック\(z_n\sim \text{Multinomial}(\theta)\)を選ぶ</li>
<li>単語\(w_n\)を\(p(w_n \mid z_n,\beta)\)から選ぶ</li>
</ul>
</li>
</ul>
<p>この過程は、以下のように、グラフィカルモデルで図示できる。
外側の枠は文書を示し、内側の枠はトピックと単語の生成を示す。
<img src="/lda/graph.png" alt="graph">
トピック数を\(k\)とすると、ディリクレ関数\(\text{Dir}(\alpha)\)は
$$
p(\theta\mid \alpha) =\frac{\Gamma (\sum^k_{i=1}\alpha_i)}{\prod^k_{i=1}\Gamma (\alpha_i)}\theta_1^{\alpha_1-1}\cdots \theta_1^{\alpha_k-1}
$$
となる。
\(\Gamma\)はガンマ関数である。
このとき
$$
p(\theta,\boldsymbol{\rm z},\boldsymbol{\rm w}\mid \alpha ,\beta)=p(\theta\mid \alpha)\prod_{n=1}^N p(z_n\mid\theta)p(w_n\mid z_n,\beta)
$$
\(\theta\)と\(\boldsymbol{\rm z}\)について周辺化すると
$$
p(\boldsymbol{\rm w}\mid\alpha, \beta) = \int p(\theta\mid \alpha)\left( \prod_{n=1}^N\sum_{z_n}p(z_n\mid\theta)p(w_n\mid z_n,\beta)\right) d\theta
$$
以上よりコーパスの生成モデルは
$$
p(\mathcal{D}\mid\alpha, \beta) = \prod^M_{d=1}\int p(\theta_d\mid \alpha)\left( \prod_{n=1}^{N_d}\sum_{z_{dn}}p(z_{dn}\mid\theta_d)p(w_{dn}\mid z_{dn},\beta)\right) d\theta_d
$$
となる。</p>
<h3 id="推定">推定</h3>
<p>文書のトピックの条件付き確率は次の式で表わされる。
$$
p(\theta , \boldsymbol{\rm z}\mid \boldsymbol{\rm w}, \alpha, \beta)=\frac{p(\theta,\boldsymbol{\rm z},\boldsymbol{\rm w}\mid\alpha, \beta)}{p(\boldsymbol{\rm w}\mid \alpha,\beta)}
$$
前節より右辺の分母は
$$
p(\boldsymbol{\rm w}\mid \alpha,\beta)=\frac{\Gamma(\sum_i\alpha_i)}{\prod_i\Gamma(\alpha_i)}\int\left(\prod^k_{i=1}\theta^{\alpha_i-1}_i\right)\left(\prod^N_{n=1}\sum^k_{i=1}\prod^V_{j=1}(\theta_i\beta_{ij})^{w^j_n}\right)d\theta
$$
となる。
\(\beta\)は\(k\times V\)の行列であり\(\beta_{ij}=p(w^j=1\mid z^i=1)\)である。
上の式は、\(\theta\)と\(\beta\)が結合しているために導出することができない(<a href="https://www.jstor.org/stable/2288131?seq=1">Dickey, 1983</a>)。
これに対して、ベイズ変分法を用いた推定方法がもちいられている。</p>
<hr>
<ul>
<li>論文を<a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">こちら</a>からダウンロードできます。</li>
<li>画像はすべて論文から引用されています。</li>
</ul>
  </div>
</main>

    
<ul class="pagination">
    
    <li>
      <a href="/posts/unsupervised_pretraining_for_sequence_to_sequence_learning/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/posts/sentence_piece/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    
</ul>


    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/nryotaro">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
