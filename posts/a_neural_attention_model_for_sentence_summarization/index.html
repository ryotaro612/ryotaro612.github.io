<!DOCTYPE html>
<html>

<head>
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-64L0YEFRY5"></script>
    <script>
        if (location.hostname != 'localhost') {
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'G-64L0YEFRY5');
        }
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
        integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
        crossorigin="anonymous" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="/css/base.css">
    
<link rel="stylesheet" href="/css/single.css">
<link rel="stylesheet" href="/css/posts/single.css">
<link rel="stylesheet" href="/css/syntax.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/4e0e0c0a41.js" crossorigin="anonymous"></script>
    <title>論文メモ A Neural Attention Model for Sentence Summarization</title>
</head>

<body>
    <header class="navigation">
        <h2><a href="/">Coda</a></h2>
        <nav>
            <ul>
                <li><a href="/about">About me</a></li>
                <li><a href="/posts">Posts</a></li>
                <li><a href="/gallery">Gallery</a></li>
            </ul>
        </nav>
    </header>
    
<main>
    <h1>論文メモ A Neural Attention Model for Sentence Summarization</h1>
    <span class="date">June 20, 2020</span>
    <div>
        <p>注意機構による深層学習で文を要約する手法である。
もとの文にない単語を含む要約文を生成できるが、生成前に文の長さを決めておかなければならない。</p>
<p>モデルへの入力文を\(\boldsymbol{\rm x}\), 出力文を\(\boldsymbol{\rm y}\)とすると、スコア関数\(s\)を含む次の式が最適な要約文になるようモデルを学習する。
$$
\underset{\boldsymbol{\rm y}\in \mathcal{Y}}{\operatorname{argmax}} s(\boldsymbol{\rm x}, \boldsymbol{\rm y})
$$
入力文\(\boldsymbol{\rm x}_1, \dots , \boldsymbol{\rm x}_M\)や出力文\(\boldsymbol{\rm y}_1, \dots , \boldsymbol{\rm y}_N\)のトークンは、要素数\(\mid V\mid\)の語彙\(\mathcal{V}\)のOne-hotベクトルで表される。
要約であるため、\(N &lt; M\)になる。</p>
<p>ウインドウサイズを\(C\)とする\(\boldsymbol{\rm y}_c\triangleq \boldsymbol{\rm y}_{[i-C+1,\dots , i]}\)によるマルコフ性を仮定し、スコア関数を次の式で近似する。
$$
\begin{align}
s(\boldsymbol{\rm x}, \boldsymbol{\rm y})&amp;\approx \sum^{N-1}_{i=0}\log p(\boldsymbol{\rm y}_{i+1}\mid \boldsymbol{\rm x}, \boldsymbol{\rm y}_c;\theta)\\<br>
p(\boldsymbol{\rm y}_{i+1}\mid \boldsymbol{\rm x}, \boldsymbol{\rm y}_c;\theta)&amp;\propto \exp (\boldsymbol{\rm Vh}+\boldsymbol{\rm W}\text{enc}(\boldsymbol{\rm x,y_c}))\\<br>
\tilde{\boldsymbol{\rm y}}_c&amp;=[\boldsymbol{\rm E}\boldsymbol{\rm y}_{i-C+1},\dots , \boldsymbol{\rm E}\boldsymbol{\rm y}_i]\\<br>
\boldsymbol{\rm h}&amp;=\tanh (\boldsymbol{\rm U}\tilde{\boldsymbol{\rm y}}_c)
\end{align}
$$
パラメタは\(\theta = (\boldsymbol{\rm E},\boldsymbol{\rm U}, \boldsymbol{\rm V}, \boldsymbol{\rm W})\)である。
\(\boldsymbol{\rm E}\in \mathbb{R}^{D\times V}\)は分散表現の行列であり、\(\boldsymbol{\rm U}\in \mathbb{R}^{(CD)\times H}, \boldsymbol{\rm V}\in \mathbb{R}^{V\times H}, \boldsymbol{\rm W}\in \mathbb{R}^{V\times H}\)は重み行列である。
\(\boldsymbol{\rm h}\)はサイズ\(H\)の隠れ層である。</p>
<p>\(\text{enc}\)は文脈と入力を表すサイズ\(H\)のベクタを返すエンコーダであり、注意機構がつかわれる。
$$
\begin{align}
\text{enc}(\boldsymbol{\rm x}, \boldsymbol{y}_c)&amp;=\boldsymbol{\rm p}^{T}\boldsymbol{\rm \bar{x}},\\<br>
\boldsymbol{\rm p}&amp;\propto \exp(\tilde{\boldsymbol{\rm x}}\boldsymbol{\rm P}\tilde{\boldsymbol{\rm y}}'_c),\\<br>
\tilde{\boldsymbol{\rm x}}&amp;=[\boldsymbol{\rm F}\boldsymbol{\rm x}_1,\dots , \boldsymbol{\rm F}\boldsymbol{\rm x}_M],\\<br>
\tilde{\boldsymbol{\rm y}}'_c&amp;=[\boldsymbol{\rm G}\boldsymbol{\rm y}_{i-C+1}, \dots , \boldsymbol{\rm G}\boldsymbol{\rm y}_i],\\<br>
\forall i\ \bar{\boldsymbol{\rm x}}_i &amp;=\sum^{i+Q}_{q=i-Q}\tilde{\boldsymbol{\rm x}}_i/Q
\end{align}
$$
\(\boldsymbol{\rm G}\in \mathbb{R}^{D\times V}\)は文脈を示す分散表現で、\(\boldsymbol{\rm P}\in \mathbb{R}^{H\times (CD)}\)は重み行列である。
\(Q\)は平準化のためのウィンドウのサイズに対応する。</p>
<p>学習では負の対数尤度関数を損失関数にもちいる。\(J\)件の学習データのうち\(j\)番目のデータを\((\boldsymbol{\rm x}^{(j)}, \boldsymbol{\rm y}^{(j)})\)とすると、損失関数は次の式になる。
$$
\begin{align}
\text{NNL}(\theta)&amp;=-\sum^J_{j=1}\log p(\boldsymbol{\rm y}^{(j)}\mid \boldsymbol{\rm x}^{(j)};\theta)\\<br>
&amp;=-\sum^J_{j=1}\sum^{N-1}_{i=1}\log p (\boldsymbol{\rm y}^{(j)}_{i+1}\mid \boldsymbol{\rm x}^{(j)},\boldsymbol{\rm y}_c;\theta)
\end{align}
$$</p>
<p>推定時は、次の\(\boldsymbol{\rm y}^*\)をBeam searchにより近似的に求める。</p>
<p>$$
\boldsymbol{\rm y}^* = \underset{\boldsymbol{\rm y}\in \mathcal{Y}}{\operatorname{argmax}}\sum^{N-1}_{i=0}g(\boldsymbol{\rm y}_{i+1},\boldsymbol{\rm x}, \boldsymbol{\rm y}_c)
$$</p>
<p><img src="/a_neural_attention_model_for_sentence_summarization.png" alt="fig"></p>
<hr>
<ul>
<li>論文は<a href="https://www.aclweb.org/anthology/D15-1044.pdf">こちら</a>からダウンロードできます。</li>
</ul>
    </div>
</main>

    
<ul class="pagination">
    
    <li>
        <a href="/posts/bleu/"><span class="material-icons">chevron_left</span></a>
    </li>
    
    
    <li>
        <a href="/posts/do_developers_discover_new_tools_on_the_toilet/"><span class="material-icons">chevron_right</span></a>
    </li>
    
</ul>


    <footer>
        <ul>
            
            <li>
                <a href="https://github.com/nryotaro">
                    <i class="fab fa-github"></i>
                </a>
            </li>
            
            
            <li>
                <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </li>
            
            <li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
            
        </ul>
        <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
</body>

</html>