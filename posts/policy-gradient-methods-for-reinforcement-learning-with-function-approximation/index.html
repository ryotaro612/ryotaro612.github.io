<!DOCTYPE html>
<html lang="ja">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="http://localhost:1313/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
    
    <link rel="stylesheet" href="http://localhost:1313/scss/base.ja.min.66816a64bc4ce50ec1c4b76199ca9d69163fc8a69f7abc6ea758d1968d8618b0.css">
    

<link rel="stylesheet" href="http://localhost:1313/scss/single.min.8ed5cbfb3dc516e9c4459a65252e4e6e8e9026aabfa9eb9f602cf53b30754966.css">

<link rel="stylesheet" href="http://localhost:1313/scss/posts/single.min.0008713f3f7556e78c13cb8efde9cf534239cb206c10ba85ce5a76e6ae9ce758.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Policy Gradient Methods for Reinforcement Learning With Function Approximation (1999)</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="http://localhost:1313/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="http://localhost:1313/about">About</a></li>
          <li><a href="http://localhost:1313/posts">Posts</a></li>
          <li><a href="http://localhost:1313/tags">Tags</a></li>
	  
	  <li><a href="http://localhost:1313/en/posts/policy-gradient-methods-for-reinforcement-learning-with-function-approximation/">en</a></li>
	  
        </ul>	
	<ul>
          
          <li>
            <a href="https://github.com/ryotaro612">
              <i class="fab fa-github"></i>
            </a>
          </li>
          
          
          <li>
            <a href="https://www.linkedin.com/in/ryotaro612/">
              <i class="fab fa-linkedin-in"></i>
            </a>
          </li>
          
          <li>
	    <a href="http://localhost:1313/index.xml">
	      <i class="fas fa-rss"></i>
            </a>
	  </li>
          
        </ul>	
      </nav>
    </header>
    
<main>
  <h1>Policy Gradient Methods for Reinforcement Learning With Function Approximation (1999)</h1>
  <ul class="tags">
    
    <li class="tag">
      <a href="http://localhost:1313/tags/%E6%96%B9%E7%AD%96%E5%8B%BE%E9%85%8D%E6%B3%95/">
	#方策勾配法
      </a>
    </li>
    
    <li class="tag">
      <a href="http://localhost:1313/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/">
	#強化学習
      </a>
    </li>
    
  </ul>  
  <span class="date">April 27, 2024</span>    
  <div>
    <p>方策勾配法は、パラメタつき方策モデルを設定し、報酬の期待値の勾配でパラメタを更新する。
行動価値関数から方策モデルを求めない点で、アクタークリティック法は、方策勾配法の一種とみなせる。
行動価値関数から方策を求める方法の多くは決定的な方策を求めるが、確率的に行動を選択する方策のほうが理想的な問題もある。
また、行動価値の微小な変化で、選択される行動が変わり、方策が収束しないことがある。
<a href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">Policy Gradient Methods for Reinforcement Learning With Function Approximation</a>は、方策勾配法において、関数近似器で近似された目的関数が局所最適解に収束する条件を示した。</p>
<p>エージェントがマルコフ決定過程とやりとりするとき、\(\theta\)を方策のパラメタ、各ステップごとの平均的な報酬のような目的関数を\(\rho\), 学習率を\(\alpha\)とおくと、方策勾配法は、勾配の比例する値\(\Delta \theta\)のパラメタへの加算をくりかえす。
$$
\Delta\theta \approx \alpha \frac{\partial \rho}{\partial \theta}
$$
状態数や行動数が多いときは目的関数を近似する必要があり、近似された目的関数が局所最適解に収束するための条件を示す。
まずは、2種類の目的関数を仮定し、その両方の偏微分の式を求める。</p>
<p>1つめの目的関数は、方策から報酬の平均値への関数である。
時刻\(t\)の報酬を\(r_t\), 行動を\(a\), 状態を\(s\), パラメタ\(\theta\)について偏微分できる方策を\(\pi(s, a, \theta) = \textit{Pr}\{a_t=a|s_t=s, \theta\}\), \(\mathcal{R}^a_s=E\{r_{t+1}|s_t=s, a_t = a\}\)として、目的関数を
$$
\rho(\pi)=\lim_{t\rightarrow\infty}\frac{1}{t}E\{r_1+r_2+\cdots+r_t|\pi\}=\sum_sd^\pi(s)\sum_a\pi(s, a)\mathcal{R}^a_s
$$
ただし、\(d^\pi(s)\)は定常分布 \(\lim_{t\rightarrow\infty}\textit{Pr}\{s_t=s|s_0, \pi\}\) とし、マルコフ決定過程はエルゴード性をみたすとする。
また、この目的関数に対する差分価値関数\(Q^\pi(s, a)\)を、\(\forall s\in \mathcal{S}, a\in \mathcal{A}\)として、
$$
Q^\pi(s, a)=\sum^\infty_{t=1}E\{r_t-\rho(\pi)|s_0=s, a_0 = a, \pi\}
$$
とおく。</p>
<p>もうひとつは、割引率\(\gamma\in[0, 1]\)に依存する時間割引ありの目的関数である。
$$
\begin{align*}
\rho(\pi) &amp;= E\left\{\sum^\infty_{t=1}\gamma^{t-1}r_t|s_0,\pi \right\}\\
Q^\pi(s, a) &amp;= E\left\{\sum^\infty_{k=1}\gamma^{k-1}r_{t+k}|s_t=s, a_t=a, \pi\right\}
\end{align*}
$$</p>
<p>任意のマルコフ決定過程において、どちらの目的関数の偏微分も同一の式で表すことができる。
証明は論文中でも付録にあるため省略するが、2つめの関数で\(d^\pi(s)=\sum^\infty_{t=0}\gamma^tPr\{s_t=s|s_0,\pi\}\)とすると、どちらの偏微分の式も以下のように書き表せる。
$$
\begin{equation}
\frac{\partial\rho}{\partial\theta}=\sum_sd^\pi(s)\sum_a\frac{\partial\pi(s, a)}{\partial\theta}Q^\pi(s, a)
\end{equation}
$$</p>
<p>一般的に、差分価値関数\(Q^\pi(s, a)\)は分からないので推定しなければならない。
ここでは、\(Q^\pi\)をパラメタ\(w\)の関数\(f_w: \mathcal{S}\times \mathcal{A}\rightarrow\mathbb{R}\) で推定する。
勾配法で\(w\)を更新する場合、\(w\)に以下の\(\Delta w\)を加算する。
$$
\Delta w_t\propto\frac{\partial}{\partial w}[\hat{Q}^\pi(s_t, a_t)-f_w(s_t, a_t)]^2\propto [\hat{Q}^\pi(s_t, a_t)-f_w(s_t, a_t)]\frac{\partial f_w(s_t, a_t)}{\partial w}
$$
\(\hat{Q}^\pi(s_t, a_t)\)は\(Q^\pi(s_t, a_t)\)の不偏推定量であり、たとえば\(R_t\)が該当する。
パラメタ\(w\)の更新をくりかえし、局所最適解に収束すると次の式がなりたつ。
$$
\begin{equation}
\sum_s d^\pi(s)\sum_a\pi(s, a)[Q^\pi(s, a)-f_w(s, a)]\frac{\partial f_w(s, a)}{\partial w} = 0
\end{equation}
$$</p>
<p>次に、\(Q\)のかわりにモデル\(f_w\)をもちいた式で目的関数の偏微分を表し、\(f_w\)をつかっても報酬を増えるように目的関数を更新できることを確認する。
まず、価値差分関数のモデルと方策モデルのパラメタに以下の(3)の関係がなりたつとする。
$$
\begin{equation}
\frac{\partial f_w(s, a)}{\partial w}=\frac{\partial \pi (s, a)}{\partial \theta}\frac{1}{\pi (s, a)}
\end{equation}
$$
さらに、\(f_w\)が局所最適解にあるとき、(2), (3)をあわせると
$$
\sum_s d^\pi(s)\sum_a\frac{\partial\pi(s, a)}{\partial \theta}[Q^\pi(s, a)-f_w(s, a)] = 0
$$
この値は\(0\)なので、(1)に加算すると
$$
\begin{align*}
\frac{\partial\rho}{\partial\theta}&amp;=\sum_sd^\pi(s)\sum_a\frac{\partial\pi(s, a)}{\partial\theta}Q^\pi(s, a) - \sum_s d^\pi(s)\sum_a\frac{\partial\pi(s, a)}{\partial \theta}[Q^\pi(s, a)-f_w(s, a)]\\
&amp;=\sum_sd^\pi(s)\sum_a\frac{\partial\pi(s, a)}{\partial\theta}[Q^\pi(s, a)-Q^\pi(s, a)+f_w(s, a)]
\end{align*}
$$
より
$$
\begin{equation}
\frac{\partial \rho}{\partial \theta}=\sum_s d^\pi(s)\sum_a\frac{\partial \pi(s, a)}{\partial \theta}f_w(s, a)
\end{equation}
$$
となる。</p>
<p>(3)にくわえて、\(\max_{\theta s, a, i, j|\frac{\partial^2 \pi(s, a)}{\partial \theta_i\partial \theta_j}|}&lt;B&lt;\infty\)と有界を設定すると、\(\frac{\partial^2\rho}{\partial \theta_i\partial\theta_j}\)も有界になり、学習率を\(\lim_{k\rightarrow\infty}\alpha_k=0, \sum_k\alpha_k=\infty\)とすれば、
$$
\begin{align*}
w_k&amp;= w\ \text{such that} \sum_s d^\pi(s)\sum_a\pi(s, a)[Q^\pi(s, a)-f_w(s, a)]\frac{\partial f_w(s, a)}{\partial w} = 0\\
\theta_{k+1}&amp;=\theta_k+\alpha_k\sum_sd^{\pi_k}(s)\sum_a\frac{\partial \pi_k(s, a)}{\partial\theta}f_{w_k}(s, a)
\end{align*}
$$
は\(\lim_{k\rightarrow\infty}\frac{\partial\rho(\pi_k)}{\partial\theta}=0\)のように収束する。</p>
  </div>
</main>

    
<ul class="pagination">
    
    <li>
      <a href="/posts/proximal-policy-optimization-algorithms/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/posts/distributed-graphlab-a-framework-for-machine-learning-and-data-mining-in-the-cloud/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    
</ul>


    <footer>
      <small>© Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
