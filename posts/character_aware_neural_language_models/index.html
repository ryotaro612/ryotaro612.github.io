<!DOCTYPE html>
<html>

<head>
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-64L0YEFRY5"></script>
    <script>
        if (location.hostname != 'localhost') {
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'G-64L0YEFRY5');
        }
    </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
        integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
        crossorigin="anonymous" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="/css/base.css">
    
<link rel="stylesheet" href="/css/single.css">
<link rel="stylesheet" href="/css/posts/single.css">
<link rel="stylesheet" href="/css/syntax.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/4e0e0c0a41.js" crossorigin="anonymous"></script>
    <title>論文メモ Character-Aware Neural Language Models</title>
</head>

<body>
    <header class="navigation">
        <h2><a href="/">Coda</a></h2>
        <nav>
            <ul>
                <li><a href="/about">About me</a></li>
                <li><a href="/posts">Posts</a></li>
                <li><a href="/gallery">Gallery</a></li>
            </ul>
        </nav>
    </header>
    
<main>
    <h1>論文メモ Character-Aware Neural Language Models</h1>
    <span class="date">April 4, 2020</span>
    <div>
        <h3 id="概要">概要</h3>
<p>文字単位の入力から次に出現する単語を予測するニューラル言語モデルの論文である。
アーキテクチャは入力から近い順にCNN, highway network, LSTMからなる。
実験データにPenn Treebankを、評価指標にPerplexityを採用してモデルを評価したところ、
論文が発表された2016年時点での<a href="https://arxiv.org/abs/1409.2329">SOTA</a>の60%程度のパラメタしかないモデルでありながら、これに匹敵する性能を発揮した。</p>
<h3 id="アーキテクチャ">アーキテクチャ</h3>
<p>モデルは、文字列を単位とする入力をうけつける。
モデルのアーキテクチャを図に示す。
入力に近い層から順にCNN, highway network, LSTMの順に説明していく。
<img src="/character_aware_neural_language_models.png" alt="img"></p>
<h4 id="cnn">CNN</h4>
<p>\(d\)を文字の分散表現の次元として、長さ\(l\)の単語\(k\)は\({\rm C}^{d\times l}\in \mathbb{R}^{d \times l}\)の分散表現としてモデルに入力される。
入力された分散表現に幅\(w\)のフィルタ\(\boldsymbol{\rm H}\in \mathbb{R}^{d \times w}\)を適用しnarrow conlovutionによって、特徴マップ\(\boldsymbol{\rm f}^k\in \mathbb{R}^{l-w+1}\)を求める。
\(\boldsymbol{\rm C}^k[*, i:i+w-1]\)を単語\(k\)の分散表現\(\boldsymbol{\rm C}^k\)の\(i\)から\(i+w-1\)までの列とすると、\(i\)番目の要素は次の式で計算される。</p>
<p>$$
\boldsymbol{\rm f}^k[i] = \tanh (\text{Tr}(\boldsymbol{\rm C}^k[*, i:i+w-1] \boldsymbol{\rm H}^T) + b)
$$</p>
<p>MAXプーリングにより各フィルタからスカラー値\(y^k\)を求める。アーキテクチャ全体ではフィルタを\([100, 1000]\)の範囲の複数のフィルタ\(\boldsymbol{\rm H}_1 ,\dots \boldsymbol{\rm H}_h\)をつかう。
このとき、単語\(k\)に対して\(\boldsymbol{y}^k=[y_1^k , \dots y_h^k]\)がhighway networkにわたされる。</p>
<p>$$
y^k=\max_i \boldsymbol{\rm f}^k[i]
$$</p>
<h4 id="highway-network">Highway Network</h4>
<p>CNNの出力をhighway networkに入力し、次の\(\boldsymbol{\rm z}\)を計算する。
$$
\boldsymbol{\rm z}=\boldsymbol{\rm t}\odot g(\boldsymbol{\rm W}_H \boldsymbol{\rm y} + \boldsymbol{\rm b}_H) + (\boldsymbol{1} - \boldsymbol{\rm t}\odot \boldsymbol{\rm y})
$$
ただし、\(\boldsymbol{\rm t}=\sigma (\boldsymbol{\rm W}_T\boldsymbol{\rm y} + \boldsymbol{\rm b}_T)\)であり、これをtransform gateという。
\(\boldsymbol{1}-\boldsymbol{\rm t}\)はcarry gateとよばれ、LSTMのメモリセルと同じく、入力を適応的にそのまま出力するはたらきがある。</p>
<h4 id="lstm">LSTM</h4>
<p>時刻\(t\)のhighway networkの出力\(\boldsymbol{\rm z}_t\in \mathbb{R}^n\)はLSTMに入力され、LSTMは次の計算により\(\boldsymbol{\rm h}_t\in \mathbb{R}^m\)を出力する。
複数のLSTMを重ねるときは、\(\boldsymbol{\rm h}_t\)を次のLSTMにわたす。</p>
<p>$$
\begin{align}
\boldsymbol{\rm i}_t &amp;= \sigma(\boldsymbol{\rm W}^i\boldsymbol{\rm z}_t + \boldsymbol{\rm U}^i\boldsymbol{\rm h}_{t-1}+\boldsymbol{\rm b}^i)\\<br>
\boldsymbol{\rm f}_t &amp;= \sigma(\boldsymbol{\rm W}^f\boldsymbol{\rm z}_t + \boldsymbol{\rm U}^f\boldsymbol{\rm h}_{t-1}+\boldsymbol{\rm b}^f)\\<br>
\boldsymbol{\rm o}_t &amp;= \sigma(\boldsymbol{\rm W}^o\boldsymbol{\rm z}_t + \boldsymbol{\rm U}^o\boldsymbol{\rm h}_{t-1}+\boldsymbol{\rm b}^o)\\<br>
\boldsymbol{\rm g}_t &amp;= \tanh(\boldsymbol{\rm W}^g\boldsymbol{\rm z}_t + \boldsymbol{\rm U}^g\boldsymbol{\rm h}_{t-1}+\boldsymbol{\rm b}^g)\\<br>
\boldsymbol{\rm c}_t &amp;= \boldsymbol{\rm f}_t \odot \boldsymbol{\rm c}_{t-1} + \boldsymbol{\rm i}_t \odot \boldsymbol{\rm g}_t \\<br>
\boldsymbol{\rm h}_t &amp;= \boldsymbol{\rm o}_t \odot \tanh (\boldsymbol{\rm c}_t)
\end{align}
$$</p>
<h4 id="出力層">出力層</h4>
<p>LSTMの出力\(\boldsymbol{\rm h}_t\)を全結合層とソフトマックス関数に入力し、単語列\(w_{1:t}=[w_1, \dots w_t]\)から次の単語\(w_{t+1}\)を予測する。</p>
<p>$$
\text{Pr}(w_{t+1}=j\mid w_{1:t}) = \frac{\exp (\boldsymbol{\rm h}_t \cdot \boldsymbol{\rm p}^j + q^j)}{\sum_{j'\in \mathcal{V}}\exp (\boldsymbol{\rm h}_t \cdot \boldsymbol{\rm p}^{j'}+ q^{j'})}
$$</p>
<p>\(\mathcal{V}\)は単語の集合をさす。
単語数\(\mid\mathcal{V}\mid\)が多い場合、階層的ソフトマックスをもちいる。
\(\mathcal{V}\)を\(c=\lceil\sqrt{\mid\mathcal{V}\mid} \rceil\)として\(\mathcal{V}_1,\dots \mathcal{V}_c\)に分割し、次のように予測する。</p>
<p>$$
\begin{align}
\text{Pr}(w_{t+1} = j\mid w_{1:t}) &amp;= \frac{\exp (\boldsymbol{\rm h}_t\cdot \boldsymbol{\rm s}^r + t^r)}{\sum^c_{r'=1}\exp (\boldsymbol{\rm h}_t \cdot \boldsymbol{\rm s}^{r'}+ t^{r'})} \\<br>
&amp;\times \frac{\exp (\boldsymbol{\rm h}_t\cdot \boldsymbol{\rm p}_r^j + q_r^j)}{\sum_{j'\in\mathcal{V}_r}\exp (\boldsymbol{\rm h}_t \cdot \boldsymbol{\rm p}_r^{j'}+ q_r^{j'})}
\end{align}
$$</p>
<h4 id="損失関数">損失関数</h4>
<p>損失関数にはnegative log-likelihood(NLL)をもちいる。</p>
<p>$$
NLL = - \sum_{t=1}^T \log \text{Pr}(w_t\mid w_{1:t-1})
$$</p>
<h3 id="備考">備考</h3>
<p>提案された言語モデルは、<a href="https://arxiv.org/abs/1802.05365">ELMo</a>で応用された。
以前、<a href="./deep_contextualized_word_representations/">こちら</a>にELMoの内容をまとめた。</p>
<h3 id="感想">感想</h3>
<p>初学者が勉強のために読むにはいい論文だと思った。
LSTM, CNN, 階層的ソフトマックス, Perplexityなどの広く応用されている手法を多く導入しているだけでなく、数式を含めた説明が十分にある。</p>
<hr>
<ul>
<li>論文は<a href="https://arxiv.org/abs/1508.06615">こちら</a>からダウンロードできます。</li>
<li>画像はすべて論文から引用されています。</li>
</ul>
    </div>
</main>

    
<ul class="pagination">
    
    <li>
        <a href="/posts/deep_contextualized_word_representations/"><span class="material-icons">chevron_left</span></a>
    </li>
    
    
    <li>
        <a href="/posts/domain_adversarial_training_of_neural_networks/"><span class="material-icons">chevron_right</span></a>
    </li>
    
</ul>


    <footer>
        <ul>
            
            <li>
                <a href="https://github.com/nryotaro">
                    <i class="fab fa-github"></i>
                </a>
            </li>
            
            
            <li>
                <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </li>
            
            <li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
            
        </ul>
        <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
</body>

</html>