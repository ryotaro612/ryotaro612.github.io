<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Model on Blanket</title>
    <link>https://ryotaro.dev/tags/large-language-model/</link>
    <description>Recent content in Large Language Model on Blanket</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 14 Mar 2024 14:41:45 +0900</lastBuildDate>
    <atom:link href="https://ryotaro.dev/tags/large-language-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLaMA Open and Efficient Foundation Language Models (2023)</title>
      <link>https://ryotaro.dev/posts/llama-open-and-efficient-foundation-language-models/</link>
      <pubDate>Thu, 14 Mar 2024 14:41:45 +0900</pubDate>
      <guid>https://ryotaro.dev/posts/llama-open-and-efficient-foundation-language-models/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2001.08361.pdf&#34;&gt;スケーリング則&lt;/a&gt;によれば、&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer&lt;/a&gt;型モデルのテストデータに対する交差エントロピーの損失は、のこり2つの要素がボトルネックにならないかぎり、パラメタ数、データ量、訓練時間とべき乗の関係にある。&#xA;パラメタをふやすほど損失を下げることができる。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15556&#34;&gt;Hoffmann et al. (2022)&lt;/a&gt; は、訓練に使えるFlop数が一定のときに、事前学習時の損失を効果的に下げるには、訓練時に参照するトークン数、パラメタ数を等しい比率でスケールすべきと結論づけた。&#xA;実験のために開発されたパラメタ数70BのモデルChinchillaはパラメタ数175BのGPT-3の性能を上回った。&lt;/p&gt;&#xA;&lt;p&gt;しかし、訓練時の損失を目標まで下げるまでにかかるFlop数が少ないモデルほど、推論時もプロセッサの利用効率がよいとはかぎらない。&#xA;たとえば、パラメタ数の大きいモデルが小さいモデルよりも高いプロセッサの利用効率で訓練時の損失を目標以下にできても、推論に必要なFlop数はパラメタ数が多いモデルのほうが大きい。&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</title>
      <link>https://ryotaro.dev/posts/lora-low-rank-adaptation-of-large-language-models/</link>
      <pubDate>Sat, 18 Nov 2023 12:21:20 -0500</pubDate>
      <guid>https://ryotaro.dev/posts/lora-low-rank-adaptation-of-large-language-models/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.08838.pdf&#34;&gt;Li et al. (2018)&lt;/a&gt;と&lt;a href=&#34;https://arxiv.org/pdf/2012.13255.pdf&#34;&gt;Aghajanyan et al. (2020)&lt;/a&gt;は、Large Language Models (LLM)のファインチューニングにかかる計算資源は大きいが、下流タスクに必要なパラメータ数はLLMのパラメタ数よりもはるかに少ないと主張した。&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;LoRA&lt;/a&gt;は、この仮説によりファインチューニングを避け、LLMの重みと線形結合すると下流タスクに最適な重みに近似できる小さな行列を求める。&#xA;LLMの重みを\(W_0\in\mathbb{R}^{d\times k}\)とすると、\(W_0+BA\ (B\in\mathbb{R}^{d\times r}, A\in\mathbb{R}^{r\times k}, r \ll\min (d, k))\)が下流タスクに最適な重みに近づくように、ファインチューニングにかわって\(W_0\)を更新せず\(B,\ A\)のみを更新する。&#xA;\(r\)が\(d, k\)よりも小さいので、ファインチューニングよりも学習時間は短い。&#xA;また、複数の下流タスクを入力に適用する場合、\(W_0x\)を共有できるので、推論に必要な計算も\(W_0\)を更新するファインチューニングより少ない。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
