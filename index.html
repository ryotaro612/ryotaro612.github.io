<!DOCTYPE html>
<html>
  <head>
	<meta name="generator" content="Hugo 0.118.2">
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.min.87b8f4076c47000c129b0c8b240a71216448e3aedd7144174c55776680a783b0.css">
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.ja.min.66816a64bc4ce50ec1c4b76199ca9d69163fc8a69f7abc6ea758d1968d8618b0.css">
    

<link rel="stylesheet" href="https://nryotaro.dev/scss/articles.min.ad090cad4f6d9fadf06a509c7ea790bd1fc13b24802f351ad71342659fdebf9f.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Blanket</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="https://nryotaro.dev/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="https://nryotaro.dev/about">About</a></li>
          <li><a href="https://nryotaro.dev/posts">Posts</a></li>
          <li><a href="https://nryotaro.dev/tags">Tags</a></li>
	  
	  <li><a href="https://nryotaro.dev/en/">en</a></li>
	  
        </ul>
      </nav>
    </header>
    
<main>
    <h1>Posts</h1>
    
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/gaussian_error_linear_units/">GAUSSIAN ERROR LINEAR UNITS(GELUs) 2016</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/abs/1606.08415">GAUSSIAN ERROR LINEAR UNITS(GELUs)</a>は、標準正規分布の累積分布関数を\(\Phi(x)\)とおくと、\(\text{GELU}(x)=x\Phi(x)\)で定義される活性化関数である。
GELUsは、Dropout, <a href="https://openreview.net/pdf?id=rJqBEPcxe">Zoneout</a>, \(\text{ReLU}(x)=\max(0, x)\)の性質を兼ね備える。
Zoneoutは、RNNむけの正則化であり、ユニットが一つ前の状態を確率的に維持するしくみである。
ReLUは、非線形性により、ニューラルネットワークを非線形関数に近似できる。
ZoneoutやDropoutは正則化の役割をはたす。
ReLUsの出力が入力に依存する一方で、Dropoutの出力は入力に依存しない。
GELUsは、確率\(\Phi(x)\)で1をとるベルヌーイ分布にしたがう0-1マスクを人工ニューロンへの入力に適用することで、非線形関数への近似と正則化の両方を実現する。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0/">
		#活性化関数
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 21, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/a_design_methodology_for_reliable_software_systems/">A design methodology for reliable software systems (1972)</a></h3>
        <div class="summary"><p>スタンフォードで数学の博士号を取得した<a href="https://computerhistory.org/profile/barbara-liskov/">Liskov</a>は以前勤めていたMitre Corporationに戻った後、最初にVenusとよばれるタイムシェアリングシステムの開発プロジェクトを担当し、その次にソフトウェア危機に対処する開発手法の研究に取り組んだ。
<a href="https://dl.acm.org/doi/pdf/10.1145/1479992.1480018">A design methodology for reliable software systems</a>は、Venusの開発から得られた構造化プログラミングの方法論である。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/structured-programming/">
		#Structured programming
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 8, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/abs/1910.13461">BART</a>は<a href="https://browse.arxiv.org/pdf/1706.03762.pdf">Transformer</a>をつかった系列変換モデルの事前学習である。
ノイズを入れたテキストからもとのテキストを復元できるようにモデルを訓練する。
BARTの特徴は、ノイズの作り方に制限がないところにある。
比較したノイズの作り方は、ランダムに選んだトークンから文書を始めることで回転する、BERTとおなじトークンのマスキング、トークンの一部の削除、文書中の文の順序の入れ換え、ある区間中にあるトークンをまとめて1つの<code>[MASK]</code>に置き換える方法の5種類である。
最後のトークンを1つのマスクキングするときに最もよい結果になった。
マスクに置き換える区間の長さは\(\lambda = 3\)のポアソン分布によって決まる。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/transformer/">
		#Transformer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 7, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/communicating_sequenctial_processes/">Communicating Sequential Processes (1978)</a></h3>
        <div class="summary"><p><a href="https://www.cs.cmu.edu/~crary/819-f09/Hoare78.pdf">Communicating Sequential Processes (CSP)</a>は、あるプロセスの出力を別のプロセスの入力に渡し、入出力のあるプロセスを並行実行するプログラミングモデルである。
たとえば、GoのgoroutinesはCSPに<a href="https://www.cs.princeton.edu/courses/archive/fall16/cos418/docs/P1-concurrency.pdf">もとづく</a>軽量スレッドである。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/communicating-sequential-processes/">
		#Communicating Sequential Processes
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/concurrent-programming/">
		#Concurrent Programming
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 30, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a>は、テキストをコサイン類似度で意味の類似度を比較できる固定長のベクトルに変換できる。
<a href="https://arxiv.org/abs/1810.04805">BERT</a>を2つのテキストを入力し類似度を出力するように訓練できるが、あるテキストに類似するテキストを求めたい場合には組合せ爆発が生じる。
Sentence-BERTは、類似度が定義されたテキストの組から、類似するテキスト同士を近い位置に写像できるように学習する。
ネットワークは、BERTを使ったシャムネットワークであり、重みを共有した2つのBERTに1つずつテキストを入力し、両方の出力を目的関数に入力する。</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted --></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/bert/">
		#BERT
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%82%B3%E3%82%B5%E3%82%A4%E3%83%B3%E9%A1%9E%E4%BC%BC%E5%BA%A6/">
		#コサイン類似度
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/transformer/">
		#Transformer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 23, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning(2008)</a></h3>
        <div class="summary"><p><a href="http://machinelearning.org/archive/icml2008/papers/391.pdf">A Unified Architecture for Natural Language Processing</a>は、複数のタスクの訓練データで重みの更新を繰り返すマルチタスクの深層学習である。
複数のタスクから順にあるタスクを選び、選んだタスクからランダムに取り出したサンプルで重みを更新するオンライン学習である。
ネットワークの層は、入力に近い方から、Word embedding、時間遅延ニューラルネットワーク(Time-Delay Neural Networks, TDNN)層、TDNN層の全時刻にわたる各ユニットの最大値を出力するMax Layer, 全結合層、ソフトマックスからなる。
Word embeddingのみタスク間で重みを共有し、後続の層の重みはタスクごとに異なる。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E3%83%9E%E3%83%AB%E3%83%81%E3%82%BF%E3%82%B9%E3%82%AF%E5%AD%A6%E7%BF%92/">
		#マルチタスク学習
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 16, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/transmission_of_information/">Transmission of Information(1927)</a></h3>
        <div class="summary"><p>Hartleyは、<a href="https://monoskop.org/images/a/a6/Hartley_Ralph_VL_1928_Transmission_of_Information.pdf">Transmission of Information</a>で、通信システムの情報伝達性能を評価するために、情報の定量的な尺度を提唱した。
シャノンが1948年に情報エントロピーを<a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">発表</a>する21年前である。
Hartleyは、通信内容の意味や解釈を捨象し、記号列の有限な候補からある記号列を選ぶことを情報みなした。
シャノンは、有限の候補から記号列を選択する考えを継承し、さらに、記号が順に確率的に選ばれるとみなすことでHartleyの情報量の定義を情報エントロピーに発展させた。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/%E6%83%85%E5%A0%B1%E9%87%8F/">
		#情報量
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 9, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/roberta/">RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>は、<a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>のアーキテクチャを変えずに、事前学習の改善によって性能を向上をはかる。
BERTの事前学習では、Masked Language ModelとNext Sentence Predictionの2つのタスクでパラメータを更新する。
一方、RoBERTaはMasked Language Modelのみを採用する。
さらに、Byte Pair Encodingのサブワードの単位を文字からバイトに変更し、語彙集合数が50,000に増えている。
ミニバッチのサイズもまたBERTより大きい。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/bert/">
		#BERT
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/byte-pair-encoding/">
		#Byte Pair Encoding
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">August 27, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/go_to_statement_considered_harmful/">Go To Statement Considered Harmful(1968)</a></h3>
        <div class="summary"><p><a href="https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf">Go To Statement Considered Harmful</a>でEdger Dijkstraは機械語以外でのGO TO文の使用を批判した。
ソースコードは機械語にコンパイルされ、プログラムとして実行される。
ソースコードは静的な成果物であるが、プログラムは時間とともに変化する。
プログラマにとって理解しやすいのは実行中のプログラムよりもソースコードの方なので、ソースコードからプログラムの実行状態を予測できることがのぞましい。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/early-programming/">
		#Early Programming
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">August 26, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/self_attention_with_relative_position_representations/">Self-Attention with Relative Position Representations(2018)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/pdf/1803.02155.pdf">Self-Attention with Relative Position Representations</a>は、<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Transformer</a>のQKV注意機構に系列の要素間の相対位置をあたえる手法である。
Transformerは、RNNやCNNとはちがい、要素の位置をモデルにあたえる決まった方法をもたない。
Transformerの原論文は、正弦関数で要素の位置からベクトルを計算し、それを埋め込みベクトルに加算することで、位置情報をエンコードする。
正弦関数を使う理由は、訓練データにない長さの系列に対する汎化性能を周期性で上げられると仮定したからである。
提案手法は、この仮定を継承し、要素間の相対位置で位置情報をエンコードすることで、訓練データにない系列長の入力に対する汎化性能の向上をはかる。</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/attention/">
		#Attention
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/tags/transformer/">
		#Transformer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">August 19, 2023</span>
        </div>
    </article>
    
</main>

    

<ul class="pagination">  
    
    
    <li>
      <a href="/page/2/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    <li>
      <a href="/page/29/">
	<i class="fa-solid fa-xl fa-angles-right"></i>
      </a>
    </li>
    
</ul>

    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/nryotaro">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="https://nryotaro.dev/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
