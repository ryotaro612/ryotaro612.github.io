<html>
  <head>
	<meta name="generator" content="Hugo 0.54.0" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155190626-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-155190626-1');
    </script>
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css"></link>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/skeleton/2.0.4/skeleton.css"></link>
    <link href="https://fonts.googleapis.com/css?family=Lato|M+PLUS+1p|M+PLUS+Rounded+1c|Roboto&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/css/base.css"></link>
    
  <link rel="stylesheet" type="text/css" href="/css/index.css"></link>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body class="container">
    <header>
      <h1><a href="/">Coda</a></h1>
      <nav>
	<ul>
	  <li><a href="/posts/">Posts</a></li>
	  <li><a href="/about/">About</a></li>
	</ul>
      </nav>
    </header>
    <main>
      
  
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/bleu/">論文メモ BLEU: a Method for Automatic Evaluation of Machine Translation</a></h2>
	  <p>June 13, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>自動翻訳を定量的に評価するための指標BLEUを提案した論文である。
指標は、専門家の翻訳に翻訳に高い評価をあたえるよう設計されている。
BLEUは、ひとつの候補訳に対する1つ以上の参照訳をあたえ、0から1の値をとるスコアを出力する。
スコアは高いほどよい。
BLEUは、参照訳にある単語を過剰に含むことや文の短さにペナルティをあたえ、適合率で候補訳を評価する。</p></div>
	<a class="readmore" href="/posts/bleu/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/sequence_to_sequence_learning_with_neural_networks/">論文メモ Sequence to Sequence Learning with Nueral Networks</a></h2>
	  <p>June 6, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>Sequence to Sequenceを発表した論文である。
RNNによって、入力と出力が系列のデータを学習する場合、入力と出力の長さが等しく、対応関係にある箇所が系列の方向に単調でなければならない。
この制約に対処するため、Sequence to Sequenceは入力全体を固定長のベクトルに一度変換し、そのベクトルをもとに出力を予測する。
Sequence to Sequenceは、2種類のLSTMをつかい、入力を与えたLSTMの最終層の隠れ状態からなる固定長ベクトルをつくる。
そして、このベクトルを、もう一方のLSTMに与え、最終的な出力を求める。
実験により、反転された入力系列をあとえると、入力と出力の対応関係にある箇所の距離が短くなり、予測性能が上がることが確認された。</p></div>
	<a class="readmore" href="/posts/sequence_to_sequence_learning_with_neural_networks/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/google_neural_machine_translation_system/">論文メモ Google&#39;s Neural Machine Transltation System: Bridging the Gap between Human and Machine Translation</a></h2>
	  <p>May 30, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>ニューラルネットワークをもちいた機械翻訳システムの論文である。
解決したい問題として、学習と推論時の処理時間の長さ、低頻出の単語を翻訳する難しさ、入力文の一部が翻訳されないことをあげ、注意機構でつながれたEncoderとDecoderからなるアーキテクチャを提案した。
学習時間を短縮するために、Decoderの最初の層とEncodeerの出力層から注意をつくる注意機構を採用し、Decoderを並列に学習できるようにしている。
また、量子化によって推論時間を短縮をしている。
低頻出の単語でも翻訳できるようにwordpieceでエンコードされた入力をうけとる。
入力文の一部が翻訳されない問題に対しては、短い出力文に罰則を課すビームサーチで出力文の候補を探索する仕組みが導入されている。</p></div>
	<a class="readmore" href="/posts/google_neural_machine_translation_system/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/zero_shot_learning_with_semantic_output_codes/">論文メモ Zero-Shot Learning with Semantic Output Codes</a></h2>
	  <p>May 23, 2020</p>
	</header>
	<div class="excerpt"><p>学習データにないラベルを推定できるようにモデルを学習する問題に対してzero-shot leanringという名をあたえ、ラベルを推定できる確率と条件を形式化した論文である。
形式化するモデルは、複数の二値分類器と1つの最近傍探索器からなる。
最近傍探索は、2値分類器の出力を要素とするベクトルをうけとり、最近傍のラベルに対応するベクトルを探す。
PACフレームワークにもとづく必要な学習データの件数を示し、そのデータで訓練されたモデルが学習データにないラベルを推定できる確率を示した。</p></div>
	<a class="readmore" href="/posts/zero_shot_learning_with_semantic_output_codes/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/a_structured_self_attentivesentence_embedding/">論文メモ A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a></h2>
	  <p>May 16, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>自己注意機構をもちいて、可変長の文を埋め込み行列に変換するアーキテクチャを発表した論文である。
埋め込み行列の各行は、それぞれ文中の異なる箇所の意味を反映する。
アーキテクチャは2つの構成からなり、入力から出力にむかい双方向LSTMを、次に自己注意機構をもつ。
自己注意機構を導入した背景は、回帰結合型のネットワークでは、全ての時刻わたって入力の意味を保持することは難しく、また不要であるという著者らの仮説である。
3つの実験により、文の分散表現を獲得する先行研究と比較し、自己注意機構の効果が確認された。
注意機構は複数のベクトルのどれを重視するかを学習できるため、埋め込まれた文の箇所を可視化できることも示した。</p></div>
	<a class="readmore" href="/posts/a_structured_self_attentivesentence_embedding/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/poincare_embeddings/">論文メモ Poincaré Embeddings for Learning Hierarchical Representations</a></h2>
	  <p>May 9, 2020</p>
	</header>
	<div class="excerpt">概要 単語のように上位下位関係のある記号を、ポアンカレ球体模型という双曲空間に埋め込む手法を発表した論文である。 ユークリッド空間よりも、記号間の類似度や上位下位関係が保たれていることを実験的に示した。 記号を木のノードとして配置し関係を表現するとき、ノード数は深さ\(l\)対して指数関数的に増加する。 双曲幾何学では、円板の面積や周は半径\(r\)に対して指数関数的に増大するため、木を2次元でモデル化できる。 たとえば、深さ\(l\)以下のノードを半径\(r \varpropto l \)の空間に配置することができる。 一方、2次元のユークリッド空間の場合、半径\(r\)に対する円周は線形、円の面積は2次関数的であるため、モデル化が難しい。 実験では、次元数が少ないほど、ポアンカレ球体模型とユークリッド空間の間で、上下関係や類似度の表現力に差があった。
損失関数 埋め込みたい上下関係\(\mathcal{D}=\{(u, v)\}\)を記号の数を\(n\)として入力すると、アルゴリズムは、埋め込みベクトルの集合\({\rm \Theta}=\{\boldsymbol{\theta}_i\}^n_{i=1}\)を出力する。 ただし、\(\boldsymbol{\theta}\in \mathcal{B}^d\), \(\mathcal{B}^d=\{\boldsymbol{x}\in \mathbb{R}^d\mid ||\boldsymbol{x}||&lt;1\}\)とする。 学習では、次の損失関数\(\mathcal{L}(\Theta)\)をもちいる。
$$ \mathcal{L}(\Theta)=\sum_{(u, v)\in \mathcal{D}}\log\frac{e^{-d(\boldsymbol{u}, \boldsymbol{v})}}{\sum_{\boldsymbol{v}&rsquo;\in \mathcal{N}(u)}e^{-d(\boldsymbol{u}, \boldsymbol{v}&lsquo;)}} $$ \(\mathcal{N}(u)=\{v&rsquo;\mid (u, v&rsquo;)\notin \mathcal{D}\} \cup \{v\}\)は\(v\)を含んだ\(u\)に対する負例である。 実験では、正例に対して10の負例をサンプリングしていた。 \(d\)は、\(\boldsymbol{u}, \boldsymbol{v}\in \mathcal{B}^d\)の距離であり、次の式であたえらえる。
$$ d(\boldsymbol{u}, \boldsymbol{v}) = \mathrm{arccosh}\left(1+2\frac{||\boldsymbol{u}-\boldsymbol{v}||^2}{(1-||\boldsymbol{u}||^2)(1-||\boldsymbol{v}||^2)}\right) $$
最適化 RSGDやRSVRGで損失関数の値を最小化する埋め込みベクトルを探す。 ここでは、RSGDについて説明する。 RSGDでは、次のパラメタの更新式をとる。
$$ \boldsymbol{\theta}_{t+1} = \mathfrak{R}_{\theta_t}(-\eta_t\nabla_R\mathcal{L}(\boldsymbol{\theta}_t)) $$ \(\mathfrak{R}_{\theta_t}\)はレトラクションで、ここでは\(\mathfrak{R}_\theta(\boldsymbol{v})=\boldsymbol{\theta}+\boldsymbol{v}\)をもちいる。 \(\eta_t\)は時刻\(t\)の学習率をさす。 \(\nabla_R\)はリーマン多様体上の勾配であり、ユークリッド空間上の勾配\(\nabla_E\)とは
$$ \nabla_R = \frac{(1-||\boldsymbol{\theta_t}||^2)^2}{4}\nabla_E $$ の関係がある。 以上より、更新式は
$$ \mathrm{proj}(\boldsymbol{\theta})= \begin{cases} \boldsymbol{\theta}/||\boldsymbol{\theta}|| - \epsilon &amp;\mathrm{if}\ ||\boldsymbol{\theta}||\ge 1 \</div>
	<a class="readmore" href="/posts/poincare_embeddings/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/learning_joint_multilingual_sentence_representations_with_neural_machine_translation/">論文 メモ Learning Joint Multilingual Sentence Representations with Neural Machine Translation</a></h2>
	  <p>April 29, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>多言語の文をあつかう分散表現モデルを発表した論文である。
異なる言語の文であっても、意味が同じであれば、同様の分散表現に変換される。
モデルのアーキテクチャにはseq2seqを、入力と出力には対訳コーパスをつかう。
ミニバッチごとに、入力または出力の言語をいれかえ、言語に依存しない文の意味の分散表現への変換方法を学習する。
本論文の成果は多言語に対応する分散表現のモデルのライブラリ<a href="https://github.com/facebookresearch/LASER">LASER</a>に応用されている。</p></div>
	<a class="readmore" href="/posts/learning_joint_multilingual_sentence_representations_with_neural_machine_translation/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/albert/">論文メモ ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></h2>
	  <p>April 25, 2020</p>
	</header>
	<div class="excerpt"><p>BERTのパラメタ数を削減し、学習時間の短縮と正則化による予測性能の向上を両立したモデルALBERTを提案し、GLUE, RACE, SQuADでSoTAを実現した。
BERT-largeと比べると、ALBERT-largeのパラメタ数は約5.3%の18Mであり、学習時間は1.7倍速い。
パラメタを削減するために、単語のOne-hotベクトルをあたえられる単語埋め込み行列の次元を減らし、隠れ層の順伝播ネットワークや注意機構のパラメタを層の間で共有した。
また、Next Sentence Prediction(NSP)による学習を、与えられた2文の前後関係を判定する学習Sentence Order Prediction(SOP)におきかえ、主タスクの予測性能を向上をはかった。</p></div>
	<a class="readmore" href="/posts/albert/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/random_walks_in_recommender_systems/">論文メモ Random Walks in recommender Systems: Exact Computation and Simulations</a></h2>
	  <p>April 18, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p><a href="https://ieeexplore.ieee.org/document/4072747">F. Foussら</a>や<a href="https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-444.pdf">M. Goriら</a>のランダムウォークによる推薦システムの先行研究を、質や計算量について比較した論文である。
比較対象には、著者らの用意したも含まれる。
実験には、MovieLensのデータセットが使われた。
F. Foussらの実験で使われた評価指標や上位kの推薦結果のヒット数で評価したところ、著者らの用意した単純な手法\(P^s\)やその拡張\(P_\alpha^s\)が質と計算量の両方で最も優れた結果を残した。</p></div>
	<a class="readmore" href="/posts/random_walks_in_recommender_systems/">Read more</a>
      </article>
    
  
    
      <article class="summary">
	<header>
	  <h2><a href="/posts/domain_adversarial_training_of_neural_networks/">論文メモ Domain Adversarial Training of Neural Networks</a></h2>
	  <p>April 11, 2020</p>
	</header>
	<div class="excerpt"><h3 id="概要">概要</h3>

<p>ニューラルネットワークをもちいたドメイン適用の論文である。
ソースドメインのラベルつきデータと目標ドメインのラベルのないデータでモデルを訓練し、目標ドメインに対する分類性能を引きあげる。
目的関数は、ソースドメインの分類器の目的関数とデータのドメインを判定する識別器の目的関数からなる。
後者は、前者の正則化項としてはたらく。
これにより、ドメイン間に共通する特徴からソースドメインのデータのラベルを高い性能で予測できるようになる。
目標関数から、ドメイン間のデータの分布が近いほど、目標ドメインのデータでも高い分類性能を発揮する。
先行研究との違いは、できるだけ共通するする特徴で分類するという着想を、通常の分類と同じく、確率的勾配降下法で実現したところにある。</p></div>
	<a class="readmore" href="/posts/domain_adversarial_training_of_neural_networks/">Read more</a>
      </article>
    
  
  <div class="pagination">
  
    <a><i class="material-icons">first_page</i></a>
  
  
    <a><i class="material-icons">chevron_left</i></a>
  
  
    <a href="/page/2/"><i class="material-icons">chevron_right</i></a>
  
  
    <a href="/page/8/"><i class="material-icons">last_page</i></a>
  
</div>


    </main>
    <footer>
      <ul>
       
       <li>
	 <a href="https://github.com/nryotaro"><i class="fab fa-github"></i></a>
       </li>
       
       
       <li>
	 <a href="https://www.linkedin.com/in/nakamura-ryotaro">
	   <i class="fab fa-linkedin"></i>
	 </a>
       
       </li>
	<li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
      </ul>
      <small>&copy; Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>
</html>
