<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    <title>
      
      論文メモ A Dual Embedding Space Model for Document Ranking - Coda
      
		</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link href="https://fonts.googleapis.com/css?family=M+PLUS+Rounded+1c" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/assets/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/icons.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/screen.css" />
    
    <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Lato:100,100i,300,300i,400,400i,700,700i|Source+Code+Pro:300,400,500,700" rel="stylesheet">
    

    
    <script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/bigfoot/dist/bigfoot.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/bigfoot/dist/bigfoot-number.css" />
    <script type="text/javascript">
        $.bigfoot();
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    
    
</head>

    <body class="post-template">
        <header class="main-header">
	<div class="main-header-content">
		<h1 class="blog-title">
			<a href="/">
				
           Coda
				
			</a>
		</h1>
		<h2 class="blog-description"></h2>
	</div>

	<div class="nav">
    
		
	</div>
</header>

        
<main class="content" role="main">
  <article class="post">
    <header class="post-header">
      
      <h2 class="post-title">論文メモ A Dual Embedding Space Model for Document Ranking</h2>
      <section class="post-meta">
        <time class="post-date">November 30, 2019</time>
      </section>
    </header>
    <section class="post-content">
      <p>Dual Embedding Space Model(DESM)は、word2vecで単語埋め込みベクトルにしたクエリと文書のランキング関数である。
実験における比較対象のBM25は、クエリの単語の文書での出現頻度をもとに順序をつける。
DESMは、クエリの単語に関係する単語をもとに判断する。
単語埋め込みベクトルの作りに特徴があり、入力側のone-hotベクトル表現にわりあてる単語埋め込みベクトルでクエリを、出力側でベクトルで文書を分散表現にする。
実験から、DESMだけで順位づけをすると偽陽性が高くなるが、DESMとBM25の加重平均をとるとBM25よりも高いNDCG値になることが分かった。</p>

<p>Continuous Bag-of-Words(CBOW)は、ウィンドウ内の単語を与えられたときの中心の単語の生起確率を学習する。
コーパスを\({\mathcal D}\), 埋め込みベクトルの次元を\(d\), 大きさ\(K-1\)のウィンドウにある単語の埋め込みベクトルを\(\boldsymbol{c}_k \in \mathbb{R}^d \), 中心の単語の\(w_i\)の埋め込みベクトルを\(\boldsymbol{w}_i \in \mathbb{R}^d\)とおくと、損失関数\({\mathcal L_{CBOW}}\)が最小値になる埋め込みベクトルを学習する。
$$
\begin{align}
{\mathcal L_{CBOW}} &amp;= \sum^{\mid D\mid}_{i=1}-\log p(w_i\mid C_K) \\<br />
&amp;= \sum^{\mid D \mid}_{i=1}-\log\frac{e^{\bar{C_k^T}\boldsymbol{w}_i}}{\sum^{V_w}_{v=1}e^{\bar{C}^T_K\boldsymbol{w}_v}}
\end{align}
$$
$$
\bar{C}_K = \frac{1}{K-1}\sum_{i-k\le k \le i~K, k\neq i} \boldsymbol{c}_k
$$
とする。
\(\boldsymbol{c}\)からなる重み行列を\(\boldsymbol{W}_{IN}\), \(\boldsymbol{w}\)からなる重み行列を\(\boldsymbol{W}_{OUT}\)と表記する。</p>

<p>通常では、\(\boldsymbol{W}_{IN}\)だけで単語埋め込みベクトルを作るが、DESMは、クエリと文書にそれぞれ\(\boldsymbol{W}_{IN}\)と\(\boldsymbol{W}_{OUT}\)をあてがう。
以下の表は、Bingのクエリ2,748,230単語で学習したyaleのベクトルにコサイン類似度で最も近いベクトルの単語を並べている。
<img src="/desm_tab1.png" alt="table1" />
IN-IN同士のベクトルでは「大学」という同じ機能をもった単語同士が近くに配置されている。
論文の洞察は、このような単語の存在をクエリと文書の関連度の手がかりにすることにある。</p>

<p>\(q_i\)をクエリ\(Q\)に含まれる単語埋め込みベクトル、\(d_j\)を文書\(D\)に含まれる単語埋め込みベクトルとすると、
\(Q\)と\(D\)のDESMの値は以下の式で定義される。</p>

<p>$$
DESM(Q, D) = \frac{1}{\mid Q\mid}\sum_{\boldsymbol{q}_i\in Q}\frac{\boldsymbol{q}_i^T\bar{\boldsymbol{D}}}{\mid\mid\boldsymbol{q_i}\mid\mid\mid\mid \bar{D}\mid\mid}
$$
$$
\bar{D}= \frac{1}{\mid D\mid}\sum_{\boldsymbol{d}_j \in D}\frac{\boldsymbol{d}_j}{\mid\mid \boldsymbol{d}_j\mid\mid}
$$</p>

<p>実験では、DESMは偽陽性率が高いため、BM25との加重平均\(MM(Q, D)\)を評価している。
$$
MM(Q, D) = \alpha DESM(Q, D) + (1-\alpha)BM25(Q, D)
$$
$$
\alpha \in \mathbb{R}, 0\le\alpha\le 1
$$
<!--
実験によると、IN-OUTベクトルは
IN-OUT間のベクトルのコサイン類似度を測ると、異なる機能をもち関連する単語が近接する
IN-OUT間のベクトルのコサイン類似度を測ると異なる機能をもち関連する単語が近接するという実験結果が示されている。
--></p>

<!--
順位づけにおいて、クエリにある単語が文書の主題にあたるか単に参照されているだけなのかを判別しなければならない。
クエリにある単語が、文書において主題であるか単に参照されているだけなのかを判別する必要がある。
区別する必要がある。

DESMとBM25の加重平均で算出されたスコアをもちいるとよい。

BM25ではスコアの低い正例の文書に対してDESMでは高いスコアで順位付けられることが実験的に確認された。
DESMで上位にくる文書単語の比較ではマッチングに失敗する文書に高
-->

<!-- 問題となるマッチングとは何か  -->

<!--
入力側と出力側の単語のone-hotベクトル表現に割り当てる単語埋め込みベクトルをどちらも使用する。
入力側でクエリを、出力側で文書の分散表現をつくる。
-->

<hr />

<p>論文は<a href="https://arxiv.org/abs/1602.01137">こちら</a>からダウンロードできます。</p>
    </section>
    <footer class="post-footer">
      
    </footer>
  </article>
</main>

        <footer class="site-footer">
  <section class="rss"><a class="subscribe-button icon-feed" href="/index.xml"></a></section>
  
  
  <section>
    <a class="fas fa-rss" href="/index.xml"></a>
    <a class="fab fa-github" href="https://github.com/nryotaro"></a>
    <a class="fab fa-linkedin" href="https://www.linkedin.com/in/nakamura-ryotaro"></a>
  </section>
  <section class="copyright">&copy; 2019 Nakamura, Ryotaro</section>
</footer>



    </body>
</html>
