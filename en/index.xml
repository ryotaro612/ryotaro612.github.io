<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blanket</title>
    <link>https://nryotaro.dev/en/</link>
    <description>Recent content on Blanket</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 08 Oct 2023 09:38:50 -0400</lastBuildDate><atom:link href="https://nryotaro.dev/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A design methodology for reliable software systems (1972)</title>
      <link>https://nryotaro.dev/en/posts/a_design_methodology_for_reliable_software_systems/</link>
      <pubDate>Sun, 08 Oct 2023 09:38:50 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/a_design_methodology_for_reliable_software_systems/</guid>
      <description>After earning her PhD in computer Science at Stanford University, Liskov worked again for MITRE Corporation. She was involved in the development of a time-sharing system called Venus, then was involved in finding ways to address the &amp;ldquo;software crisis.&amp;rdquo; at MITRE. A design methodology for reliable software systems describes a design methodology of structured programming developed as part of the second project.
The methodology uses testing to guarantee reliability. To test a program, it is necessary to identify relevant test cases, and the set of them must be small enough to implement.</description>
    </item>
    
    <item>
      <title>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</title>
      <link>https://nryotaro.dev/en/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/</link>
      <pubDate>Sat, 07 Oct 2023 10:28:13 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.13461&#34;&gt;BART&lt;/a&gt; is a denoising autoencoder for Pretraining sequence-to-sequence models.
BART is trained on corrupted text, and updates the parameters to reconstruct the original text.
The authors experimented with several noising functions that corrupt text like token masking, token deletion, text infilling, sentence permutation, and document rotation.
BART with text infilling, where text spans are sampled with span lengths drawn from a Poisson distribution(\(\lambda = 3\)), demonstrated the most consistently strong performance.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Communicating Sequential Processes (1978)</title>
      <link>https://nryotaro.dev/en/posts/communicating_sequenctial_processes/</link>
      <pubDate>Sat, 30 Sep 2023 11:34:47 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/communicating_sequenctial_processes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~crary/819-f09/Hoare78.pdf&#34;&gt;Communicating Sequential Processes (CSP)&lt;/a&gt; is a program structuring method that constructs a program as a parallel composition of a fixed number of sequential processes.
A process is a sequence of commands.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks(2019)</title>
      <link>https://nryotaro.dev/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</link>
      <pubDate>Sat, 23 Sep 2023 10:21:03 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.10084&#34;&gt;Sentence-BERT&lt;/a&gt; derives semantically meaningful sentence embedding that can be compared using cosine-similarity.
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; achieved new state-of-the art performance on various sentence-pair regression tasks using a cross-encoder.
A cross-encoder accepts two sentences as input to the transformer network and the target value is predicted.
Semantic textual similarity is one of the sentence-pair regression tasks.
However, this setup is often not scalable for various pair regression tasks due to many possible combinations.
The semantic search that maps each sentence to a vector space where semantically similar sentences are close alleviates the combinatorial explosion.
Sentence-BERT uses a siamese network in which the two BERT networks have tied weights such that the produced sentence embeddings can be semantically compared using cosine-similarity.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning(2008)</title>
      <link>https://nryotaro.dev/en/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/</link>
      <pubDate>Sat, 16 Sep 2023 08:59:45 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://machinelearning.org/archive/icml2008/papers/391.pdf&#34;&gt;A Unified Architecture for Natural Language Processing&lt;/a&gt; is an instance of multitask learning.
The first layer is a lookup table that stores embeddings of a fixed dictionary and size.
The second layer is a &lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf&#34;&gt;Time-Delay Neural Networks&lt;/a&gt; layer.
It extracts features from the sentence treating it as a sequence with local structure.
The third layer takes the maximum value for each of the output features of the second layer over time.
The following layers are classical NN layers.
The lookup-table is shared among the tasks, and the other layers can be task specific to each task.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Transmission of Information(1927)</title>
      <link>https://nryotaro.dev/en/posts/transmission_of_information/</link>
      <pubDate>Sat, 09 Sep 2023 12:10:45 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/transmission_of_information/</guid>
      <description>In Transmission of Information, Hartley developed a quantitative measure of &amp;ldquo;information&amp;rdquo; in 1927. Hartley claims that information is the outcome of a selection among a finite set of possible messages. Shannon&amp;rsquo;s &amp;ldquo;A mathematical theory of communication&amp;rdquo;, which is based in part on Hartley&amp;rsquo;s ideas, published in 1947. Hartley did not model the source of information probabilistically. Shannon modeled the source of information as a random process.
Hartley stated that information should be proportional to the number of selections for practical engineering value.</description>
    </item>
    
    <item>
      <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</title>
      <link>https://nryotaro.dev/en/posts/roberta/</link>
      <pubDate>Sun, 27 Aug 2023 13:13:39 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/roberta/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.11692.pdf&#34;&gt;RoBERTa&lt;/a&gt;(Robusty optimized BERT approach) is an improved recipe for training &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt; models.
BERT uses two objectives, masked language modeling and next sequence prediction (NSP), during pretraining.
RoBERTa uses masked language modeling only.
The authors increased the batch size and Byte-Pair Encoding (BPE) vocabulary size.
RobERTa is trained with byte-level BPE, which uses bytes instead of unicode characters as the base subword units.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Go To Statement Considered Harmful(1968)</title>
      <link>https://nryotaro.dev/en/posts/go_to_statement_considered_harmful/</link>
      <pubDate>Sat, 26 Aug 2023 10:46:18 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/go_to_statement_considered_harmful/</guid>
      <description>&lt;p&gt;Edgar Djkstra criticized the excessive use of the go to statement in &lt;a href=&#34;https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf&#34;&gt;Go To Statement Considered Harmful&lt;/a&gt;.
While the source code is static, the process taking place under the control of the source code is dynamic.
The programmers should aim to shorten the conceptual gap between the source code and its process to describe the progress.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Attention with Relative Position Representations(2018)</title>
      <link>https://nryotaro.dev/en/posts/self_attention_with_relative_position_representations/</link>
      <pubDate>Sat, 19 Aug 2023 10:02:50 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/self_attention_with_relative_position_representations/</guid>
      <description>&lt;p&gt;The authors of &lt;a href=&#34;https://arxiv.org/pdf/1803.02155.pdf&#34;&gt;Self-Attention with Relative Position Representations&lt;/a&gt; presented a way of injecting relative position representations in the self-attention mechanism of the &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Transformer&lt;/a&gt;.
In contrast to recurrent and convolutional neural networks, Transformer does not explicitly model position information in its structure.
&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;The original position encoding&lt;/a&gt; employs sine and cosine functions of different frequencies.
The authors of Transformer hypothesized that sinusoidal position encodings would help Transformer to generalize to sequence lengths unseen during training.
Positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks of Transformer.
This hypothesis was shared by the relative position representations.
In contrast to absolute position representations, the relative position representations are invariant to the total sequence length.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self-Adjusting Binary Search Trees(1985)</title>
      <link>https://nryotaro.dev/en/posts/self_adjusting_binary_search_trees/</link>
      <pubDate>Sat, 12 Aug 2023 13:49:56 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/self_adjusting_binary_search_trees/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~sleator/papers/self-adjusting.pdf&#34;&gt;The splay tree&lt;/a&gt;, a self-adjusting form of a binary search tree, is a binary search tree that moves an accessed node to the root after each access.
On an \(n\)-node splay tree, accessing, inserting and deleting have an amortized time bound of \(O(\log n)\) per operation.
In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(2020)</title>
      <link>https://nryotaro.dev/en/posts/exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer/</link>
      <pubDate>Sat, 05 Aug 2023 13:27:09 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/exploring_the_limits_of_transfer_learning_with_a_unified_text-to-text_transformer/</guid>
      <description>The authors of Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer performed experiments with Text-to-Text Transfer Transformer(T5), a unified framework for NLP. The basic idea underlying T5 is to treat various NLP problems as taking text as input and producing new text as output. Their goal is to explore general language learning abilities instead of providing new methods. They are interested in exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.</description>
    </item>
    
    <item>
      <title>Catalan Numbers(2016)</title>
      <link>https://nryotaro.dev/en/posts/catalan_number/</link>
      <pubDate>Sat, 29 Jul 2023 15:24:15 -0400</pubDate>
      
      <guid>https://nryotaro.dev/en/posts/catalan_number/</guid>
      <description>The Catalan numbers are a sequence of natural numbers that occur in various problems in combinatorial mathematics. For example, the number of expressions containing \(n\) valid pairs of parentheses is the \(n\)th Catalan number. Suppose you have a grid of \(n \times n\) squares, the \(n\)th Catalan number represents the number of paths with a length of \(2n\) that lead from the upper left corner to the lower right corner without intersecting the diagonal dotted line running from the upper left to the lower right.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://nryotaro.dev/en/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nryotaro.dev/en/about/</guid>
      <description>Disclaimer All the information on this website is published in good faith and for general information purpose only. This website does not make any warranties about the completeness, reliability and accuracy of this information. Any action you take upon the information you find on this website, is strictly at your own risk. Ryotaro Nakamura will not be liable for any losses and/or damages in connection with the use of this website.</description>
    </item>
    
  </channel>
</rss>
