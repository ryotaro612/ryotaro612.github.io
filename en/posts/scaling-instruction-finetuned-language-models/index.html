<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
  
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W5TDG76');
  </script>
  
  
  <meta charset="utf-8">
  <meta name="viewport" content="width=300,initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
    integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
    crossorigin="anonymous" />
  
  <link rel="stylesheet" href="http://localhost:1313/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
  
  <link rel="stylesheet" href="http://localhost:1313/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
  

<link rel="stylesheet" href="http://localhost:1313/scss/single.min.8ed5cbfb3dc516e9c4459a65252e4e6e8e9026aabfa9eb9f602cf53b30754966.css">

<link rel="stylesheet" href="http://localhost:1313/scss/posts/single.min.0008713f3f7556e78c13cb8efde9cf534239cb206c10ba85ce5a76e6ae9ce758.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  <title>Scaling Instruction Finetuned Language Models (2022)</title>
</head>

<body>
  
  
  <noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0"
      style="display:none;visibility:hidden" />
  </noscript>
  
  
  <header class="navigation">
    <h2><a href="http://localhost:1313/en/">Blanket</a></h2>
    <nav>
      <ul>
        <li><a href="http://localhost:1313/en/about">About</a></li>
        <li><a href="http://localhost:1313/en/posts">Posts</a></li>
        <li><a href="http://localhost:1313/en/tags">Tags</a></li>
        
        <li><a href="http://localhost:1313/posts/scaling-instruction-finetuned-language-models/">ja</a></li>
        
      </ul>
      <ul>
        
        <li>
          <a href="https://github.com/ryotaro612">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/ryotaro612/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://speakerdeck.com/ryotaro612/">
            <i class="fa-brands fa-speaker-deck"></i>
          </a>
        </li>
        
        <li>
          <a href="http://localhost:1313/en/%20index.xml">
            <i class="fas fa-rss"></i>
          </a>
        </li>
        
      </ul>
    </nav>
  </header>
  
<main>
  <h1>Scaling Instruction Finetuned Language Models (2022)</h1>
  <ul class="tags">
    
    <li class="tag">
      <a href="http://localhost:1313/en/tags/large-language-models/">
        #Large Language Models
      </a>
    </li>
    
    <li class="tag">
      <a href="http://localhost:1313/en/tags/instruction-finetuning/">
        #Instruction Finetuning
      </a>
    </li>
    
  </ul>
  <span class="date">
    
    May 7, 2025
    
  </span>
  <div>
    <p>Instruction finetuning is a technique for enhancing the zero-shot performance of large language models (LLMs). The seminal work, <a href="https://arxiv.org/pdf/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a>, refers to this method as instruction tuning.
Building on this, <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a> explored scaling the number of tasks, model sizes, and the incorporation of chain-of-thought (CoT) data. Their findings demonstrate that instruction finetuning can significantly improve LLM performance across a wide range of settings.</p>
<p>Instruction finetuning is grounded in the insight that many NLP tasks can be framed as natural language instructions—examples include translation, sentiment analysis, and question answering. Notably, the original paper introduced instruction tuning through examples rather than formal definitions. Two representative examples in the paper are:</p>
<blockquote>
<p>INPUT<br>
Jane knocked on Susan’s door, but there was no answer.<br>
OPTIONS:<br>
Jane was out.<br>
Susan was out.<br>
TARGET<br>
Susan was out.</p></blockquote>
<blockquote>
<p>INPUT<br>
Question: who is the girl in more than you know??<br>
Answer:<br>
TARGET<br>
Romi Van Renterghe</p></blockquote>
<p>In their work, the authors reformatted datasets into instruction-style prompts and trained LLMs to evaluate the impact of this finetuning strategy under zero-shot conditions. Models trained in this manner were referred to as FLAN (Finetuned Language Net).</p>
<p>The <em>Scaling Instruction-Finetuned Language Models</em> paper extended this line of research by focusing on three main areas: scaling the number of instruction tasks, scaling the size of the models, incorporating chain-of-thought annotations into training data.</p>
<p>Their experiments utilized <a href="https://arxiv.org/pdf/2204.02311">PaLM</a> models with 8B, 62B, and 540B parameters, trained on progressively larger sets of tasks—specifically, 0, 9, 89, 282, and 1,836 tasks.
Here, a task is defined as a specific pairing of a dataset and a task category, such as extractive question answering or query generation. A dataset can be associated with multiple task categories. For example, the SQuAD dataset supports extractive question answering, query generation, and context generation.</p>
<p>Across all model sizes, increasing the number of tasks up to 282 led to substantial improvements. However, adding more tasks beyond this threshold yielded only marginal gains. The authors propose two explanations for this observation.
First, the additional tasks likely offer limited new information for models already trained on 282 tasks.
Second, instruction finetuning predominantly enhances the model’s capacity to express knowledge acquired during pre-training, rather than providing entirely new knowledge not seen in the pre-training corpus.</p>
<p>In contrast, incorporating chain-of-thought annotations into the training datasets resulted in significant performance improvements on previously unseen reasoning tasks, highlighting the value of CoT data for enhancing models&rsquo; reasoning abilities.</p>
  </div>
</main>

  
<ul class="pagination">
  
  <li>
    <a href="/en/posts/reading-wikipedia-to-answer-open-domain-questions/">
      <i class="fa-solid fa-xl fa-angle-left"></i>
    </a>
  </li>
  
  
  <li>
    <a href="/en/posts/supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data/">
      <i class="fa-solid fa-xl fa-angle-right"></i>
    </a>
  </li>
  
</ul>


  <footer>
    <small>© Ryotaro Nakamura. All Rights Reserved.</small>
  </footer>
</body>


</html>
