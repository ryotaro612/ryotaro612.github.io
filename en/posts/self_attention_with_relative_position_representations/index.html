<!DOCTYPE html>
<html lang="en">

<head>
  
  
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W5TDG76');
  </script>
  
  
  <meta charset="utf-8">
  <meta name="viewport" content="width=300,initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
    integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
    crossorigin="anonymous" />
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
  

<link rel="stylesheet" href="https://ryotaro.dev/scss/single.min.8ed5cbfb3dc516e9c4459a65252e4e6e8e9026aabfa9eb9f602cf53b30754966.css">

<link rel="stylesheet" href="https://ryotaro.dev/scss/posts/single.min.0008713f3f7556e78c13cb8efde9cf534239cb206c10ba85ce5a76e6ae9ce758.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  <title>Self-Attention with Relative Position Representations(2018)</title>
</head>

<body>
  
  
  <noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0"
      style="display:none;visibility:hidden" />
  </noscript>
  
  
  <header class="navigation">
    <h2><a href="https://ryotaro.dev/en/">Blanket</a></h2>
    <nav>
      <ul>
        <li><a href="https://ryotaro.dev/en/%20about">About</a></li>
        <li><a href="https://ryotaro.dev/en/%20posts">Posts</a></li>
        <li><a href="https://ryotaro.dev/en/%20tags">Tags</a></li>
        
        <li><a href="https://ryotaro.dev/posts/self_attention_with_relative_position_representations/">ja</a></li>
        
      </ul>
      <ul>
        
        <li>
          <a href="https://github.com/ryotaro612">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/ryotaro612/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://speakerdeck.com/ryotaro612/">
            <i class="fa-brands fa-speaker-deck"></i>
          </a>
        </li>
        
        <li>
          <a href="https://ryotaro.dev/en/%20index.xml">
            <i class="fas fa-rss"></i>
          </a>
        </li>
        
      </ul>
    </nav>
  </header>
  
<main>
  <h1>Self-Attention with Relative Position Representations(2018)</h1>
  <ul class="tags">
    
    <li class="tag">
      <a href="https://ryotaro.dev/en/tags/attention/">
	#Attention
      </a>
    </li>
    
    <li class="tag">
      <a href="https://ryotaro.dev/en/tags/transformer/">
	#Transformer
      </a>
    </li>
    
  </ul>  
  <span class="date">August 19, 2023</span>    
  <div>
    <p>The authors of <a href="https://arxiv.org/pdf/1803.02155.pdf">Self-Attention with Relative Position Representations</a> presented a way of injecting relative position representations in the self-attention mechanism of the <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Transformer</a>.
In contrast to recurrent and convolutional neural networks, Transformer does not explicitly model position information in its structure.
<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">The original position encoding</a> employs sine and cosine functions of different frequencies.
The authors of Transformer hypothesized that sinusoidal position encodings would help Transformer to generalize to sequence lengths unseen during training.
Positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks of Transformer.
This hypothesis was shared by the relative position representations.
In contrast to absolute position representations, the relative position representations are invariant to the total sequence length.</p>
<p>Self-attention mechanism maps an input sequence, \(x=(x_1 , \dots , x_n)\) of \(n\) elements where \(x_i\in\mathbb{R}^{d_x}\), to a new sequence, \(z=(z_1,\dots , z_n)\) of \(n\) where \(z_i\in\mathbb{R}^{d_z}\).
The output is computed as a weighted sum of a linearly transformed input elements:
$$
\begin{align}
z_i&amp;=\sum^n_{j=1}\alpha_{ij}(x_jW^V)\\
\alpha_{ij}&amp;=\frac{\exp e_{ij}}{\sum^n_{k=1}\exp e_{ik}}\\
e_{ij}&amp;=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}
\end{align}
$$
Where the projections are parameter matrices \(W^Q, W^K, W^V \in \mathbb{R}^{d_x\times d_z}\).</p>
<p>They extended the self-attention to consider the pairwise relationships between input elements without additional linear transformations.
They modified equations (1) and (3):
$$
\begin{align}
z_i&amp;=\sum^n_{j=1}\alpha_{ij}(x_jW^V+w^V_{\text{clip}(j-i,k)})\\
e_{ij}&amp;=\frac{(x_iW^Q)(x_jW^K+w^K_{\text{clip}(j-i, k)})^T}{\sqrt{d_z}}
\end{align}
$$
They employed the function \(\text{clip}(x, k)\) to clip the maximum relative position to an absolute value of \(k\):
$$
\text{clip}(x, k)=\max(-k, \min(k, x))
$$
They hypothesized that precise relative position information is not useful beyond a certain distance, \(k\), and the parameters of the relative position representations are \(w^K_i,w^V_i\in \mathbb{R}^{d_a}\) and \(w^K=(w^K_{-k},\dots ,w^K_{k})\) where \(w^V=(w_{-k}^V,\dots w_k^V)\).</p>
  </div>
</main>

  
<ul class="pagination">
    
    <li>
      <a href="/en/posts/self_adjusting_binary_search_trees/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/en/posts/go_to_statement_considered_harmful/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    
</ul>


  <footer>
    <small>Â© Ryotaro. All Rights Reserved.</small>
  </footer>
</body>


</html>
