<!DOCTYPE html>
<html lang="en">
  <head>
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://ryotaro.dev/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
    
    <link rel="stylesheet" href="https://ryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
    

<link rel="stylesheet" href="https://ryotaro.dev/scss/single.min.8ed5cbfb3dc516e9c4459a65252e4e6e8e9026aabfa9eb9f602cf53b30754966.css">

<link rel="stylesheet" href="https://ryotaro.dev/scss/posts/single.min.0008713f3f7556e78c13cb8efde9cf534239cb206c10ba85ce5a76e6ae9ce758.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="https://ryotaro.dev/en/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="https://ryotaro.dev/en/about">About</a></li>
          <li><a href="https://ryotaro.dev/en/posts">Posts</a></li>
          <li><a href="https://ryotaro.dev/en/tags">Tags</a></li>
	  
	  <li><a href="https://ryotaro.dev/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/">ja</a></li>
	  
        </ul>	
	<ul>
          
          <li>
            <a href="https://github.com/ryotaro612">
              <i class="fab fa-github"></i>
            </a>
          </li>
          
          
          <li>
            <a href="https://www.linkedin.com/in/ryotaro612/">
              <i class="fab fa-linkedin-in"></i>
            </a>
          </li>
          
          <li>
	    <a href="https://ryotaro.dev/en/index.xml">
	      <i class="fas fa-rss"></i>
            </a>
	  </li>
          
        </ul>	
      </nav>
    </header>
    
<main>
  <h1>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</h1>
  <ul class="tags">
    
    <li class="tag">
      <a href="https://ryotaro.dev/en/tags/transformer/">
	#Transformer
      </a>
    </li>
    
  </ul>  
  <span class="date">October 7, 2023</span>    
  <div>
    <p><a href="https://arxiv.org/abs/1910.13461">BART</a> is a denoising autoencoder for Pretraining sequence-to-sequence models.
BART is trained on corrupted text, and updates the parameters to reconstruct the original text.
The authors experimented with several noising functions that corrupt text like token masking, token deletion, text infilling, sentence permutation, and document rotation.
BART with text infilling, where text spans are sampled with span lengths drawn from a Poisson distribution(\(\lambda = 3\)), demonstrated the most consistently strong performance.</p>
<p>BART is a sequence-to-sequence model with a bidirectional encoder and a left-to-right autoregressive decoder.
BART replaces the ReLUs in <a href="https://browse.arxiv.org/pdf/1706.03762.pdf">Transformer</a> with <a href="https://browse.arxiv.org/pdf/1606.08415.pdf">GeLUs</a>.
The authors of BART see the model as a standard Transformer-based neural machine translation architecture that generalizes BERT and GPT.
Comparing BART and BERT, only BERT uses an additional feed-forward network before word-predictions, and the second multi-head attention mechanism in each decoder performs cross-attention over the final hidden layer of the encoder.</p>
<p>If a downstream task is a classification task, the same input is fed into the encoder and decoder.
An additional token like <code>CLS</code> in BERT is added to the end, and its final hidden state of the final decoder is fed into a linear classifier.</p>
  </div>
</main>

    
<ul class="pagination">
    
    <li>
      <a href="/en/posts/communicating_sequential_processes/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/en/posts/a_design_methodology_for_reliable_software_systems/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    
</ul>


    <footer>
      <small>Â© Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
