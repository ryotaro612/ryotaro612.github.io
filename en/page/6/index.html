<!DOCTYPE html>
<html lang="en">

<head>
	<meta name="generator" content="Hugo 0.155.3">
  
  
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W5TDG76');
  </script>
  
  
  <meta charset="utf-8">
  <meta name="viewport" content="width=300,initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
    integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
    crossorigin="anonymous" />
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
  

<link rel="stylesheet" href="https://ryotaro.dev/scss/articles.min.3ceb81a62f07e7057aa63e938b7f0479d4643c093f1c72984a82a68278ddbba3.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css"
  integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi"
  crossorigin="anonymous"
>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"
  integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ"
  crossorigin="anonymous">
</script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},  
        {left: '$$', right: '$$', display: true},    
        {left: '\\(', right: '\\)', display: false},  
        {left: '$', right: '$', display: false}  
      ],
      throwOnError : false
    });
  });
</script>
  <title>Blanket</title>
</head>

<body>
  
  
  <noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0"
      style="display:none;visibility:hidden" />
  </noscript>
  
  
  <header class="navigation">
    <h2><a href="https://ryotaro.dev/en/">Blanket</a></h2>
    <nav>
      <ul>
        <li><a href="https://ryotaro.dev/en/about">About</a></li>
        <li><a href="https://ryotaro.dev/en/posts">Posts</a></li>
        <li><a href="https://ryotaro.dev/en/tags">Tags</a></li>
        
        <li><a href="https://ryotaro.dev/">ja</a></li>
        
      </ul>
      <ul>
        
        <li>
          <a href="https://github.com/ryotaro612">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/ryotaro612/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://speakerdeck.com/ryotaro612/">
            <i class="fa-brands fa-speaker-deck"></i>
          </a>
        </li>
        
        <li>
          <a href="https://ryotaro.dev/en/%20index.xml">
            <i class="fas fa-rss"></i>
          </a>
        </li>
        
      </ul>
    </nav>
  </header>
  
<main>
  <h1>Posts</h1>
  
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/a_design_methodology_for_reliable_software_systems/">A design methodology for reliable software systems (1972)</a></h3>
    <div class="summary"><p>After earning her PhD in computer Science at Stanford University, Liskov worked again for MITRE Corporation.
She was involved in the development of a time-sharing system called Venus, then was involved in finding ways to address the &ldquo;software crisis.&rdquo; at MITRE.
<a href="https://dl.acm.org/doi/pdf/10.1145/1479992.1480018">A design methodology for reliable software systems</a> describes a design methodology of structured programming developed as part of the second project.</p>
<p>The methodology uses testing to guarantee reliability.
To test a program, it is necessary to identify relevant test cases, and the set of them must be small enough to implement.
Structured programming helps to identify the relevant test cases and reduce the number of required test cases by dividing a system into modules.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/structured-programming/">
            #Structured Programming
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        October 8, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</a></h3>
    <div class="summary"><p><a href="https://arxiv.org/abs/1910.13461">BART</a> is a denoising autoencoder for Pretraining sequence-to-sequence models.
BART is trained on corrupted text, and updates the parameters to reconstruct the original text.
The authors experimented with several noising functions that corrupt text like token masking, token deletion, text infilling, sentence permutation, and document rotation.
BART with text infilling, where text spans are sampled with span lengths drawn from a Poisson distribution(\(\lambda = 3\)), demonstrated the most consistently strong performance.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/transformer/">
            #Transformer
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        October 7, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/communicating_sequential_processes/">Communicating Sequential Processes (1978)</a></h3>
    <div class="summary"><p><a href="https://www.cs.cmu.edu/~crary/819-f09/Hoare78.pdf">Communicating Sequential Processes (CSP)</a> is a program structuring method that constructs a program as a parallel composition of a fixed number of sequential processes.
A process is a sequence of commands.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/communicating-sequential-processes/">
            #Communicating Sequential Processes
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/concurrent-programming/">
            #Concurrent Programming
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 30, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks(2019)</a></h3>
    <div class="summary"><p><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a> derives semantically meaningful sentence embedding that can be compared using cosine-similarity.
<a href="https://arxiv.org/abs/1810.04805">BERT</a> achieved new state-of-the art performance on various sentence-pair regression tasks using a cross-encoder.
A cross-encoder accepts two sentences as input to the transformer network and the target value is predicted.
Semantic textual similarity is one of the sentence-pair regression tasks.
However, this setup is often not scalable for various pair regression tasks due to many possible combinations.
The semantic search that maps each sentence to a vector space where semantically similar sentences are close alleviates the combinatorial explosion.
Sentence-BERT uses a siamese network in which the two BERT networks have tied weights such that the produced sentence embeddings can be semantically compared using cosine-similarity.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/bert/">
            #BERT
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/cosine-similarity/">
            #Cosine Similarity
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/transformer/">
            #Transformer
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/sentence-embedding/">
            #Sentence Embedding
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 23, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning (2008)</a></h3>
    <div class="summary"><p><a href="http://machinelearning.org/archive/icml2008/papers/391.pdf">A Unified Architecture for Natural Language Processing</a> is an instance of multitask learning.
The first layer is a lookup table that stores embeddings of a fixed dictionary and size.
The second layer is a <a href="https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf">Time-Delay Neural Networks</a> layer.
It extracts features from the sentence treating it as a sequence with local structure.
The third layer takes the maximum value for each of the output features of the second layer over time.
The following layers are classical NN layers.
The lookup-table is shared among the tasks, and the other layers can be task specific to each task.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/multitask-learning/">
            #Multitask Learning
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 16, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/transmission_of_information/">Transmission of Information(1927)</a></h3>
    <div class="summary"><p>In <a href="https://monoskop.org/images/a/a6/Hartley_Ralph_VL_1928_Transmission_of_Information.pdf">Transmission of Information</a>, Hartley developed a quantitative measure of &ldquo;information&rdquo; in 1927.
Hartley claims that information is the outcome of a selection among a finite set of possible messages.
Shannon&rsquo;s &ldquo;A mathematical theory of communication&rdquo;, which is based in part on Hartley&rsquo;s ideas, published in 1947.
Hartley did not model the source of information probabilistically.
Shannon modeled the source of information as a random process.</p>
<p>Hartley stated that information should be proportional to the number of selections for practical engineering value.
He uses the letter \(H\) to denote the amount of information associated with \(n\) selections.
</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/entropy/">
            #Entropy
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 9, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/roberta/">RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</a></h3>
    <div class="summary"><p><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>(Robusty optimized BERT approach) is an improved recipe for training <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> models.
BERT uses two objectives, masked language modeling and next sequence prediction (NSP), during pretraining.
RoBERTa uses masked language modeling only.
The authors increased the batch size and Byte-Pair Encoding (BPE) vocabulary size.
RobERTa is trained with byte-level BPE, which uses bytes instead of unicode characters as the base subword units.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/bert/">
            #BERT
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/byte-pair-encoding/">
            #Byte Pair Encoding
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 27, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/go_to_statement_considered_harmful/">Go To Statement Considered Harmful (1968)</a></h3>
    <div class="summary"><p>Edgar Djkstra criticized the excessive use of the go to statement in <a href="https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf">Go To Statement Considered Harmful</a>.
While the source code is static, the process taking place under the control of the source code is dynamic.
The programmers should aim to shorten the conceptual gap between the source code and its process to describe the progress.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/early-programming/">
            #Early Programming
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 26, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/self_attention_with_relative_position_representations/">Self-Attention with Relative Position Representations(2018)</a></h3>
    <div class="summary"><p>The authors of <a href="https://arxiv.org/pdf/1803.02155.pdf">Self-Attention with Relative Position Representations</a> presented a way of injecting relative position representations in the self-attention mechanism of the <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Transformer</a>.
In contrast to recurrent and convolutional neural networks, Transformer does not explicitly model position information in its structure.
<a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">The original position encoding</a> employs sine and cosine functions of different frequencies.
The authors of Transformer hypothesized that sinusoidal position encodings would help Transformer to generalize to sequence lengths unseen during training.
Positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks of Transformer.
This hypothesis was shared by the relative position representations.
In contrast to absolute position representations, the relative position representations are invariant to the total sequence length.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/attention/">
            #Attention
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/transformer/">
            #Transformer
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 19, 2023
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/self_adjusting_binary_search_trees/">Self-Adjusting Binary Search Trees(1985)</a></h3>
    <div class="summary"><p><a href="https://www.cs.cmu.edu/~sleator/papers/self-adjusting.pdf">The splay tree</a>, a self-adjusting form of a binary search tree, is a binary search tree that moves an accessed node to the root after each access.
On an \(n\)-node splay tree, accessing, inserting and deleting have an amortized time bound of \(O(\log n)\) per operation.
In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/splay-tree/">
            #Splay Tree
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 12, 2023
        
      </span>
    </div>
  </article>
  
</main>

  

<ul class="pagination">
  
  <li>
    <a href="/en/">
      <i class="fa-solid fa-xl fa-angles-left"></i>
    </a>
  </li>
  <li>
    <a href="/en/page/5/">
      <i class="fa-solid fa-xl fa-angle-left"></i>
    </a>
  </li>
  
  
  <li>
    <a href="/en/page/7/">
      <i class="fa-solid fa-xl fa-angle-right"></i>
    </a>
  </li>
  <li>
    <a href="/en/page/7/">
      <i class="fa-solid fa-xl fa-angles-right"></i>
    </a>
  </li>
  
</ul>

  <footer>
    <small>Â© Ryotaro Nakamura. All Rights Reserved.</small>
  </footer>
</body>


</html>
