<!DOCTYPE html>
<html>
  <head>
	<meta name="generator" content="Hugo 0.121.0">
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.min.87b8f4076c47000c129b0c8b240a71216448e3aedd7144174c55776680a783b0.css">
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
    

<link rel="stylesheet" href="https://nryotaro.dev/scss/articles.min.ad090cad4f6d9fadf06a509c7ea790bd1fc13b24802f351ad71342659fdebf9f.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Blanket</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="https://nryotaro.dev/en/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="https://nryotaro.dev/en/about">About</a></li>
          <li><a href="https://nryotaro.dev/en/posts">Posts</a></li>
          <li><a href="https://nryotaro.dev/en/tags">Tags</a></li>
	  
	  <li><a href="https://nryotaro.dev/">ja</a></li>
	  
        </ul>
      </nav>
    </header>
    
<main>
    <h1>Posts</h1>
    
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/lora-low-rank-adaptation-of-large-language-models/">LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></h3>
        <div class="summary">LoRA is inspired by Li et al. (2018) and Aghajanyan et al. (2020) which show that the learned over-parameterized models reside on a low intrinsic dimension. An intrinsic dimension is the minimum number of parameters needed to reach a satisfactory solution to the objective. LoRA injects weight matrices into each attention layer of the Transformer architecture instead of fine-tuning the pre-trained model weights. The injected weight matrices are rank decomposition matrices, and LoRA can reduce the number of trainable parameters for downstream tasks.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/large-language-model/">
		#Large language model
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 18, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/pastry-scalable-decentralized-object-location-and-routing-for-large-scale-peer-to-peer-systems/">Pastry: Scalable Decentralized Object Location and Routing for Large Scale Peer to Peer Systems (2001)</a></h3>
        <div class="summary"><p><a href="https://rowstron.azurewebsites.net/PAST/pastry.pdf">Pastry</a> is a decentralized peer-to-peer object location and routing system based on nodes connected via the Internet.
When each node in the Pastry network receives a message and a numeric key, it routes them to the node with a nodeId that is numerically close to the key.
If a node is not the final destination of a message, it forwards the message to another node with a nodeId that is numerically closer to the key than the nodeId of the present node.
The nodeId ranges from 0 to \(2^{128}-1\).</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/peer-to-peer/">
		#Peer-to-peer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 7, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/gaussian_error_linear_units/">GAUSSIAN ERROR LINEAR UNITS (GELUs) (2016)</a></h3>
        <div class="summary"><p>The <a href="https://arxiv.org/abs/1606.08415">GAUSSIAN ERROR LINEAR UNITS (GELUs)</a>, a neural network activation function,is \(x\Phi(s)\), where \(\Phi(x)\) the standard Gaussian cumulative distribution function.
GELUs have properties of Dropout, <a href="https://openreview.net/pdf?id=rJqBEPcxe">Zoneout</a>, and ReLUs.
Zoneout is a method for regularizing RNNs, and stochastically forces some hidden units to maintain their previous values.
ReLUs introduce nonlinearity to neural networks.
Dropout is a regularizer.
GELUs merge the both functionalities by multipling the neuron input \(x\) by \(m \sim \text{Bernoulli}(\Phi (x))\) , where \(\Phi(x)=P(X\le x), X\sim \mathcal{N}(0, 1)\).
GELU is the expected transformation on an input \(x\), which is \(\Phi(x) \times Ix + (1-\Phi(x))\times 0x = x\Phi(x)\)</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/activation-function/">
		#Activation function
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 21, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/a_design_methodology_for_reliable_software_systems/">A design methodology for reliable software systems (1972)</a></h3>
        <div class="summary">After earning her PhD in computer Science at Stanford University, Liskov worked again for MITRE Corporation. She was involved in the development of a time-sharing system called Venus, then was involved in finding ways to address the &ldquo;software crisis.&rdquo; at MITRE. A design methodology for reliable software systems describes a design methodology of structured programming developed as part of the second project.
The methodology uses testing to guarantee reliability. To test a program, it is necessary to identify relevant test cases, and the set of them must be small enough to implement.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/structured-programming/">
		#Structured programming
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 8, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/bart_denoising_sequence_to_sequence_pre-training_for_natural_language_generation_translation_and_comprehension/">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/abs/1910.13461">BART</a> is a denoising autoencoder for Pretraining sequence-to-sequence models.
BART is trained on corrupted text, and updates the parameters to reconstruct the original text.
The authors experimented with several noising functions that corrupt text like token masking, token deletion, text infilling, sentence permutation, and document rotation.
BART with text infilling, where text spans are sampled with span lengths drawn from a Poisson distribution(\(\lambda = 3\)), demonstrated the most consistently strong performance.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/transformer/">
		#Transformer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">October 7, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/communicating_sequential_processes/">Communicating Sequential Processes (1978)</a></h3>
        <div class="summary"><p><a href="https://www.cs.cmu.edu/~crary/819-f09/Hoare78.pdf">Communicating Sequential Processes (CSP)</a> is a program structuring method that constructs a program as a parallel composition of a fixed number of sequential processes.
A process is a sequence of commands.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/communicating-sequential-processes/">
		#Communicating Sequential Processes
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/concurrent-programming/">
		#Concurrent Programming
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 30, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks(2019)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a> derives semantically meaningful sentence embedding that can be compared using cosine-similarity.
<a href="https://arxiv.org/abs/1810.04805">BERT</a> achieved new state-of-the art performance on various sentence-pair regression tasks using a cross-encoder.
A cross-encoder accepts two sentences as input to the transformer network and the target value is predicted.
Semantic textual similarity is one of the sentence-pair regression tasks.
However, this setup is often not scalable for various pair regression tasks due to many possible combinations.
The semantic search that maps each sentence to a vector space where semantically similar sentences are close alleviates the combinatorial explosion.
Sentence-BERT uses a siamese network in which the two BERT networks have tied weights such that the produced sentence embeddings can be semantically compared using cosine-similarity.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/bert/">
		#BERT
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/cosine-similarity/">
		#cosine similarity
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/transformer/">
		#Transformer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 23, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/a_unified_architecture_for_natural_language_processing_deep_neural_networks_with_multitask_learning/">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning(2008)</a></h3>
        <div class="summary"><p><a href="http://machinelearning.org/archive/icml2008/papers/391.pdf">A Unified Architecture for Natural Language Processing</a> is an instance of multitask learning.
The first layer is a lookup table that stores embeddings of a fixed dictionary and size.
The second layer is a <a href="https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf">Time-Delay Neural Networks</a> layer.
It extracts features from the sentence treating it as a sequence with local structure.
The third layer takes the maximum value for each of the output features of the second layer over time.
The following layers are classical NN layers.
The lookup-table is shared among the tasks, and the other layers can be task specific to each task.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/multitask-learning/">
		#Multitask learning
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 16, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/transmission_of_information/">Transmission of Information(1927)</a></h3>
        <div class="summary">In Transmission of Information, Hartley developed a quantitative measure of &ldquo;information&rdquo; in 1927. Hartley claims that information is the outcome of a selection among a finite set of possible messages. Shannon&rsquo;s &ldquo;A mathematical theory of communication&rdquo;, which is based in part on Hartley&rsquo;s ideas, published in 1947. Hartley did not model the source of information probabilistically. Shannon modeled the source of information as a random process.
Hartley stated that information should be proportional to the number of selections for practical engineering value.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/entropy/">
		#Entropy
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">September 9, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/roberta/">RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</a></h3>
        <div class="summary"><p><a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa</a>(Robusty optimized BERT approach) is an improved recipe for training <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> models.
BERT uses two objectives, masked language modeling and next sequence prediction (NSP), during pretraining.
RoBERTa uses masked language modeling only.
The authors increased the batch size and Byte-Pair Encoding (BPE) vocabulary size.
RobERTa is trained with byte-level BPE, which uses bytes instead of unicode characters as the base subword units.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/bert/">
		#BERT
	      </a>
	    </li>
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/byte-pair-encoding/">
		#Byte Pair Encoding
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">August 27, 2023</span>
        </div>
    </article>
    
</main>

    

<ul class="pagination">  
    
    <li>
      <a href="/en/">
	<i class="fa-solid fa-xl fa-angles-left"></i>
      </a>
    </li>
    <li>
      <a href="/en/">
	<i class="fa-solid fa-xl fa-angle-left"></i>
      </a>
    </li>
    
    
    <li>
      <a href="/en/page/3/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    <li>
      <a href="/en/page/3/">
	<i class="fa-solid fa-xl fa-angles-right"></i>
      </a>
    </li>
    
</ul>

    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/nryotaro">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="https://nryotaro.dev/en/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
