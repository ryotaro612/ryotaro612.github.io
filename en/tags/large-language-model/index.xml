<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Model on Blanket</title>
    <link>https://ryotaro.dev/en/tags/large-language-model/</link>
    <description>Recent content in Large Language Model on Blanket</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 14 Mar 2024 14:41:48 +0900</lastBuildDate>
    <atom:link href="https://ryotaro.dev/en/tags/large-language-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLaMA Open and Efficient Foundation Language Models (2023)</title>
      <link>https://ryotaro.dev/en/posts/llama-open-and-efficient-foundation-language-models/</link>
      <pubDate>Thu, 14 Mar 2024 14:41:48 +0900</pubDate>
      <guid>https://ryotaro.dev/en/posts/llama-open-and-efficient-foundation-language-models/</guid>
      <description>Scaling Laws for Neural Language Models refers to empirical observations that model test performance has a power-law relationship with each of the three scale factors: the number of model parameters, the dataset size in tokens, and the amount of compute used for training, when not bottlenecked by the other two. Hoffmann et al. (2022) investigated the bottleneck and found that, given a fixed FLOPs budget, the number of model parameters and the training tokens should be scaled equally to minimize pre-training loss.</description>
    </item>
    <item>
      <title>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</title>
      <link>https://ryotaro.dev/en/posts/lora-low-rank-adaptation-of-large-language-models/</link>
      <pubDate>Sat, 18 Nov 2023 12:21:20 -0500</pubDate>
      <guid>https://ryotaro.dev/en/posts/lora-low-rank-adaptation-of-large-language-models/</guid>
      <description>LoRA is inspired by Li et al. (2018) and Aghajanyan et al. (2020) which show that the learned over-parameterized models reside on a low intrinsic dimension. An intrinsic dimension is the minimum number of parameters needed to reach a satisfactory solution to the objective. LoRA injects weight matrices into each attention layer of the Transformer architecture instead of fine-tuning the pre-trained model weights. The injected weight matrices are rank decomposition matrices, and LoRA can reduce the number of trainable parameters for downstream tasks.</description>
    </item>
  </channel>
</rss>
