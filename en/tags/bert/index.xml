<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>BERT on Blanket</title>
    <link>http://localhost:1313/en/tags/bert/</link>
    <description>Recent content in BERT on Blanket</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 04 Nov 2023 06:43:01 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/en/tags/bert/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (2019)</title>
      <link>http://localhost:1313/en/posts/distilbert-a-distilled-version-of-bert-smaller-fastercheater-and-lighter/</link>
      <pubDate>Sat, 04 Nov 2023 06:43:01 -0400</pubDate>
      <guid>http://localhost:1313/en/posts/distilbert-a-distilled-version-of-bert-smaller-fastercheater-and-lighter/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.01108&#34;&gt;DistilBERT&lt;/a&gt; is a method to pre-train a smaller BERT with &lt;a href=&#34;https://arxiv.org/pdf/1503.02531.pdf&#34;&gt;knowledge distillation&lt;/a&gt;.&#xA;The training loss of DistilBERT is a linear combination of the masked-language modeling loss, cross-entropy loss between the DistilBERT model and the BERT model, a cosine similarity between the hidden states of both models.&#xA;During pre-training, DistilBERT uses a softmax-temperature.&#xA;While at inference, the temperature is set to \(1\).&#xA;The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of two.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks(2019)</title>
      <link>http://localhost:1313/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</link>
      <pubDate>Sat, 23 Sep 2023 10:21:03 -0400</pubDate>
      <guid>http://localhost:1313/en/posts/sentence-bert_sentence_embeddings_using_siamese_bert-networks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.10084&#34;&gt;Sentence-BERT&lt;/a&gt; derives semantically meaningful sentence embedding that can be compared using cosine-similarity.&#xA;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; achieved new state-of-the art performance on various sentence-pair regression tasks using a cross-encoder.&#xA;A cross-encoder accepts two sentences as input to the transformer network and the target value is predicted.&#xA;Semantic textual similarity is one of the sentence-pair regression tasks.&#xA;However, this setup is often not scalable for various pair regression tasks due to many possible combinations.&#xA;The semantic search that maps each sentence to a vector space where semantically similar sentences are close alleviates the combinatorial explosion.&#xA;Sentence-BERT uses a siamese network in which the two BERT networks have tied weights such that the produced sentence embeddings can be semantically compared using cosine-similarity.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)</title>
      <link>http://localhost:1313/en/posts/roberta/</link>
      <pubDate>Sun, 27 Aug 2023 13:13:39 -0400</pubDate>
      <guid>http://localhost:1313/en/posts/roberta/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.11692.pdf&#34;&gt;RoBERTa&lt;/a&gt;(Robusty optimized BERT approach) is an improved recipe for training &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt; models.&#xA;BERT uses two objectives, masked language modeling and next sequence prediction (NSP), during pretraining.&#xA;RoBERTa uses masked language modeling only.&#xA;The authors increased the batch size and Byte-Pair Encoding (BPE) vocabulary size.&#xA;RobERTa is trained with byte-level BPE, which uses bytes instead of unicode characters as the base subword units.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)</title>
      <link>http://localhost:1313/en/posts/bert/</link>
      <pubDate>Sat, 14 Dec 2019 17:39:09 +0900</pubDate>
      <guid>http://localhost:1313/en/posts/bert/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1810.04805&#34;&gt;BERT&lt;/a&gt;, &lt;em&gt;B&lt;/em&gt;idirectional &lt;em&gt;E&lt;/em&gt;ncoder &lt;em&gt;R&lt;/em&gt;epresentations from &lt;em&gt;T&lt;/em&gt;ransformers, is a language model based on the Transformer encoder architecture.&#xA;During pre-training, BERT learns bidirectional representations from unlabeled text using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).&#xA;MLM involves predicting randomly masked tokens in a sequence using surrounding context.&#xA;NSP trains the model to predict whether the second sentence in a pair follows the first in the original document.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
