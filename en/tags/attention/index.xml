<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Attention on Blanket</title>
    <link>https://ryotaro.dev/en/tags/attention/</link>
    <description>Recent content in Attention on Blanket</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 29 Dec 2025 12:34:47 +0900</lastBuildDate>
    <atom:link href="https://ryotaro.dev/en/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)</title>
      <link>https://ryotaro.dev/en/posts/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness/</link>
      <pubDate>Mon, 29 Dec 2025 12:34:47 +0900</pubDate>
      <guid>https://ryotaro.dev/en/posts/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness/</guid>
      <description>&lt;p&gt;Let $N$ be the sequence length. In a standard Transformer, self-attention takes $O(N^2)$ time and $O(N^2)$ memory.&#xA;Approximate methods such as &lt;a href=&#34;https://arxiv.org/pdf/2001.04451&#34;&gt;Reformer: The Efficient Transformer&lt;/a&gt; reduce FLOPs by approximating attention, but they often do not yield significant wall-clock speedups and are therefore not widely used.&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2205.14135&#34;&gt;FLASHATTENTION&lt;/a&gt; instead targets memory traffic: it reduces data movement between GPU high-bandwidth memory (HBM) and on-chip SRAM, which can be a major bottleneck.&#xA;HBM is much larger but has lower bandwidth than SRAM; SRAM is fast but small.&#xA;FLASHATTENTION tiles the computation: it loads blocks of $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ into SRAM, updates a block of the output $\mathbf{O}$, and then writes that block back to HBM.&#xA;By repeating this over all blocks, FLASHATTENTION computes exact attention.&lt;/p&gt;&#xA;&lt;p&gt;FLASHATTENTION also reduces HBMâ€“SRAM traffic in the backward pass.&#xA;In a typical implementation, one materializes the attention score matrix $\mathbf{S}$ and the softmax output $\mathbf{P}$ so they can be reused to compute gradients of $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$, and $\mathbf{O}$.&#xA;When $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}\in \mathbb{R}^{N\times d}$, both $\mathbf{S}$ and $\mathbf{P}$ are $O(N^2)$ in size.&#xA;FLASHATTENTION avoids storing these $N\times N$ matrices by recomputing $\mathbf{S}$ and $\mathbf{P}$ from $\mathbf{O}$ and intermediate values during backpropagation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Attention with Relative Position Representations(2018)</title>
      <link>https://ryotaro.dev/en/posts/self_attention_with_relative_position_representations/</link>
      <pubDate>Sat, 19 Aug 2023 10:02:50 -0400</pubDate>
      <guid>https://ryotaro.dev/en/posts/self_attention_with_relative_position_representations/</guid>
      <description>&lt;p&gt;The authors of &lt;a href=&#34;https://arxiv.org/pdf/1803.02155.pdf&#34;&gt;Self-Attention with Relative Position Representations&lt;/a&gt; presented a way of injecting relative position representations in the self-attention mechanism of the &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Transformer&lt;/a&gt;.&#xA;In contrast to recurrent and convolutional neural networks, Transformer does not explicitly model position information in its structure.&#xA;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;The original position encoding&lt;/a&gt; employs sine and cosine functions of different frequencies.&#xA;The authors of Transformer hypothesized that sinusoidal position encodings would help Transformer to generalize to sequence lengths unseen during training.&#xA;Positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks of Transformer.&#xA;This hypothesis was shared by the relative position representations.&#xA;In contrast to absolute position representations, the relative position representations are invariant to the total sequence length.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
