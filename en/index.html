<!DOCTYPE html>
<html>
  <head>
	<meta name="generator" content="Hugo 0.121.0">
    
        
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
		    })(window,document,'script','dataLayer','GTM-W5TDG76');
    </script>
        
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
          integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
          crossorigin="anonymous" />
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.min.87b8f4076c47000c129b0c8b240a71216448e3aedd7144174c55776680a783b0.css">
    
    <link rel="stylesheet" href="https://nryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
    

<link rel="stylesheet" href="https://nryotaro.dev/scss/articles.min.ad090cad4f6d9fadf06a509c7ea790bd1fc13b24802f351ad71342659fdebf9f.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    <title>Blanket</title>
  </head>

  <body>
        
    
    <noscript>
      <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0" style="display:none;visibility:hidden"/>
    </noscript>
    
        
    <header class="navigation">
      <h2><a href="https://nryotaro.dev/en/">Blanket</a></h2>
      <nav>
        <ul>
          <li><a href="https://nryotaro.dev/en/about">About</a></li>
          <li><a href="https://nryotaro.dev/en/posts">Posts</a></li>
          <li><a href="https://nryotaro.dev/en/tags">Tags</a></li>
	  
	  <li><a href="https://nryotaro.dev/">ja</a></li>
	  
        </ul>
      </nav>
    </header>
    
<main>
    <h1>Posts</h1>
    
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/billion-scale-similarity-search-with-gpus/">Billion Scale Similarity Search With GPUs (2017)</a></h3>
        <div class="summary"><p>The paper <a href="https://arxiv.org/abs/1702.08734">Billion-scale-similarity search with GPUs</a> presents an approximate nearest-neighbor search method that uses GPUs and combines the <a href="https://inria.hal.science/inria-00514462v2/document">IVFADC</a> indexing structure with <a href="https://www.cs.kent.edu/~batcher/sort.pdf">Batcher&rsquo;s bitonic sorting network</a>.
The authors have implemented this method in a library called <a href="https://github.com/facebookresearch/faiss">faiss</a>.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/approximate-nearest-neighbors/">
		#Approximate Nearest Neighbors
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">January 1, 2024</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/conflict-free-replicated-data-types/">Conflict-free Replicated Data Types (2011)</a></h3>
        <div class="summary">In a distributed system with replicated objects distributed across processes interconnected by an asynchronous network, updating a replica without synchronization can lead to conflicts when the update is sent to other replicas. Conflict-free Replicated Data Types (CRDTs) are data structures whose states form a join semilattice and monotonically non-decreasing across updates. The replicas of CRDTs that have delivered the same updates eventually reach the same state if clents stop submitting updates.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/conflict-free-replicated-data-types/">
		#Conflict-free Replicated Data Types
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 31, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/dense-passage-retrieval-for-open-domain-question-answering/">Dense Passage Retrieval for Open-Domain Question Answering (2020)</a></h3>
        <div class="summary"><p>Open-domain question answering (QA) involves answering fact-based questions using documents.
An open-domain QA system can be divided into two components: one that retrieves relevant passages and another that extracts the answer spans from those passages <a href="https://arxiv.org/abs/1704.00051">(Chen et al., 2017)</a>.
While traditional approaches use sparse vector space models like BM25 for the retrieval step, <a href="https://aclanthology.org/2020.emnlp-main.550/">Dense Passage Retrieval for Open-Domain Question Answering</a> shows that dense representations can also be practically implemented using dense representations.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The embeddings are learned from the training dataset, optimizing for maximizing inner products between question and relevant passage vectors.
The training is essentially metric learning, and each question needs irrelvant passages.
In experiments, it was found that utilizing both the top passages returned by BM25, excluding the answer, and positive passages paired with other questions as negatives yielded best performance.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted --></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/question-answering/">
		#Question answering
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 30, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/cap-twelve-years-later-how-the-rules-have-changed/">CAP Twelve Years Later: How the &#34;Rules&#34; Have Changed (2012)</a></h3>
        <div class="summary">The CAP theorem, coined by Eric Brewer, states that in a shared-data system, you can have either Consistency (C), Availability (A), or Parition tolerance (P), but not all three simultaneously. However, Martin Kleppmann challenges the CAP theorem and discusses its limitations in Designing Data-Intensive Applications, and A Critique of the CAP Theorem. In CAP Twelve Years Later: How the “Rules” Have Changed, Brewer clarifies that the notion of &ldquo;2 of 3&rdquo; is misleading.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/cap-theorem/">
		#CAP theorem
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 23, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/wide-and-deep-learning-for-recommender-systems/">Wide and Deep Learning for Recommender Systems (2016)</a></h3>
        <div class="summary"><p>Generalized linear models are widely used for large-scale regression and classification problems with sparse inputs because they are simple, scale and interpretable.
One limitation of interactions or cross-prodcution transformations in generalized linear models is that they do not generalize to query-item feature pairs that have not appeared in the training data.
Compared with generalized linear models, deep neural networks can improve the diversity of the recommendations.
Howerver it is difficult to learn effective low-dimentional dense embedding vectors.
<a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a> jointly trains a generalized linear model and a feed-forward neural network (FFN) to combines their benefits.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/recommendation/">
		#Recommendation
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 16, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/a_protocol_for_packet_network_intercommunication/">A Protocol for Packet Network Intercommunication (1974)</a></h3>
        <div class="summary"><p>Vinton Cerf and Robert Kahn presented a protocol that supports packet communication between hosts in different packet switching networks in <a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">A Protocol for Packet Network Intercommunication</a>.
The protocol assumes that a Transmission Control Program (TCP) in a host handles the transmission and acceptance of messages on behalf of processes.
Later, the program was divided into the Transmission Control Protocol (TCP) and the Internet Protocol (IP).
At that time, several protocols that supported exchanging packets between computers were developed, but they assumed the computers were on the same network.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/tcp/">
		#TCP
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 9, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/probabilistic_latent_semantic_indexing/">Probablistic Latent Semantic Indexing (1999)</a></h3>
        <div class="summary"><p><a href="http://wordvec.colorado.edu/papers/Deerwester_1990.pdf">Latent Semantic Analysis</a> approximates an original term-document matrix by singular value decomposition (SVD).
The components consist of the frequencies with which each term occurred in each document.
The Latent Semantic Analysis thresholds all but the largest \(K\) singular values to zero.
The left singular vectors are a \(t \times K\) matrix, and the transpose of the right singular vectors are a \(K\times d\) matrix where \(t\) is the number of the terms and \(d\) is the number of the documents.</p>
<p><a href="https://sigir.org/wp-content/uploads/2017/06/p211.pdf">Probabilistic Latent Semantic Indexing</a> (PLSI) is a generative model that associates each term and document with an unobserved class variable \(z \in \mathcal{Z} = \{z_1,\dots , z_K\}\).
The likelihoods of the documents and terms are estimated by the Expectation Maximization (EM) algorithm, and the joint probability model \(\textbf{P}\) can be written as \(\textbf{P}=\hat{\textbf{U}}\hat{\Sigma}\hat{\textbf{V}}^t\) where \(\hat{\textbf{U}}=(P(d_i|z_k))_{i, k}\), \(\hat{\textbf{V}}=(P(w_j|z_k))_{j, k}\), \(\hat{\Sigma}=\text{diag} (P(z_k))_k\).
Compared to the Latent Semantic Analysis, the components of \(\textbf{P}\) have a clear probabilistic meaning in terms of mixture component distributions.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/latent-semantic-analysis/">
		#Latent Semantic Analysis
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">December 2, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/chord-a-scalable-peer-to-peer-lookup-service-for-internet-application/">Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications (2001)</a></h3>
        <div class="summary"><p><a href="https://pdos.csail.mit.edu/papers/ton:chord/paper-ton.pdf">Chord</a> is a distributed lookup protocol that supports just one operation: mapping a given key onto a node in a Chord cluster.
If an \(N\)-node system is in the steady state, each node resolves all the queries via \(O(\log N)\) messages to other nodes.
Chord uses <a href="https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/chash.pdf">consistent hashing</a> to assign keys to Chord nodes.</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/peer-to-peer/">
		#Peer-to-peer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 23, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/lora-low-rank-adaptation-of-large-language-models/">LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></h3>
        <div class="summary">LoRA is inspired by Li et al. (2018) and Aghajanyan et al. (2020) which show that the learned over-parameterized models reside on a low intrinsic dimension. An intrinsic dimension is the minimum number of parameters needed to reach a satisfactory solution to the objective. LoRA injects weight matrices into each attention layer of the Transformer architecture instead of fine-tuning the pre-trained model weights. The injected weight matrices are rank decomposition matrices, and LoRA can reduce the number of trainable parameters for downstream tasks.</div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/large-language-model/">
		#Large language model
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 18, 2023</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/en/posts/pastry-scalable-decentralized-object-location-and-routing-for-large-scale-peer-to-peer-systems/">Pastry: Scalable Decentralized Object Location and Routing for Large Scale Peer to Peer Systems (2001)</a></h3>
        <div class="summary"><p><a href="https://rowstron.azurewebsites.net/PAST/pastry.pdf">Pastry</a> is a decentralized peer-to-peer object location and routing system based on nodes connected via the Internet.
When each node in the Pastry network receives a message and a numeric key, it routes them to the node with a nodeId that is numerically close to the key.
If a node is not the final destination of a message, it forwards the message to another node with a nodeId that is numerically closer to the key than the nodeId of the present node.
The nodeId ranges from 0 to \(2^{128}-1\).</p></div>
        <div class="article-footer">
	  	  
	  <ul class="tags">
	    
            <li class="tag">
	      <a href="https://nryotaro.dev/en/tags/peer-to-peer/">
		#Peer-to-peer
	      </a>
	    </li>
	    
	  </ul>
	  
          <span class="date">November 7, 2023</span>
        </div>
    </article>
    
</main>

    

<ul class="pagination">  
    
    
    <li>
      <a href="/en/page/2/">
	<i class="fa-solid fa-xl fa-angle-right"></i>
      </a>
    </li>
    <li>
      <a href="/en/page/3/">
	<i class="fa-solid fa-xl fa-angles-right"></i>
      </a>
    </li>
    
</ul>

    <footer>
      <ul>
        
        <li>
          <a href="https://github.com/nryotaro">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        <li><a href="https://nryotaro.dev/en/index.xml"><i class="fas fa-rss"></i></a></li>
        
      </ul>
      <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
  </body>

</html>
