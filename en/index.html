<!DOCTYPE html>
<html lang="en">

<head>
	<meta name="generator" content="Hugo 0.155.3">
  
  
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-W5TDG76');
  </script>
  
  
  <meta charset="utf-8">
  <meta name="viewport" content="width=300,initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
    integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
    crossorigin="anonymous" />
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.min.d5f9c6d3a478082690f838fa06a179ccee4dea5bd3f0cb3f4a357e2803c48d33.css">
  
  <link rel="stylesheet" href="https://ryotaro.dev/scss/base.en.min.fabf7968ffbe5ff626ec9a346f0d38e91d0078974112f713b9d25782380a5073.css">
  

<link rel="stylesheet" href="https://ryotaro.dev/scss/articles.min.3ceb81a62f07e7057aa63e938b7f0479d4643c093f1c72984a82a68278ddbba3.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://kit.fontawesome.com/e48a1b5aa5.js" crossorigin="anonymous"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css"
  integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi"
  crossorigin="anonymous"
>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"
  integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ"
  crossorigin="anonymous">
</script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous"
  onload="renderMathInElement(document.body);">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},  
        {left: '$$', right: '$$', display: true},    
        {left: '\\(', right: '\\)', display: false},  
        {left: '$', right: '$', display: false}  
      ],
      throwOnError : false
    });
  });
</script>
  <title>Blanket</title>
</head>

<body>
  
  
  <noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76" height="0" width="0"
      style="display:none;visibility:hidden" />
  </noscript>
  
  
  <header class="navigation">
    <h2><a href="https://ryotaro.dev/en/">Blanket</a></h2>
    <nav>
      <ul>
        <li><a href="https://ryotaro.dev/en/about">About</a></li>
        <li><a href="https://ryotaro.dev/en/posts">Posts</a></li>
        <li><a href="https://ryotaro.dev/en/tags">Tags</a></li>
        
        <li><a href="https://ryotaro.dev/">ja</a></li>
        
      </ul>
      <ul>
        
        <li>
          <a href="https://github.com/ryotaro612">
            <i class="fab fa-github"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://www.linkedin.com/in/ryotaro612/">
            <i class="fab fa-linkedin-in"></i>
          </a>
        </li>
        
        
        <li>
          <a href="https://speakerdeck.com/ryotaro612/">
            <i class="fa-brands fa-speaker-deck"></i>
          </a>
        </li>
        
        <li>
          <a href="https://ryotaro.dev/en/%20index.xml">
            <i class="fas fa-rss"></i>
          </a>
        </li>
        
      </ul>
    </nav>
  </header>
  
<main>
  <h1>Posts</h1>
  
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness/">FLASHATTENTION: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)</a></h3>
    <div class="summary"><p>Let $N$ be the sequence length. In a standard Transformer, self-attention takes $O(N^2)$ time and $O(N^2)$ memory.
Approximate methods such as <a href="https://arxiv.org/pdf/2001.04451">Reformer: The Efficient Transformer</a> reduce FLOPs by approximating attention, but they often do not yield significant wall-clock speedups and are therefore not widely used.
<a href="https://arxiv.org/pdf/2205.14135">FLASHATTENTION</a> instead targets memory traffic: it reduces data movement between GPU high-bandwidth memory (HBM) and on-chip SRAM, which can be a major bottleneck.
HBM is much larger but has lower bandwidth than SRAM; SRAM is fast but small.
FLASHATTENTION tiles the computation: it loads blocks of $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ into SRAM, updates a block of the output $\mathbf{O}$, and then writes that block back to HBM.
By repeating this over all blocks, FLASHATTENTION computes exact attention.</p>
<p>FLASHATTENTION also reduces HBM–SRAM traffic in the backward pass.
In a typical implementation, one materializes the attention score matrix $\mathbf{S}$ and the softmax output $\mathbf{P}$ so they can be reused to compute gradients of $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$, and $\mathbf{O}$.
When $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}\in \mathbb{R}^{N\times d}$, both $\mathbf{S}$ and $\mathbf{P}$ are $O(N^2)$ in size.
FLASHATTENTION avoids storing these $N\times N$ matrices by recomputing $\mathbf{S}$ and $\mathbf{P}$ from $\mathbf{O}$ and intermediate values during backpropagation.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/attention/">
            #Attention
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        December 29, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/improving_language_understanding_by_generative_pre_training/">Improving Language Understanding by Generative Pre-Training (2018)</a></h3>
    <div class="summary"><p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> describes the original generative pre-trained transformer model. The paper shows that pre-training on unlabeled text followed by fine-tuning on supervised tasks leads to strong performance. During fine-tuning, the output of the pre-trained model is passed to a linear output layer.</p>
<p>The model described in the paper is based on a <a href="https://arxiv.org/pdf/1801.10198">multi-layer Transformer decoder</a>. Unlike the architecture in <a href="https://arxiv.org/pdf/1801.10198">Attention Is All You Need</a>, this model uses only the decoder stack. Representations for upstream tasks are constructed so that their format aligns with the unlabeled text used in pre-training. For example, an entailment example is created by concatenating the premise and the hypothesis with a delimiter. A text similarity task also concatenates two texts with a delimiter and feeds the sequence into the pre-trained model.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/large-language-models/">
            #Large Language Models
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/gpt/">
            #GPT
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        November 16, 2025 (Originally posted on August 7, 2020)</p>
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/deep-neural-networks-for-youtube-recommendations/">Deep Neural Networks for Youtube Recommendations (2016)</a></h3>
    <div class="summary"><p><a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/45530.pdf">Deep Neural Networks for YouTube Recommendations</a>, published in 2016, describes YouTube’s large-scale recommendation system.<br>
The system consists of two feed-forward neural networks: a candidate generation model and a ranking model, together containing approximately one billion parameters.</p>
<p>The candidate generation network suggests hundreds of videos using collaborative filtering.
A user’s representation includes coarse features such as the IDs of watched videos and tokens from search queries.
The ranking network predicts the expected watch time of the videos suggested by the candidate generation network.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/recommendation/">
            #Recommendation
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/collaborative-filtering/">
            #Collaborative Filtering
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 23, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/kademlia-a-peer-to-peer-information-system-based-on-the-xor-metric/">Kademlia: A Peer-to-Peer Information System Based on the XOR Metric (2002)</a></h3>
    <div class="summary"><p><a href="https://www.cs.helsinki.fi/u/lxwang/publications/P2P2013_13.pdf">Mainline DHT</a>, the distributed hash table used by BitTorrent, is based on <a href="https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf">Kademlia</a>.
In Kademlia, both nodes and keys are identified by 160-bit identifiers. The distance between two identifiers is defined as their bitwise XOR interpreted as an integer. To locate other nodes or values, a node repeatedly sends RPC requests to peers whose addresses it knows and that appear closer to the target according to the XOR metric.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/peer-to-peer/">
            #Peer-to-Peer
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 13, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/multi-probe-consistent-hashing/">Multi-probe consistent hashing (2015)</a></h3>
    <div class="summary"><p>Karger et al&rsquo;s <a href="https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/chash.pdf">Consistent hashing</a> is widely used for load balancing in content delivery networks and key-value store databases.
Consistent hashing maps both keys and nodes to values in the unit interval, and assigns each key to its closest node.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/consistent-hashing/">
            #Consistent Hashing
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        September 6, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/infinispan/">Distributed Caches in Infinispan</a></h3>
    <div class="summary"><p><a href="https://infinispan.org/">Infinispan</a> is an open-source in-memory data grid and forms the core of <a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid">Red Hat Data Grid</a>.
It can also serve as a <a href="https://www.keycloak.org/">cache server</a> for <a href="https://www.keycloak.org/">KeyCloak</a>, which provides single sign-on and identity brokering.</p>
<p>The official documentation describes Infinispan as a <a href="https://infinispan.org/use-cases/">drop-in replacement for Redis or Memcached</a>.
It also implements <a href="https://github.com/jsr107/jsr107spec">JSR107</a>, which allows Java clients to interact with an Infinispan cluster via the JCache API.
For example, a client can use a <a href="https://www.javadoc.io/doc/javax.cache/cache-api/latest/index.html">CacheManager</a> to obtain a <a href="https://www.javadoc.io/doc/javax.cache/cache-api/latest/index.html">Cache</a>—a map-like data structure—and perform updates or lookups.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/infinispan/">
            #Infinispan
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/consistent-hashing/">
            #Consistent Hashing
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 23, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/amazon.com-recommendations-item-to-item-collaborative-filtering/">Amazon.com Recommendations Item to Item Collaborative Filtering (2003)</a></h3>
    <div class="summary"><p>When the journal IEEE Internet Computing was celebrating its 20th anniversary in 2017, its editorial board selected <a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf">Amazon.com Recommendations: Item-to-Item Collaborative Filtering</a>, published in 2003, as the paper that best stood the test of time.
The paper views traditional collaborative filtering as the approach of finding users similar to the target user and recommending items those users have selected but the user has not.
Let \(N\) be the number of the items. In the traditional approach, each user is represented as an \(N\)-dimensional vector to measure the similarity between users, the item to item collaborative filtering constructs an \(N\) x \(N\) matrix, where each component represents the similarity between two items.
By computing the item to item matrix offline, the proposed approach can recommend similar items in time independent of both \(N\) and the number of users \(M\).</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/collaborative-filtering/">
            #Collaborative Filtering
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 16, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/outrageously-large-neural-networks-the-sparsely-gated-mixture-of-expert-layer/">OUTRAGEOUSLY LARGE NEURAL NETWORKS THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER (2017)</a></h3>
    <div class="summary"><p>Increasing a model’s number of parameters enhances its capacity to learn complex patterns, but it also raises computational costs.
<a href="https://arxiv.org/pdf/1701.06538">OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</a> (MoE)  introduces a method to scale model capacity efficiently by using a gating network that dynamically selects a sparse subset of feed-forward networks (called experts) for each input, while skipping computation for the others.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/mixture-of-experts/">
            #Mixture-of-Experts
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        August 1, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/training-language-models-to-follow-instructions-with-human-feedback/">Training Language Models to Follow Instructions With Human Feedback (2022)</a></h3>
    <div class="summary"><p>Increasing the number of parameters does not inherently improve a model’s ability to follow users&rsquo; intent. Large Language Models (LLMs) are typically trained to predict the next token in large-scale internet text corpora, but this objective does not explicitly encourage models to follow user instructions in a helpful and safe manner. [Training Language Models to Follow Instructions with Human Feedback](<a href="https://arxiv.org/pdf/2203.02155">https://arxiv.org/pdf/2203.02155</a>  introduces a method for aligning models with human intent using Reinforcement Learning from Human Feedback (RLHF).</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/reinforcement-learning-from-human-feedback/">
            #Reinforcement Learning From Human Feedback
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/large-language-models/">
            #Large Language Models
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        July 24, 2025
        
      </span>
    </div>
  </article>
  
  <article>
    <h3><a href="https://ryotaro.dev/en/posts/react-synergizing-reasoning-and-acting-in-language-models/">REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS (2023)</a></h3>
    <div class="summary"><p><a href="https://arxiv.org/pdf/2210.03629">ReAct (Synergizing <em>Re</em>asoning + <em>Act</em>ing)</a> is a prompting strategy that enables large language models (LLMs) to generate reasoning traces and actions in an interleaved manner, allowing them to more effectively combine thought and actions.
ReAct provides LLMs with reasoning traces that guide their actions.
Additionally, it allows models to incorporate external knowledge obtained through actions into their reasoning process.</p></div>
    <div class="article-footer">
      
      <ul class="tags">
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/large-language-models/">
            #Large Language Models
          </a>
        </li>
        
        <li class="tag">
          <a href="https://ryotaro.dev/en/tags/prompt-engineering/">
            #Prompt Engineering
          </a>
        </li>
        
      </ul>
      
      <span class="date">
        
        July 11, 2025
        
      </span>
    </div>
  </article>
  
</main>

  

<ul class="pagination">
  
  
  <li>
    <a href="/en/page/2/">
      <i class="fa-solid fa-xl fa-angle-right"></i>
    </a>
  </li>
  <li>
    <a href="/en/page/7/">
      <i class="fa-solid fa-xl fa-angles-right"></i>
    </a>
  </li>
  
</ul>

  <footer>
    <small>© Ryotaro Nakamura. All Rights Reserved.</small>
  </footer>
</body>


</html>
