<!DOCTYPE html>
<html>

<head>
    
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W5TDG76');</script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=300,initial-scale=1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css"
        integrity="sha512-NmLkDIU1C/C88wi324HBc+S2kLhi08PN5GDeUVVVC/BVt/9Izdsc9SVeVfA1UZbY3sHUlDSyRXhCzHfr6hmPPw=="
        crossorigin="anonymous" />
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="/css/base.css">
    
    <link rel="stylesheet" href="/css/articles.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/4e0e0c0a41.js" crossorigin="anonymous"></script>
    <title>Categories</title>
</head>

<body>
    
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W5TDG76"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <header class="navigation">
        <h2><a href="/">Blanket</a></h2>
        <nav>
            <ul>
                <li><a href="/about">About me</a></li>
                <li><a href="/posts">Posts</a></li>
                <li><a href="/gallery">Gallery</a></li>
            </ul>
        </nav>
    </header>
    
<main>
    <h1>Posts</h1>

    
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/a_family_of_experiments_on_test_driven_development/">A Family of Experiments on Test-Driven Development</a></h3>
        <div class="summary"><p>TDDがコードに及ぼす影響を調べた先行研究はあるが、研究手法、被検者、プログラミング環境などの実験条件は様々、結論は違い、明確な結論はない。
表題の論文は、個別のTDDの実験結果の精度と汎化性を上げるために、実験結果に影響するとみられる実験条件をかえ、TDDとITL(iterative test-last development)を比較する12の実験を実施した。
被検者は4大学と企業12社で、5つのトイタスクを解かせ、通ったテストオラクルの割合で実装の品質を評価した。
結果、TDD初心者の被検者はITLによる実装のほうが品質が高かった。
エディタや言語、ITLとTDDの実験の被験順序、TDDとITLをどちらを先に学んだかは品質に影響しなかった。
学生よりも企業に所属するプロフェッショナルのほうがTDD、ITL両方で品質がよかったが、TDDで実装したときの品質の下がり幅は学生の下がり幅の2倍に及んだ。</p></div>
        <div class="article-footer">
            <span class="date">April 29, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/simcse/">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a></h3>
        <div class="summary"><p>対照学習は、意味の近い要素同士を近くに、異なる要素を遠くに配置する分散表現を獲得する。
SimCSEは、文の分散表現のための対照学習であり、教師なしと教師ありの2つの学習方法を提供する。
教師なし学習は、Dropout層を通したサンプルが近くに配置されるように、Dropout層の出力2つからなるペアを教師データにする。
教師あり学習は、自然言語推論(含意関係認識)の教師データをつかい、前提と含意の分散表現が近くになるように学習する。</p></div>
        <div class="article-footer">
            <span class="date">April 29, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/stochastic_gradient_boosting/">Stochastic Gradient Boosting</a></h3>
        <div class="summary"><p>Gradient Boostingは、反復的に、モデルの予測と正解の残差に弱識別器をあてはめ、弱識別器をモデルに追加する。
Stochastic Gradient Boostingは、弱識別器の学習に非復元抽出したデータの部分集合をつかい、精度と学習速度を向上する。</p></div>
        <div class="article-footer">
            <span class="date">April 23, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/lightgbm/">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a></h3>
        <div class="summary"><p>LightGBMは、GBDTを高速化したアルゴリズムであり、XGBoostよりも必要な計算時間と消費メモリが少ない。
GBDTの処理時間のボトルネックは決定木の分岐を決めるところである。
その前処理で特徴の値をソートする場合は、ソートがボトルネックになる。
勾配の小さいサンプルを除外することでデータを減らし、また、同時に0でない値にならない排他的な特徴をマージすることで特徴の種類を減らし、ソートを高速化した。</p></div>
        <div class="article-footer">
            <span class="date">April 16, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/adam/">Adam: A Method for Stochastic Optimization</a></h3>
        <div class="summary"><p>ADAMはAdaptive moment estimationに由来し、名前のとおり、推定した1, 2次のモーメントによる学習率最適化のアルゴリズムである。
勾配が疎なときに有効なAdaGradの利点と、目的関数が時間とともに変化してもよいRMSPropの利点をそなえる。
一次や二次のモーメントを、指数関数的に加重を減少させる移動平均で推定する。
ただし、モーメントの初期値を0にすると最初のうちはモーメントの推定値が0に偏ってしまう。
そこで、反復回数がすくないほど推定値を大きくなるよう補正する。</p></div>
        <div class="article-footer">
            <span class="date">April 9, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/xgboost/">XGBoost: A Scalable Tree Boosting System</a></h3>
        <div class="summary"><p>XGBoostは、キャッシュやシャーディングによる高速な勾配ブースティングのライブラリであり、スパースなデータでも高速に学習できる。
情報利得が大きくなるにノードから枝をのばすときは、ノードにあるサンプルで分岐の条件を決定する。
このとき、分岐条件の特徴が欠損するサンプルを左右どちらかに無条件にふり分けると利得が大きくなるかを計算する。
これにより、欠損のないサンプル数の線形オーダまで計算量を削減する。</p></div>
        <div class="article-footer">
            <span class="date">March 26, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/classifier_chains_for_multi-label_classification/">Classifier Chains for Multi-label Classification</a></h3>
        <div class="summary"><p>scikit-learnの<a href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html?highlight=chain#sklearn-multioutput-classifierchain">Classifier Chain</a>で実装されたClassifier Chainsは、ラベルの相関関係を特徴につかうマルチラベル分類のモデルで、相関関係をもちいる既存手法よりも計算量がすくない。
より細かくみれば、Classifier Chainsは、Classifier Chain Model(CC)とCCのアンサンブル学習であるEnsembles of Classifier Chains(ECC)の2つにわかれる。</p></div>
        <div class="article-footer">
            <span class="date">March 22, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/effective_multi-label_active_learning_for_text_classification/">Effective Multi-Label Active Learning for Text Classification</a></h3>
        <div class="summary"><p>SVMをつかったマルチラベル文書分類のための能動学習である。
ラベルをつければモデルの損失を最も小さくできるデータをさがす。
ラベルつきデータでSVMを学習し、さらに、その識別関数の値を特徴としてラベルの数を予測するロジスティック回帰を学習する。
ラベルのないデータを両モデルに入力し、ロジスティック回帰が予測するラベルの数だけ、識別関数の値の高い順にラベルを選び、そのデータのマルチラベルとみなす。
このとき、その推定したマルチラベルと識別関数の値がほど、損失関数を最も小さくできるデータとみなす。</p></div>
        <div class="article-footer">
            <span class="date">March 12, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/pegasos/">Pegasos: Primal Estimated sub-GrAdient SOlver for SVM</a></h3>
        <div class="summary"><p>Pegasosは、SVMの学習のための反復的なアルゴリズムであり、2022年3月現在、scikit-learnの<a href="https://scikit-learn.org/stable/modules/sgd.html#implementation-details">SGDClassifier</a>で学習率を更新に採用されている。
Pegasosは、目的関数の最小値の近似値をもとめる。
求める最小値との誤差を\(\epsilon\), SVMの正則化パラメタを\(\lambda\), 各サンプルの説明変数の0でない要素数の上限を\(d\)とすると、線形カーネルをつかうSVMの時間計算量は、\(\tilde{O}(d/\lambda \epsilon)\)になる。
訓練データの数が計算量に影響しないので、教師データの数に対してスケールする。</p></div>
        <div class="article-footer">
            <span class="date">March 12, 2022</span>
        </div>
    </article>
    
    <article>
        <h3><a href="https://nryotaro.dev/posts/an_interior_point_method_for_large_scale_l1_regularized_least_squares/">An Interior-Point Method for Large-Scale L1-Regularized Least Squares</a></h3>
        <div class="summary"><p>前処理付共役勾配法により、スパースな説明変数をもつリッジ回帰の学習を高速化する。
リッジ回帰の目的関数は、凸関数だが微分可能ではない。
また、リッジ回帰の係数はスパースになりやすい。
そこで、目的関数を最小化する係数の探索を、線形不等式制約つきの凸二次計画問題とらえ、前処理付共役勾配法をつかった内点法で係数の更新方向を探索する。</p></div>
        <div class="article-footer">
            <span class="date">March 5, 2022</span>
        </div>
    </article>
    
</main>

    

<ul class="pagination">
    
    <li>
        <a href="/categories/"><span class="material-icons">first_page</span></a>
    </li>
    <li>
        <a href="/categories/page/3/"><span class="material-icons">chevron_left</span></a>
    </li>
    
    
    <li>
        <a href="/categories/page/5/"><span class="material-icons">chevron_right</span></a>
    </li>
    <li>
        <a href="/categories/page/25/"><span class="material-icons">last_page</span></a>
    </li>
    
</ul>

    <footer>
        <ul>
            
            <li>
                <a href="https://github.com/nryotaro">
                    <i class="fab fa-github"></i>
                </a>
            </li>
            
            
            <li>
                <a href="https://www.linkedin.com/in/nakamura-ryotaro/">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </li>
            
            <li><a href="/index.xml"><i class="fas fa-rss"></i></a></li>
            
        </ul>
        <small>© Nakamura, Ryotaro. All Rights Reserved.</small>
    </footer>
</body>

</html>
